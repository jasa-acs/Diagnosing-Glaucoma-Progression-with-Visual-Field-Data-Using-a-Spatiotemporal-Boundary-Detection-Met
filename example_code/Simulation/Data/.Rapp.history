Save
1:n
(1:n) + 6
DeltaMean <- MeanParameters[(1:n) + 6]
(1:n) + 6 + 52
###Set seed#
set.seed(54)#
#
###Load packages#
suppressMessages(library(mvtnorm)) #multivariate normal#
suppressMessages(library(pscl)) #inverse gamma#
suppressMessages(library(msm)) #truncated normal#
suppressMessages(library(coda)) #mcmc#
suppressMessages(library(bayesm)) #lndMvn#
#
###Define the blind spot#
blind_spot <- c(26, 35)#
###Read in Data for ID = 101, Eye = Left#
glaucoma <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/all_josh.csv")#
# glaucoma <- read.csv("/home/users/berchuck/Dissertation/Files/all_josh.csv")#
dat <- glaucoma[(glaucoma$patid == patid) & (glaucoma$eye == eye), ]#
dat <- dat[order(dat$field), ]#
dat <- dat[!dat$point_ID %in% blind_spot, ] # remove blind spot 26, 35#
temp <- cbind(dat$sens_raw, dat$day)#
#
###Data objects#
n <- length(unique(dat$point_ID))#
T <- length(unique(dat$field))#
N <- n * T#
YStar <- matrix(temp[, 1], nrow = N, ncol = 1) #no scale#
t <- matrix(unique(dat$day), nrow = T, ncol = 1) / 365 #scale to years#
t <- ((t - mean(t)) / sd(t)) * 0.5 # center at zero with sd of 0.5.#
#
###Matrix objects#
OneT <- matrix(1, nrow = T,ncol = 1)#
Eyen <- diag(n)#
Onen <- matrix(1, nrow = n, ncol = 1)#
OneN <- matrix(1, nrow = N, ncol = 1)#
EyeN <- diag(N)#
ZAlpha <- OneN#
ZBeta <- kronecker(t, Onen)#
ZDelta <- kronecker(OneT, Eyen)#
ZEta <- kronecker(t, Eyen)#
#
###Initial values#
SigmaEpsilon2 <- Tau2 <- Kappa2 <- 1#
Alpha <- Beta <- 0#
Delta <- Eta <- matrix(0, nrow = n, ncol = 1)#
#
###Adjacency matrix#
source("/Users/Sam/Documents/Sam/School/Dissertation/Models/Likelihoods/Glaucoma_R_Functions/adjacency.R")#
# source("/home/users/berchuck/Dissertation/Files/adjacency.R")#
W <- adj.mat(Brigid)#
W <- W[-blind_spot, -blind_spot] #Remove blind spot 26,35#
Dw <- diag(apply(W, 1, sum))#
WStar <- Dw - W#
#
###Hyperparameters#
AlphaSigma <- 5.6#
BetaSigma <- 43.2#
MuAlpha <- 24#
SigmaAlpha2 <- 18^2 #
AlphaBeta <- 0 #
BetaBeta <- 5#
AlphaTau <- AlphaKappa <- 1 / 2#
BetaTau <- BetaKappa <- 0.01 / 2#
#
###Tobit information#
Tobit <- YStar == 0#
NBelow <- sum(Tobit)#
WhichTobit <- which(Tobit)#
#
###MCMC objects#
NSims <- 100000#
NBurn <- 12000#
NThin <- 10#
NTotal <- NBurn + NSims#
WhichKeep <- NBurn + (1:(NSims / NThin)) * NThin#
NKeep <- length(WhichKeep)#
#
###Pilot adaptation objects#
NPilot <- 24 #
WhichPilotAdapt <- (1:NPilot ) * NBurn / NPilot#
PilotAdaptDenominator <- WhichPilotAdapt[1]#
#
###Metropolis object#
TuningParameter <- 1#
Acceptance <- 0#
#
###Create storage object#
RawStorage <- matrix(nrow = 6 + 2 * n, ncol = NKeep)#
#
###Time MCMC sampler#
Begin <- Sys.time()#
#
###Begin sampler#
for (s in 1:NTotal) {#
	###Tobit step#
	if (NBelow == 0) Y <- YStar#
	if (NBelow != 0) {#
		Y <- YStar#
		MuTobit <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Y[Tobit] <- rtnorm(NBelow, MuTobit[NBelow], sqrt(SigmaEpsilon2), upper = 0)#
	}#
	###Alpha full conditional#
	VarAlpha <- chol2inv(chol(t(ZAlpha) %*% ZAlpha / SigmaEpsilon2 + 1 / SigmaAlpha2))#
	Gamma <- Y - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	MeanAlpha <- VarAlpha %*% (t(ZAlpha) %*% Gamma / SigmaEpsilon2 + MuAlpha / SigmaAlpha2)#
	Alpha <- rnorm(1, MeanAlpha, sqrt(VarAlpha))#
	###Delta full conditional#
	CovDelta <- chol2inv(chol(t(ZDelta) %*% ZDelta / SigmaEpsilon2 + WStar / Tau2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZEta %*% Eta#
	MeanDelta <- CovDelta %*% (t(ZDelta) %*% Gamma / SigmaEpsilon2)#
	Delta <- matrix(rmvnorm(1, MeanDelta, CovDelta), ncol = 1)#
	Delta <- (Delta - mean(Delta)) # sum to zero constraint#
#
	###Eta full conditional#
	CovEta <- chol2inv(chol(t(ZEta) %*% ZEta / SigmaEpsilon2 + WStar / Kappa2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta#
	MeanEta <- CovEta %*% (t(ZEta) %*% Gamma / SigmaEpsilon2)#
	Eta <- matrix(rmvnorm(1, MeanEta, CovEta), ncol = 1)#
	Eta <- (Eta - mean(Eta)) # sum to zero constraint#
#
	###SigmaEpsilon2 full conditional#
	AlphaSigmaNew <- AlphaSigma + N / 2#
	Residuals <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	BetaSigmaNew <- BetaSigma + t(Residuals) %*% Residuals / 2#
	SigmaEpsilon2 <- rigamma(1, AlphaSigmaNew, BetaSigmaNew)#
#
	###Tau2 full conditional#
	AlphaTauNew <- AlphaTau + (n - 1) / 2 #
	BetaTauNew <- BetaTau + t(Delta) %*% WStar %*% Delta / 2#
	Tau2 <- rigamma(1, AlphaTauNew, BetaTauNew)#
	if ((s <= NBurn) & (Tau2 >= 20)) Tau2 <- 20 # Tau2 is truncated above at 20 in the burn-in phase.#
#
	##Kappa2 Full Conditional#
	AlphaKappaNew <- AlphaKappa + (n - 1) / 2#
	BetaKappaNew <- BetaKappa + t(Eta) %*% WStar %*% Eta / 2#
	Kappa2 <- rigamma(1, AlphaKappaNew, BetaKappaNew)#
#
	###Metropolis step for Beta#
		###Sample proposal	#
    	ProposalBeta <- rnorm(1, Beta, sqrt(TuningParameter))#
#
    	###Likelihood Component#
    	MeanProposal <- Alpha * ZAlpha + ProposalBeta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Mean <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Component1A <- lndMvn(Y, MeanProposal, SigmaEpsilon2 * EyeN)#
    	Component1B <- lndMvn(Y, Mean, SigmaEpsilon2 * EyeN)#
        Component1 <- Component1A - Component1B#
#
    	###Prior components    	#
    	Component2A <- dcauchy(ProposalBeta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2B <- dcauchy(Beta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2 <- Component2A - Component2B#
#
    	###Log acceptance ratio#
    	LogR <- Component1 + Component2#
#
   	 	###Metropolis update#
    	RandU <- runif(1)#
    	if (log(RandU) < LogR) {#
#
     		###Keep count of acceptances#
      		Acceptance <- Acceptance + 1#
#
      		###Update parameters output#
      		Beta <- ProposalBeta#
#
    	}#
	##Compute Deviance for DIC#
	if (s %in% WhichKeep) {#
    	MeanDev <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Dev1 <- -2 * sum(dnorm(YStar[!Tobit], MeanDev[!Tobit], sqrt(SigmaEpsilon2), log = TRUE))#
		Dev2 <- -2 * sum(pnorm(MeanDev[Tobit] / sqrt(SigmaEpsilon2), 0, 1, lower.tail = FALSE, log.p = TRUE))	#
		Dev <- Dev1 + Dev2	#
	}#
#
    ###Pilot adaptation#
    if (s %in% WhichPilotAdapt) {#
    	AcceptancePct <- Acceptance / PilotAdaptDenominator#
    	if (AcceptancePct >= 0.90) TuningParameter <- TuningParameter * 1.3#
		if ((AcceptancePct >= 0.75) & (AcceptancePct < 0.90)) TuningParameter <- TuningParameter * 1.2#
		if ((AcceptancePct >= 0.45) & (AcceptancePct < 0.75)) TuningParameter <- TuningParameter * 1.1#
		if ((AcceptancePct <= 0.25) & (AcceptancePct > 0.15)) TuningParameter <- TuningParameter * 0.9#
		if ((AcceptancePct <= 0.15) & (AcceptancePct > 0.10)) TuningParameter <- TuningParameter * 0.8#
		if (AcceptancePct <= 0.10) TuningParameter <- TuningParameter * 0.7#
		Acceptance <- 0#
    }#
	###Store samples#
	if (s %in% WhichKeep) {#
 		Save <- c(Alpha, Beta, SigmaEpsilon2, Tau2, Kappa2, Dev, as.numeric(Delta), as.numeric(Eta))#
  		RawSamples[ , which(s == WhichKeep)] <- Save#
	}#
	##Verbose#
	if (s %in% WhichKeep) {#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / (NTotal - NBurn)) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
	}#
#
	###Time MCMC Sampler#
	if (s == NTotal) {#
		After <- Sys.time()#
		RunTime <- After - Begin#
		cat(paste0("Run Time: ", round(RunTime, digits = 2), " ", attr(RunTime, "unit")))#
		cat("######################################################################################################\n")#
#
	}#
###End MCMC Sampler	#
}
###Set seed#
set.seed(54)#
#
###Load packages#
suppressMessages(library(mvtnorm)) #multivariate normal#
suppressMessages(library(pscl)) #inverse gamma#
suppressMessages(library(msm)) #truncated normal#
suppressMessages(library(coda)) #mcmc#
suppressMessages(library(bayesm)) #lndMvn#
#
###Define the blind spot#
blind_spot <- c(26, 35)#
###Read in Data for ID = 101, Eye = Left#
glaucoma <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/all_josh.csv")#
# glaucoma <- read.csv("/home/users/berchuck/Dissertation/Files/all_josh.csv")#
dat <- glaucoma[(glaucoma$patid == patid) & (glaucoma$eye == eye), ]#
dat <- dat[order(dat$field), ]#
dat <- dat[!dat$point_ID %in% blind_spot, ] # remove blind spot 26, 35#
temp <- cbind(dat$sens_raw, dat$day)#
#
###Data objects#
n <- length(unique(dat$point_ID))#
T <- length(unique(dat$field))#
N <- n * T#
YStar <- matrix(temp[, 1], nrow = N, ncol = 1) #no scale#
t <- matrix(unique(dat$day), nrow = T, ncol = 1) / 365 #scale to years#
t <- ((t - mean(t)) / sd(t)) * 0.5 # center at zero with sd of 0.5.#
#
###Matrix objects#
OneT <- matrix(1, nrow = T,ncol = 1)#
Eyen <- diag(n)#
Onen <- matrix(1, nrow = n, ncol = 1)#
OneN <- matrix(1, nrow = N, ncol = 1)#
EyeN <- diag(N)#
ZAlpha <- OneN#
ZBeta <- kronecker(t, Onen)#
ZDelta <- kronecker(OneT, Eyen)#
ZEta <- kronecker(t, Eyen)#
#
###Initial values#
SigmaEpsilon2 <- Tau2 <- Kappa2 <- 1#
Alpha <- Beta <- 0#
Delta <- Eta <- matrix(0, nrow = n, ncol = 1)#
#
###Adjacency matrix#
source("/Users/Sam/Documents/Sam/School/Dissertation/Models/Likelihoods/Glaucoma_R_Functions/adjacency.R")#
# source("/home/users/berchuck/Dissertation/Files/adjacency.R")#
W <- adj.mat(Brigid)#
W <- W[-blind_spot, -blind_spot] #Remove blind spot 26,35#
Dw <- diag(apply(W, 1, sum))#
WStar <- Dw - W#
#
###Hyperparameters#
AlphaSigma <- 5.6#
BetaSigma <- 43.2#
MuAlpha <- 24#
SigmaAlpha2 <- 18^2 #
AlphaBeta <- 0 #
BetaBeta <- 5#
AlphaTau <- AlphaKappa <- 1 / 2#
BetaTau <- BetaKappa <- 0.01 / 2#
#
###Tobit information#
Tobit <- YStar == 0#
NBelow <- sum(Tobit)#
WhichTobit <- which(Tobit)#
#
###MCMC objects#
NSims <- 100000#
NBurn <- 12000#
NThin <- 10#
NTotal <- NBurn + NSims#
WhichKeep <- NBurn + (1:(NSims / NThin)) * NThin#
NKeep <- length(WhichKeep)#
#
###Pilot adaptation objects#
NPilot <- 24 #
WhichPilotAdapt <- (1:NPilot ) * NBurn / NPilot#
PilotAdaptDenominator <- WhichPilotAdapt[1]#
#
###Metropolis object#
TuningParameter <- 1#
Acceptance <- 0#
#
###Create storage object#
RawSamples <- matrix(nrow = 6 + 2 * n, ncol = NKeep)#
#
###Time MCMC sampler#
Begin <- Sys.time()#
#
###Begin sampler#
for (s in 1:NTotal) {#
	###Tobit step#
	if (NBelow == 0) Y <- YStar#
	if (NBelow != 0) {#
		Y <- YStar#
		MuTobit <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Y[Tobit] <- rtnorm(NBelow, MuTobit[NBelow], sqrt(SigmaEpsilon2), upper = 0)#
	}#
	###Alpha full conditional#
	VarAlpha <- chol2inv(chol(t(ZAlpha) %*% ZAlpha / SigmaEpsilon2 + 1 / SigmaAlpha2))#
	Gamma <- Y - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	MeanAlpha <- VarAlpha %*% (t(ZAlpha) %*% Gamma / SigmaEpsilon2 + MuAlpha / SigmaAlpha2)#
	Alpha <- rnorm(1, MeanAlpha, sqrt(VarAlpha))#
	###Delta full conditional#
	CovDelta <- chol2inv(chol(t(ZDelta) %*% ZDelta / SigmaEpsilon2 + WStar / Tau2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZEta %*% Eta#
	MeanDelta <- CovDelta %*% (t(ZDelta) %*% Gamma / SigmaEpsilon2)#
	Delta <- matrix(rmvnorm(1, MeanDelta, CovDelta), ncol = 1)#
	Delta <- (Delta - mean(Delta)) # sum to zero constraint#
#
	###Eta full conditional#
	CovEta <- chol2inv(chol(t(ZEta) %*% ZEta / SigmaEpsilon2 + WStar / Kappa2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta#
	MeanEta <- CovEta %*% (t(ZEta) %*% Gamma / SigmaEpsilon2)#
	Eta <- matrix(rmvnorm(1, MeanEta, CovEta), ncol = 1)#
	Eta <- (Eta - mean(Eta)) # sum to zero constraint#
#
	###SigmaEpsilon2 full conditional#
	AlphaSigmaNew <- AlphaSigma + N / 2#
	Residuals <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	BetaSigmaNew <- BetaSigma + t(Residuals) %*% Residuals / 2#
	SigmaEpsilon2 <- rigamma(1, AlphaSigmaNew, BetaSigmaNew)#
#
	###Tau2 full conditional#
	AlphaTauNew <- AlphaTau + (n - 1) / 2 #
	BetaTauNew <- BetaTau + t(Delta) %*% WStar %*% Delta / 2#
	Tau2 <- rigamma(1, AlphaTauNew, BetaTauNew)#
	if ((s <= NBurn) & (Tau2 >= 20)) Tau2 <- 20 # Tau2 is truncated above at 20 in the burn-in phase.#
#
	##Kappa2 Full Conditional#
	AlphaKappaNew <- AlphaKappa + (n - 1) / 2#
	BetaKappaNew <- BetaKappa + t(Eta) %*% WStar %*% Eta / 2#
	Kappa2 <- rigamma(1, AlphaKappaNew, BetaKappaNew)#
#
	###Metropolis step for Beta#
		###Sample proposal	#
    	ProposalBeta <- rnorm(1, Beta, sqrt(TuningParameter))#
#
    	###Likelihood Component#
    	MeanProposal <- Alpha * ZAlpha + ProposalBeta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Mean <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Component1A <- lndMvn(Y, MeanProposal, SigmaEpsilon2 * EyeN)#
    	Component1B <- lndMvn(Y, Mean, SigmaEpsilon2 * EyeN)#
        Component1 <- Component1A - Component1B#
#
    	###Prior components    	#
    	Component2A <- dcauchy(ProposalBeta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2B <- dcauchy(Beta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2 <- Component2A - Component2B#
#
    	###Log acceptance ratio#
    	LogR <- Component1 + Component2#
#
   	 	###Metropolis update#
    	RandU <- runif(1)#
    	if (log(RandU) < LogR) {#
#
     		###Keep count of acceptances#
      		Acceptance <- Acceptance + 1#
#
      		###Update parameters output#
      		Beta <- ProposalBeta#
#
    	}#
	##Compute Deviance for DIC#
	if (s %in% WhichKeep) {#
    	MeanDev <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Dev1 <- -2 * sum(dnorm(YStar[!Tobit], MeanDev[!Tobit], sqrt(SigmaEpsilon2), log = TRUE))#
		Dev2 <- -2 * sum(pnorm(MeanDev[Tobit] / sqrt(SigmaEpsilon2), 0, 1, lower.tail = FALSE, log.p = TRUE))	#
		Dev <- Dev1 + Dev2	#
	}#
#
    ###Pilot adaptation#
    if (s %in% WhichPilotAdapt) {#
    	AcceptancePct <- Acceptance / PilotAdaptDenominator#
    	if (AcceptancePct >= 0.90) TuningParameter <- TuningParameter * 1.3#
		if ((AcceptancePct >= 0.75) & (AcceptancePct < 0.90)) TuningParameter <- TuningParameter * 1.2#
		if ((AcceptancePct >= 0.45) & (AcceptancePct < 0.75)) TuningParameter <- TuningParameter * 1.1#
		if ((AcceptancePct <= 0.25) & (AcceptancePct > 0.15)) TuningParameter <- TuningParameter * 0.9#
		if ((AcceptancePct <= 0.15) & (AcceptancePct > 0.10)) TuningParameter <- TuningParameter * 0.8#
		if (AcceptancePct <= 0.10) TuningParameter <- TuningParameter * 0.7#
		Acceptance <- 0#
    }#
	###Store samples#
	if (s %in% WhichKeep) {#
 		Save <- c(Alpha, Beta, SigmaEpsilon2, Tau2, Kappa2, Dev, as.numeric(Delta), as.numeric(Eta))#
  		RawSamples[ , which(s == WhichKeep)] <- Save#
	}#
	##Verbose#
	if (s %in% WhichKeep) {#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / (NTotal - NBurn)) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
	}#
#
	###Time MCMC Sampler#
	if (s == NTotal) {#
		After <- Sys.time()#
		RunTime <- After - Begin#
		cat(paste0("Run Time: ", round(RunTime, digits = 2), " ", attr(RunTime, "unit")))#
		cat("######################################################################################################\n")#
#
	}#
###End MCMC Sampler	#
}
Acceptance
Acceptance / (NTotal - NBurn)
###Set seed#
set.seed(54)#
#
###Load packages#
suppressMessages(library(mvtnorm)) #multivariate normal#
suppressMessages(library(pscl)) #inverse gamma#
suppressMessages(library(msm)) #truncated normal#
suppressMessages(library(coda)) #mcmc#
suppressMessages(library(bayesm)) #lndMvn#
#
###Define the blind spot#
blind_spot <- c(26, 35)#
###Read in Data for ID = 101, Eye = Left#
glaucoma <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/all_josh.csv")#
# glaucoma <- read.csv("/home/users/berchuck/Dissertation/Files/all_josh.csv")#
dat <- glaucoma[(glaucoma$patid == patid) & (glaucoma$eye == eye), ]#
dat <- dat[order(dat$field), ]#
dat <- dat[!dat$point_ID %in% blind_spot, ] # remove blind spot 26, 35#
temp <- cbind(dat$sens_raw, dat$day)#
#
###Data objects#
n <- length(unique(dat$point_ID))#
T <- length(unique(dat$field))#
N <- n * T#
YStar <- matrix(temp[, 1], nrow = N, ncol = 1) #no scale#
t <- matrix(unique(dat$day), nrow = T, ncol = 1) / 365 #scale to years#
t <- ((t - mean(t)) / sd(t)) * 0.5 # center at zero with sd of 0.5.#
#
###Matrix objects#
OneT <- matrix(1, nrow = T,ncol = 1)#
Eyen <- diag(n)#
Onen <- matrix(1, nrow = n, ncol = 1)#
OneN <- matrix(1, nrow = N, ncol = 1)#
EyeN <- diag(N)#
ZAlpha <- OneN#
ZBeta <- kronecker(t, Onen)#
ZDelta <- kronecker(OneT, Eyen)#
ZEta <- kronecker(t, Eyen)#
#
###Initial values#
SigmaEpsilon2 <- Tau2 <- Kappa2 <- 1#
Alpha <- Beta <- 0#
Delta <- Eta <- matrix(0, nrow = n, ncol = 1)#
#
###Adjacency matrix#
source("/Users/Sam/Documents/Sam/School/Dissertation/Models/Likelihoods/Glaucoma_R_Functions/adjacency.R")#
# source("/home/users/berchuck/Dissertation/Files/adjacency.R")#
W <- adj.mat(Brigid)#
W <- W[-blind_spot, -blind_spot] #Remove blind spot 26,35#
Dw <- diag(apply(W, 1, sum))#
WStar <- Dw - W#
#
###Hyperparameters#
AlphaSigma <- 5.6#
BetaSigma <- 43.2#
MuAlpha <- 24#
SigmaAlpha2 <- 18^2 #
AlphaBeta <- 0 #
BetaBeta <- 5#
AlphaTau <- AlphaKappa <- 1 / 2#
BetaTau <- BetaKappa <- 0.01 / 2#
#
###Tobit information#
Tobit <- YStar == 0#
NBelow <- sum(Tobit)#
WhichTobit <- which(Tobit)#
#
###MCMC objects#
NSims <- 100000#
NBurn <- 12000#
NThin <- 10#
NTotal <- NBurn + NSims#
WhichKeep <- NBurn + (1:(NSims / NThin)) * NThin#
NKeep <- length(WhichKeep)#
#
###Pilot adaptation objects#
NPilot <- 24 #
WhichPilotAdapt <- (1:NPilot ) * NBurn / NPilot#
PilotAdaptDenominator <- WhichPilotAdapt[1]#
#
###Metropolis object#
TuningParameter <- 0.3#
Acceptance <- 0#
#
###Create storage object#
RawSamples <- matrix(nrow = 6 + 2 * n, ncol = NKeep)#
#
###Time MCMC sampler#
Begin <- Sys.time()#
#
###Begin sampler#
for (s in 1:NTotal) {#
	###Tobit step#
	if (NBelow == 0) Y <- YStar#
	if (NBelow != 0) {#
		Y <- YStar#
		MuTobit <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Y[Tobit] <- rtnorm(NBelow, MuTobit[NBelow], sqrt(SigmaEpsilon2), upper = 0)#
	}#
	###Alpha full conditional#
	VarAlpha <- chol2inv(chol(t(ZAlpha) %*% ZAlpha / SigmaEpsilon2 + 1 / SigmaAlpha2))#
	Gamma <- Y - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	MeanAlpha <- VarAlpha %*% (t(ZAlpha) %*% Gamma / SigmaEpsilon2 + MuAlpha / SigmaAlpha2)#
	Alpha <- rnorm(1, MeanAlpha, sqrt(VarAlpha))#
	###Delta full conditional#
	CovDelta <- chol2inv(chol(t(ZDelta) %*% ZDelta / SigmaEpsilon2 + WStar / Tau2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZEta %*% Eta#
	MeanDelta <- CovDelta %*% (t(ZDelta) %*% Gamma / SigmaEpsilon2)#
	Delta <- matrix(rmvnorm(1, MeanDelta, CovDelta), ncol = 1)#
	Delta <- (Delta - mean(Delta)) # sum to zero constraint#
#
	###Eta full conditional#
	CovEta <- chol2inv(chol(t(ZEta) %*% ZEta / SigmaEpsilon2 + WStar / Kappa2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta#
	MeanEta <- CovEta %*% (t(ZEta) %*% Gamma / SigmaEpsilon2)#
	Eta <- matrix(rmvnorm(1, MeanEta, CovEta), ncol = 1)#
	Eta <- (Eta - mean(Eta)) # sum to zero constraint#
#
	###SigmaEpsilon2 full conditional#
	AlphaSigmaNew <- AlphaSigma + N / 2#
	Residuals <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	BetaSigmaNew <- BetaSigma + t(Residuals) %*% Residuals / 2#
	SigmaEpsilon2 <- rigamma(1, AlphaSigmaNew, BetaSigmaNew)#
#
	###Tau2 full conditional#
	AlphaTauNew <- AlphaTau + (n - 1) / 2 #
	BetaTauNew <- BetaTau + t(Delta) %*% WStar %*% Delta / 2#
	Tau2 <- rigamma(1, AlphaTauNew, BetaTauNew)#
	if ((s <= NBurn) & (Tau2 >= 20)) Tau2 <- 20 # Tau2 is truncated above at 20 in the burn-in phase.#
#
	##Kappa2 Full Conditional#
	AlphaKappaNew <- AlphaKappa + (n - 1) / 2#
	BetaKappaNew <- BetaKappa + t(Eta) %*% WStar %*% Eta / 2#
	Kappa2 <- rigamma(1, AlphaKappaNew, BetaKappaNew)#
#
	###Metropolis step for Beta#
		###Sample proposal	#
    	ProposalBeta <- rnorm(1, Beta, sqrt(TuningParameter))#
#
    	###Likelihood Component#
    	MeanProposal <- Alpha * ZAlpha + ProposalBeta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Mean <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Component1A <- lndMvn(Y, MeanProposal, SigmaEpsilon2 * EyeN)#
    	Component1B <- lndMvn(Y, Mean, SigmaEpsilon2 * EyeN)#
        Component1 <- Component1A - Component1B#
#
    	###Prior components    	#
    	Component2A <- dcauchy(ProposalBeta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2B <- dcauchy(Beta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2 <- Component2A - Component2B#
#
    	###Log acceptance ratio#
    	LogR <- Component1 + Component2#
#
   	 	###Metropolis update#
    	RandU <- runif(1)#
    	if (log(RandU) < LogR) {#
#
     		###Keep count of acceptances#
      		Acceptance <- Acceptance + 1#
#
      		###Update parameters output#
      		Beta <- ProposalBeta#
#
    	}#
	##Compute Deviance for DIC#
	if (s %in% WhichKeep) {#
    	MeanDev <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Dev1 <- -2 * sum(dnorm(YStar[!Tobit], MeanDev[!Tobit], sqrt(SigmaEpsilon2), log = TRUE))#
		Dev2 <- -2 * sum(pnorm(MeanDev[Tobit] / sqrt(SigmaEpsilon2), 0, 1, lower.tail = FALSE, log.p = TRUE))	#
		Dev <- Dev1 + Dev2	#
	}#
#
    ###Pilot adaptation#
    if (s %in% WhichPilotAdapt) {#
    	AcceptancePct <- Acceptance / PilotAdaptDenominator#
    	if (AcceptancePct >= 0.90) TuningParameter <- TuningParameter * 1.3#
		if ((AcceptancePct >= 0.75) & (AcceptancePct < 0.90)) TuningParameter <- TuningParameter * 1.2#
		if ((AcceptancePct >= 0.45) & (AcceptancePct < 0.75)) TuningParameter <- TuningParameter * 1.1#
		if ((AcceptancePct <= 0.25) & (AcceptancePct > 0.15)) TuningParameter <- TuningParameter * 0.9#
		if ((AcceptancePct <= 0.15) & (AcceptancePct > 0.10)) TuningParameter <- TuningParameter * 0.8#
		if (AcceptancePct <= 0.10) TuningParameter <- TuningParameter * 0.7#
		Acceptance <- 0#
    }#
	###Store samples#
	if (s %in% WhichKeep) {#
 		Save <- c(Alpha, Beta, SigmaEpsilon2, Tau2, Kappa2, Dev, as.numeric(Delta), as.numeric(Eta))#
  		RawSamples[ , which(s == WhichKeep)] <- Save#
	}#
	##Verbose#
	if (s %in% WhichKeep) {#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / (NTotal - NBurn)) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
	}#
#
	###Time MCMC Sampler#
	if (s == NTotal) {#
		After <- Sys.time()#
		RunTime <- After - Begin#
		cat(paste0("Run Time: ", round(RunTime, digits = 2), " ", attr(RunTime, "unit")))#
		cat("######################################################################################################\n")#
#
	}#
###End MCMC Sampler	#
}
###Set seed#
set.seed(54)#
#
###Load packages#
suppressMessages(library(mvtnorm)) #multivariate normal#
suppressMessages(library(pscl)) #inverse gamma#
suppressMessages(library(msm)) #truncated normal#
suppressMessages(library(coda)) #mcmc#
suppressMessages(library(bayesm)) #lndMvn#
#
###Define the blind spot#
blind_spot <- c(26, 35)#
###Read in Data for ID = 101, Eye = Left#
glaucoma <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/all_josh.csv")#
# glaucoma <- read.csv("/home/users/berchuck/Dissertation/Files/all_josh.csv")#
dat <- glaucoma[(glaucoma$patid == patid) & (glaucoma$eye == eye), ]#
dat <- dat[order(dat$field), ]#
dat <- dat[!dat$point_ID %in% blind_spot, ] # remove blind spot 26, 35#
temp <- cbind(dat$sens_raw, dat$day)#
#
###Data objects#
n <- length(unique(dat$point_ID))#
T <- length(unique(dat$field))#
N <- n * T#
YStar <- matrix(temp[, 1], nrow = N, ncol = 1) #no scale#
t <- matrix(unique(dat$day), nrow = T, ncol = 1) / 365 #scale to years#
t <- ((t - mean(t)) / sd(t)) * 0.5 # center at zero with sd of 0.5.#
#
###Matrix objects#
OneT <- matrix(1, nrow = T,ncol = 1)#
Eyen <- diag(n)#
Onen <- matrix(1, nrow = n, ncol = 1)#
OneN <- matrix(1, nrow = N, ncol = 1)#
EyeN <- diag(N)#
ZAlpha <- OneN#
ZBeta <- kronecker(t, Onen)#
ZDelta <- kronecker(OneT, Eyen)#
ZEta <- kronecker(t, Eyen)#
#
###Initial values#
SigmaEpsilon2 <- Tau2 <- Kappa2 <- 1#
Alpha <- Beta <- 0#
Delta <- Eta <- matrix(0, nrow = n, ncol = 1)#
#
###Adjacency matrix#
source("/Users/Sam/Documents/Sam/School/Dissertation/Models/Likelihoods/Glaucoma_R_Functions/adjacency.R")#
# source("/home/users/berchuck/Dissertation/Files/adjacency.R")#
W <- adj.mat(Brigid)#
W <- W[-blind_spot, -blind_spot] #Remove blind spot 26,35#
Dw <- diag(apply(W, 1, sum))#
WStar <- Dw - W#
#
###Hyperparameters#
AlphaSigma <- 5.6#
BetaSigma <- 43.2#
MuAlpha <- 24#
SigmaAlpha2 <- 18^2 #
AlphaBeta <- 0 #
BetaBeta <- 5#
AlphaTau <- AlphaKappa <- 1 / 2#
BetaTau <- BetaKappa <- 0.01 / 2#
#
###Tobit information#
Tobit <- YStar == 0#
NBelow <- sum(Tobit)#
WhichTobit <- which(Tobit)#
#
###MCMC objects#
NSims <- 100000#
NBurn <- 12000#
NThin <- 10#
NTotal <- NBurn + NSims#
WhichKeep <- NBurn + (1:(NSims / NThin)) * NThin#
NKeep <- length(WhichKeep)#
#
###Pilot adaptation objects#
NPilot <- 24 #
WhichPilotAdapt <- (1:NPilot ) * NBurn / NPilot#
PilotAdaptDenominator <- WhichPilotAdapt[1]#
#
###Metropolis object#
TuningParameter <- 0.3#
Acceptance <- 0#
#
###Create storage object#
RawSamples <- matrix(nrow = 6 + 2 * n, ncol = NKeep)#
#
###Time MCMC sampler#
Begin <- Sys.time()#
#
###Begin sampler#
for (s in 1:NTotal) {#
	###Tobit step#
	if (NBelow == 0) Y <- YStar#
	if (NBelow != 0) {#
		Y <- YStar#
		MuTobit <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Y[Tobit] <- rtnorm(NBelow, MuTobit[NBelow], sqrt(SigmaEpsilon2), upper = 0)#
	}#
	###Alpha full conditional#
	VarAlpha <- chol2inv(chol(t(ZAlpha) %*% ZAlpha / SigmaEpsilon2 + 1 / SigmaAlpha2))#
	Gamma <- Y - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	MeanAlpha <- VarAlpha %*% (t(ZAlpha) %*% Gamma / SigmaEpsilon2 + MuAlpha / SigmaAlpha2)#
	Alpha <- rnorm(1, MeanAlpha, sqrt(VarAlpha))#
	###Delta full conditional#
	CovDelta <- chol2inv(chol(t(ZDelta) %*% ZDelta / SigmaEpsilon2 + WStar / Tau2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZEta %*% Eta#
	MeanDelta <- CovDelta %*% (t(ZDelta) %*% Gamma / SigmaEpsilon2)#
	Delta <- matrix(rmvnorm(1, MeanDelta, CovDelta), ncol = 1)#
	Delta <- (Delta - mean(Delta)) # sum to zero constraint#
#
	###Eta full conditional#
	CovEta <- chol2inv(chol(t(ZEta) %*% ZEta / SigmaEpsilon2 + WStar / Kappa2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta#
	MeanEta <- CovEta %*% (t(ZEta) %*% Gamma / SigmaEpsilon2)#
	Eta <- matrix(rmvnorm(1, MeanEta, CovEta), ncol = 1)#
	Eta <- (Eta - mean(Eta)) # sum to zero constraint#
#
	###SigmaEpsilon2 full conditional#
	AlphaSigmaNew <- AlphaSigma + N / 2#
	Residuals <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	BetaSigmaNew <- BetaSigma + t(Residuals) %*% Residuals / 2#
	SigmaEpsilon2 <- rigamma(1, AlphaSigmaNew, BetaSigmaNew)#
#
	###Tau2 full conditional#
	AlphaTauNew <- AlphaTau + (n - 1) / 2 #
	BetaTauNew <- BetaTau + t(Delta) %*% WStar %*% Delta / 2#
	Tau2 <- rigamma(1, AlphaTauNew, BetaTauNew)#
	if ((s <= NBurn) & (Tau2 >= 20)) Tau2 <- 20 # Tau2 is truncated above at 20 in the burn-in phase.#
#
	##Kappa2 Full Conditional#
	AlphaKappaNew <- AlphaKappa + (n - 1) / 2#
	BetaKappaNew <- BetaKappa + t(Eta) %*% WStar %*% Eta / 2#
	Kappa2 <- rigamma(1, AlphaKappaNew, BetaKappaNew)#
#
	###Metropolis step for Beta#
		###Sample proposal	#
    	ProposalBeta <- rnorm(1, Beta, sqrt(TuningParameter))#
#
    	###Likelihood Component#
    	MeanProposal <- Alpha * ZAlpha + ProposalBeta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Mean <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Component1A <- lndMvn(Y, MeanProposal, SigmaEpsilon2 * EyeN)#
    	Component1B <- lndMvn(Y, Mean, SigmaEpsilon2 * EyeN)#
        Component1 <- Component1A - Component1B#
#
    	###Prior components    	#
    	Component2A <- dcauchy(ProposalBeta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2B <- dcauchy(Beta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2 <- Component2A - Component2B#
#
    	###Log acceptance ratio#
    	LogR <- Component1 + Component2#
#
   	 	###Metropolis update#
    	RandU <- runif(1)#
    	if (log(RandU) < LogR) {#
#
     		###Keep count of acceptances#
      		Acceptance <- Acceptance + 1#
#
      		###Update parameters output#
      		Beta <- ProposalBeta#
#
    	}#
	##Compute Deviance for DIC#
	if (s %in% WhichKeep) {#
    	MeanDev <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Dev1 <- -2 * sum(dnorm(YStar[!Tobit], MeanDev[!Tobit], sqrt(SigmaEpsilon2), log = TRUE))#
		Dev2 <- -2 * sum(pnorm(MeanDev[Tobit] / sqrt(SigmaEpsilon2), 0, 1, lower.tail = FALSE, log.p = TRUE))	#
		Dev <- Dev1 + Dev2	#
	}#
#
    ###Pilot adaptation#
    if (s %in% WhichPilotAdapt) {#
    	AcceptancePct <- Acceptance / PilotAdaptDenominator#
    	if (AcceptancePct >= 0.90) TuningParameter <- TuningParameter * 1.3#
		if ((AcceptancePct >= 0.75) & (AcceptancePct < 0.90)) TuningParameter <- TuningParameter * 1.2#
		if ((AcceptancePct >= 0.45) & (AcceptancePct < 0.75)) TuningParameter <- TuningParameter * 1.1#
		if ((AcceptancePct <= 0.25) & (AcceptancePct > 0.15)) TuningParameter <- TuningParameter * 0.9#
		if ((AcceptancePct <= 0.15) & (AcceptancePct > 0.10)) TuningParameter <- TuningParameter * 0.8#
		if (AcceptancePct <= 0.10) TuningParameter <- TuningParameter * 0.7#
		Acceptance <- 0#
    }#
	###Store samples#
	if (s %in% WhichKeep) {#
 		Save <- c(Alpha, Beta, SigmaEpsilon2, Tau2, Kappa2, Dev, as.numeric(Delta), as.numeric(Eta))#
  		RawSamples[ , which(s == WhichKeep)] <- Save#
	}#
	##Verbose#
	# if (s %in% WhichKeep) {#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / (NTotal - NBurn)) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
	# }#
#
	###Time MCMC Sampler#
	if (s == NTotal) {#
		After <- Sys.time()#
		RunTime <- After - Begin#
		cat(paste0("Run Time: ", round(RunTime, digits = 2), " ", attr(RunTime, "unit")))#
		cat("######################################################################################################\n")#
#
	}#
###End MCMC Sampler	#
}#
#
###Compute Diagnostics#
#
	##Compute Posteior Means For DIC#
	MeanParameters <- apply(RawSamples, 1, mean)#
	AlphaMean <- MeanParameters[1]#
	BetaMean <- MeanParameters[2]#
	SigmaEpsilon2Mean <- MeanParameters[3]#
	DBar <- MeanParameters[6]#
	DeltaMean <- MeanParameters[(1:n) + 6]#
	EtaMean <- MeanParameters[(1:n) + 6 + 52]#
	##DHat#
	MeanDev <- AlphaMean * ZAlpha + BetaMean * ZBeta + ZDelta %*% DeltaMean + ZEta %*% EtaMean#
	Dev1 <- -2 * sum(dnorm(YStar[!Tobit], MeanDev[!Tobit], sqrt(SigmaEpsilon2Mean), log = TRUE))#
	Dev2 <- -2 * sum(pnorm(MeanDev[Tobit] / sqrt(SigmaEpsilon2Mean), 0, 1, lower.tail = FALSE, log.p = TRUE))	#
	DHat <- Dev1 + Dev2	#
#
	##DIC, pD#
	pD <- DBar - DHat#
	DIC <- DBar + pD#
	##Summarize Diagnostics#
	Diagnostics <- list(DIC, pD)#
	names(Diagnostics)<-c("DIC","pD")#
	Diagnostics<-unlist(Diagnostics)#
	names(Diagnostics)<-c("DIC","pD")#
#
###Return objects#
out<-list(beta,delta,eta,sigma2,tau2,kappa2,deviance,ppd,Diagnostics)#
names(out)<-c("beta","delta","eta","sigma2","tau2","kappa2","deviance","ppd","diagnostics")#
return(out)#
}
Acceptance
Acceptance / (NTotal - NBurn)
NBurn
s
YStar
YStarWide <- matrix(YStar, ncol = T)
YStarWide
YStarWide[, 1]
YStar1 <- YStarWide[, 1]
YStar1
YStar1 - mean(YStar1)
YStar1 <- YStarWide[, 1]
Delta <- YStar1 - mean(YStar1)
Delta
###Set seed#
set.seed(54)#
#
###Load packages#
suppressMessages(library(mvtnorm)) #multivariate normal#
suppressMessages(library(pscl)) #inverse gamma#
suppressMessages(library(msm)) #truncated normal#
suppressMessages(library(coda)) #mcmc#
suppressMessages(library(bayesm)) #lndMvn#
#
###Define the blind spot#
blind_spot <- c(26, 35)#
###Read in Data for ID = 101, Eye = Left#
glaucoma <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/all_josh.csv")#
# glaucoma <- read.csv("/home/users/berchuck/Dissertation/Files/all_josh.csv")#
dat <- glaucoma[(glaucoma$patid == patid) & (glaucoma$eye == eye), ]#
dat <- dat[order(dat$field), ]#
dat <- dat[!dat$point_ID %in% blind_spot, ] # remove blind spot 26, 35#
temp <- cbind(dat$sens_raw, dat$day)#
#
###Data objects#
n <- length(unique(dat$point_ID))#
T <- length(unique(dat$field))#
N <- n * T#
YStar <- matrix(temp[, 1], nrow = N, ncol = 1) #no scale#
YStarWide <- matrix(YStar, ncol = T)#
t <- matrix(unique(dat$day), nrow = T, ncol = 1) / 365 #scale to years#
t <- ((t - mean(t)) / sd(t)) * 0.5 # center at zero with sd of 0.5.#
#
###Matrix objects#
OneT <- matrix(1, nrow = T,ncol = 1)#
Eyen <- diag(n)#
Onen <- matrix(1, nrow = n, ncol = 1)#
OneN <- matrix(1, nrow = N, ncol = 1)#
EyeN <- diag(N)#
ZAlpha <- OneN#
ZBeta <- kronecker(t, Onen)#
ZDelta <- kronecker(OneT, Eyen)#
ZEta <- kronecker(t, Eyen)#
#
###Initial values#
SigmaEpsilon2 <- #
Tau2 <- 5#
Kappa2 <- 1#
Alpha <- 23#
Beta <- 0#
YStar1 <- YStarWide[, 1]#
Delta <- YStar1 - mean(YStar1)#
Eta <- matrix(0, nrow = n, ncol = 1)#
#
###Adjacency matrix#
source("/Users/Sam/Documents/Sam/School/Dissertation/Models/Likelihoods/Glaucoma_R_Functions/adjacency.R")#
# source("/home/users/berchuck/Dissertation/Files/adjacency.R")#
W <- adj.mat(Brigid)#
W <- W[-blind_spot, -blind_spot] #Remove blind spot 26,35#
Dw <- diag(apply(W, 1, sum))#
WStar <- Dw - W#
#
###Hyperparameters#
AlphaSigma <- 5.6#
BetaSigma <- 43.2#
MuAlpha <- 24#
SigmaAlpha2 <- 18^2 #
AlphaBeta <- 0 #
BetaBeta <- 5#
AlphaTau <- AlphaKappa <- 1 / 2#
BetaTau <- BetaKappa <- 0.01 / 2#
#
###Tobit information#
Tobit <- YStar == 0#
NBelow <- sum(Tobit)#
WhichTobit <- which(Tobit)#
#
###MCMC objects#
NSims <- 100000#
NBurn <- 12000#
NThin <- 40#
NTotal <- NBurn + NSims#
WhichKeep <- NBurn + (1:(NSims / NThin)) * NThin#
NKeep <- length(WhichKeep)#
#
###Pilot adaptation objects#
NPilot <- 24 #
WhichPilotAdapt <- (1:NPilot ) * NBurn / NPilot#
PilotAdaptDenominator <- WhichPilotAdapt[1]#
#
###Metropolis object#
TuningParameter <- 0.001#
Acceptance <- 0#
#
###Create storage object#
RawSamples <- matrix(nrow = 6 + 2 * n, ncol = NKeep)#
#
###Time MCMC sampler#
Begin <- Sys.time()#
#
###Begin sampler#
for (s in 1:NTotal) {#
	###Tobit step#
	if (NBelow == 0) Y <- YStar#
	if (NBelow != 0) {#
		Y <- YStar#
		MuTobit <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Y[Tobit] <- rtnorm(NBelow, MuTobit[NBelow], sqrt(SigmaEpsilon2), upper = 0)#
	}#
	###Alpha full conditional#
	VarAlpha <- chol2inv(chol(t(ZAlpha) %*% ZAlpha / SigmaEpsilon2 + 1 / SigmaAlpha2))#
	Gamma <- Y - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	MeanAlpha <- VarAlpha %*% (t(ZAlpha) %*% Gamma / SigmaEpsilon2 + MuAlpha / SigmaAlpha2)#
	Alpha <- rnorm(1, MeanAlpha, sqrt(VarAlpha))#
	###Delta full conditional#
	CovDelta <- chol2inv(chol(t(ZDelta) %*% ZDelta / SigmaEpsilon2 + WStar / Tau2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZEta %*% Eta#
	MeanDelta <- CovDelta %*% (t(ZDelta) %*% Gamma / SigmaEpsilon2)#
	Delta <- matrix(rmvnorm(1, MeanDelta, CovDelta), ncol = 1)#
	Delta <- (Delta - mean(Delta)) # sum to zero constraint#
#
	###Eta full conditional#
	CovEta <- chol2inv(chol(t(ZEta) %*% ZEta / SigmaEpsilon2 + WStar / Kappa2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta#
	MeanEta <- CovEta %*% (t(ZEta) %*% Gamma / SigmaEpsilon2)#
	Eta <- matrix(rmvnorm(1, MeanEta, CovEta), ncol = 1)#
	Eta <- (Eta - mean(Eta)) # sum to zero constraint#
#
	###SigmaEpsilon2 full conditional#
	AlphaSigmaNew <- AlphaSigma + N / 2#
	Residuals <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	BetaSigmaNew <- BetaSigma + t(Residuals) %*% Residuals / 2#
	SigmaEpsilon2 <- rigamma(1, AlphaSigmaNew, BetaSigmaNew)#
#
	###Tau2 full conditional#
	AlphaTauNew <- AlphaTau + (n - 1) / 2 #
	BetaTauNew <- BetaTau + t(Delta) %*% WStar %*% Delta / 2#
	Tau2 <- rigamma(1, AlphaTauNew, BetaTauNew)#
	if ((s <= NBurn) & (Tau2 >= 20)) Tau2 <- 20 # Tau2 is truncated above at 20 in the burn-in phase.#
#
	##Kappa2 Full Conditional#
	AlphaKappaNew <- AlphaKappa + (n - 1) / 2#
	BetaKappaNew <- BetaKappa + t(Eta) %*% WStar %*% Eta / 2#
	Kappa2 <- rigamma(1, AlphaKappaNew, BetaKappaNew)#
#
	###Metropolis step for Beta#
		###Sample proposal	#
    	ProposalBeta <- rnorm(1, Beta, sqrt(TuningParameter))#
#
    	###Likelihood Component#
    	MeanProposal <- Alpha * ZAlpha + ProposalBeta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Mean <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Component1A <- lndMvn(Y, MeanProposal, SigmaEpsilon2 * EyeN)#
    	Component1B <- lndMvn(Y, Mean, SigmaEpsilon2 * EyeN)#
        Component1 <- Component1A - Component1B#
#
    	###Prior components    	#
    	Component2A <- dcauchy(ProposalBeta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2B <- dcauchy(Beta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2 <- Component2A - Component2B#
#
    	###Log acceptance ratio#
    	LogR <- Component1 + Component2#
#
   	 	###Metropolis update#
    	RandU <- runif(1)#
    	if (log(RandU) < LogR) {#
#
     		###Keep count of acceptances#
      		Acceptance <- Acceptance + 1#
#
      		###Update parameters output#
      		Beta <- ProposalBeta#
#
    	}#
	##Compute Deviance for DIC#
	if (s %in% WhichKeep) {#
    	MeanDev <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Dev1 <- -2 * sum(dnorm(YStar[!Tobit], MeanDev[!Tobit], sqrt(SigmaEpsilon2), log = TRUE))#
		Dev2 <- -2 * sum(pnorm(MeanDev[Tobit] / sqrt(SigmaEpsilon2), 0, 1, lower.tail = FALSE, log.p = TRUE))	#
		Dev <- Dev1 + Dev2	#
	}#
#
    ###Pilot adaptation#
    if (s %in% WhichPilotAdapt) {#
    	AcceptancePct <- Acceptance / PilotAdaptDenominator#
    	if (AcceptancePct >= 0.90) TuningParameter <- TuningParameter * 1.3#
		if ((AcceptancePct >= 0.75) & (AcceptancePct < 0.90)) TuningParameter <- TuningParameter * 1.2#
		if ((AcceptancePct >= 0.45) & (AcceptancePct < 0.75)) TuningParameter <- TuningParameter * 1.1#
		if ((AcceptancePct <= 0.25) & (AcceptancePct > 0.15)) TuningParameter <- TuningParameter * 0.9#
		if ((AcceptancePct <= 0.15) & (AcceptancePct > 0.10)) TuningParameter <- TuningParameter * 0.8#
		if (AcceptancePct <= 0.10) TuningParameter <- TuningParameter * 0.7#
		Acceptance <- 0#
    }#
	###Store samples#
	if (s %in% WhichKeep) {#
 		Save <- c(Alpha, Beta, SigmaEpsilon2, Tau2, Kappa2, Dev, as.numeric(Delta), as.numeric(Eta))#
  		RawSamples[ , which(s == WhichKeep)] <- Save#
	}#
	##Verbose#
	# if (s %in% WhichKeep) {#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / (NTotal - NBurn)) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
	# }#
#
	###Time MCMC Sampler#
	if (s == NTotal) {#
		After <- Sys.time()#
		RunTime <- After - Begin#
		cat(paste0("Run Time: ", round(RunTime, digits = 2), " ", attr(RunTime, "unit")))#
		cat("######################################################################################################\n")#
#
	}#
###End MCMC Sampler	#
}
Acceptance
NBurn
s
###Set seed#
set.seed(54)#
#
###Load packages#
suppressMessages(library(mvtnorm)) #multivariate normal#
suppressMessages(library(pscl)) #inverse gamma#
suppressMessages(library(msm)) #truncated normal#
suppressMessages(library(coda)) #mcmc#
suppressMessages(library(bayesm)) #lndMvn#
#
###Define the blind spot#
blind_spot <- c(26, 35)#
###Read in Data for ID = 101, Eye = Left#
glaucoma <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/all_josh.csv")#
# glaucoma <- read.csv("/home/users/berchuck/Dissertation/Files/all_josh.csv")#
dat <- glaucoma[(glaucoma$patid == patid) & (glaucoma$eye == eye), ]#
dat <- dat[order(dat$field), ]#
dat <- dat[!dat$point_ID %in% blind_spot, ] # remove blind spot 26, 35#
temp <- cbind(dat$sens_raw, dat$day)#
#
###Data objects#
n <- length(unique(dat$point_ID))#
T <- length(unique(dat$field))#
N <- n * T#
YStar <- matrix(temp[, 1], nrow = N, ncol = 1) #no scale#
YStarWide <- matrix(YStar, ncol = T)#
t <- matrix(unique(dat$day), nrow = T, ncol = 1) / 365 #scale to years#
t <- ((t - mean(t)) / sd(t)) * 0.5 # center at zero with sd of 0.5.#
#
###Matrix objects#
OneT <- matrix(1, nrow = T,ncol = 1)#
Eyen <- diag(n)#
Onen <- matrix(1, nrow = n, ncol = 1)#
OneN <- matrix(1, nrow = N, ncol = 1)#
EyeN <- diag(N)#
ZAlpha <- OneN#
ZBeta <- kronecker(t, Onen)#
ZDelta <- kronecker(OneT, Eyen)#
ZEta <- kronecker(t, Eyen)#
#
###Initial values#
SigmaEpsilon2 <- #
Tau2 <- 5#
Kappa2 <- 1#
Alpha <- 23#
Beta <- 0#
YStar1 <- YStarWide[, 1]#
Delta <- YStar1 - mean(YStar1)#
Eta <- matrix(0, nrow = n, ncol = 1)#
#
###Adjacency matrix#
source("/Users/Sam/Documents/Sam/School/Dissertation/Models/Likelihoods/Glaucoma_R_Functions/adjacency.R")#
# source("/home/users/berchuck/Dissertation/Files/adjacency.R")#
W <- adj.mat(Brigid)#
W <- W[-blind_spot, -blind_spot] #Remove blind spot 26,35#
Dw <- diag(apply(W, 1, sum))#
WStar <- Dw - W#
#
###Hyperparameters#
AlphaSigma <- 5.6#
BetaSigma <- 43.2#
MuAlpha <- 24#
SigmaAlpha2 <- 18^2 #
AlphaBeta <- 0 #
BetaBeta <- 5#
AlphaTau <- AlphaKappa <- 1 / 2#
BetaTau <- BetaKappa <- 0.01 / 2#
#
###Tobit information#
Tobit <- YStar == 0#
NBelow <- sum(Tobit)#
WhichTobit <- which(Tobit)#
#
###MCMC objects#
NSims <- 100000#
NBurn <- 12000#
NThin <- 40#
NTotal <- NBurn + NSims#
WhichKeep <- NBurn + (1:(NSims / NThin)) * NThin#
NKeep <- length(WhichKeep)#
#
###Pilot adaptation objects#
NPilot <- 24 #
WhichPilotAdapt <- (1:NPilot ) * NBurn / NPilot#
PilotAdaptDenominator <- WhichPilotAdapt[1]#
#
###Metropolis object#
TuningParameter <- 0.001#
Acceptance <- 0#
#
###Create storage object#
RawSamples <- matrix(nrow = 6 + 2 * n, ncol = NKeep)#
#
###Time MCMC sampler#
Begin <- Sys.time()#
#
###Begin sampler#
for (s in 1:NTotal) {#
	###Tobit step#
	if (NBelow == 0) Y <- YStar#
	if (NBelow != 0) {#
		Y <- YStar#
		MuTobit <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Y[Tobit] <- rtnorm(NBelow, MuTobit[NBelow], sqrt(SigmaEpsilon2), upper = 0)#
	}#
	###Alpha full conditional#
	VarAlpha <- chol2inv(chol(t(ZAlpha) %*% ZAlpha / SigmaEpsilon2 + 1 / SigmaAlpha2))#
	Gamma <- Y - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	MeanAlpha <- VarAlpha %*% (t(ZAlpha) %*% Gamma / SigmaEpsilon2 + MuAlpha / SigmaAlpha2)#
	Alpha <- rnorm(1, MeanAlpha, sqrt(VarAlpha))#
	###Delta full conditional#
	CovDelta <- chol2inv(chol(t(ZDelta) %*% ZDelta / SigmaEpsilon2 + WStar / Tau2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZEta %*% Eta#
	MeanDelta <- CovDelta %*% (t(ZDelta) %*% Gamma / SigmaEpsilon2)#
	Delta <- matrix(rmvnorm(1, MeanDelta, CovDelta), ncol = 1)#
	Delta <- (Delta - mean(Delta)) # sum to zero constraint#
#
	###Eta full conditional#
	CovEta <- chol2inv(chol(t(ZEta) %*% ZEta / SigmaEpsilon2 + WStar / Kappa2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta#
	MeanEta <- CovEta %*% (t(ZEta) %*% Gamma / SigmaEpsilon2)#
	Eta <- matrix(rmvnorm(1, MeanEta, CovEta), ncol = 1)#
	Eta <- (Eta - mean(Eta)) # sum to zero constraint#
#
	###SigmaEpsilon2 full conditional#
	AlphaSigmaNew <- AlphaSigma + N / 2#
	Residuals <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	BetaSigmaNew <- BetaSigma + t(Residuals) %*% Residuals / 2#
	SigmaEpsilon2 <- rigamma(1, AlphaSigmaNew, BetaSigmaNew)#
#
	###Tau2 full conditional#
	AlphaTauNew <- AlphaTau + (n - 1) / 2 #
	BetaTauNew <- BetaTau + t(Delta) %*% WStar %*% Delta / 2#
	Tau2 <- rigamma(1, AlphaTauNew, BetaTauNew)#
	if ((s <= NBurn) & (Tau2 >= 20)) Tau2 <- 20 # Tau2 is truncated above at 20 in the burn-in phase.#
#
	##Kappa2 Full Conditional#
	AlphaKappaNew <- AlphaKappa + (n - 1) / 2#
	BetaKappaNew <- BetaKappa + t(Eta) %*% WStar %*% Eta / 2#
	Kappa2 <- rigamma(1, AlphaKappaNew, BetaKappaNew)#
#
	###Metropolis step for Beta#
		###Sample proposal	#
    	ProposalBeta <- rnorm(1, Beta, sqrt(TuningParameter))#
#
    	###Likelihood Component#
    	MeanProposal <- Alpha * ZAlpha + ProposalBeta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Mean <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Component1A <- lndMvn(Y, MeanProposal, SigmaEpsilon2 * EyeN)#
    	Component1B <- lndMvn(Y, Mean, SigmaEpsilon2 * EyeN)#
        Component1 <- Component1A - Component1B#
#
    	###Prior components    	#
    	Component2A <- dcauchy(ProposalBeta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2B <- dcauchy(Beta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2 <- Component2A - Component2B#
#
    	###Log acceptance ratio#
    	LogR <- Component1 + Component2#
#
   	 	###Metropolis update#
    	RandU <- runif(1)#
    	if (log(RandU) < LogR) {#
#
     		###Keep count of acceptances#
      		Acceptance <- Acceptance + 1#
#
      		###Update parameters output#
      		Beta <- ProposalBeta#
#
    	}#
	##Compute Deviance for DIC#
	if (s %in% WhichKeep) {#
    	MeanDev <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Dev1 <- -2 * sum(dnorm(YStar[!Tobit], MeanDev[!Tobit], sqrt(SigmaEpsilon2), log = TRUE))#
		Dev2 <- -2 * sum(pnorm(MeanDev[Tobit] / sqrt(SigmaEpsilon2), 0, 1, lower.tail = FALSE, log.p = TRUE))	#
		Dev <- Dev1 + Dev2	#
	}#
#
    ###Pilot adaptation#
    if (s %in% WhichPilotAdapt) {#
    	AcceptancePct <- Acceptance / PilotAdaptDenominator#
    	if (AcceptancePct >= 0.90) TuningParameter <- TuningParameter * 1.3#
		if ((AcceptancePct >= 0.75) & (AcceptancePct < 0.90)) TuningParameter <- TuningParameter * 1.2#
		if ((AcceptancePct >= 0.45) & (AcceptancePct < 0.75)) TuningParameter <- TuningParameter * 1.1#
		if ((AcceptancePct <= 0.25) & (AcceptancePct > 0.15)) TuningParameter <- TuningParameter * 0.9#
		if ((AcceptancePct <= 0.15) & (AcceptancePct > 0.10)) TuningParameter <- TuningParameter * 0.8#
		if (AcceptancePct <= 0.10) TuningParameter <- TuningParameter * 0.7#
		Acceptance <- 0#
    }#
	###Store samples#
	if (s %in% WhichKeep) {#
 		Save <- c(Alpha, Beta, SigmaEpsilon2, Tau2, Kappa2, Dev, as.numeric(Delta), as.numeric(Eta))#
  		RawSamples[ , which(s == WhichKeep)] <- Save#
	}#
	##Verbose#
	if (s %in% WhichKeep) {#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / NBurn) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
	}#
	if (s %in% WhichKeep) {#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / (NTotal - NBurn)) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
	}#
#
	###Time MCMC Sampler#
	if (s == NTotal) {#
		After <- Sys.time()#
		RunTime <- After - Begin#
		cat(paste0("Run Time: ", round(RunTime, digits = 2), " ", attr(RunTime, "unit")))#
		cat("######################################################################################################\n")#
#
	}#
###End MCMC Sampler	#
}
###Set seed#
set.seed(54)#
#
###Load packages#
suppressMessages(library(mvtnorm)) #multivariate normal#
suppressMessages(library(pscl)) #inverse gamma#
suppressMessages(library(msm)) #truncated normal#
suppressMessages(library(coda)) #mcmc#
suppressMessages(library(bayesm)) #lndMvn#
#
###Define the blind spot#
blind_spot <- c(26, 35)#
###Read in Data for ID = 101, Eye = Left#
glaucoma <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/all_josh.csv")#
# glaucoma <- read.csv("/home/users/berchuck/Dissertation/Files/all_josh.csv")#
dat <- glaucoma[(glaucoma$patid == patid) & (glaucoma$eye == eye), ]#
dat <- dat[order(dat$field), ]#
dat <- dat[!dat$point_ID %in% blind_spot, ] # remove blind spot 26, 35#
temp <- cbind(dat$sens_raw, dat$day)#
#
###Data objects#
n <- length(unique(dat$point_ID))#
T <- length(unique(dat$field))#
N <- n * T#
YStar <- matrix(temp[, 1], nrow = N, ncol = 1) #no scale#
YStarWide <- matrix(YStar, ncol = T)#
t <- matrix(unique(dat$day), nrow = T, ncol = 1) / 365 #scale to years#
t <- ((t - mean(t)) / sd(t)) * 0.5 # center at zero with sd of 0.5.#
#
###Matrix objects#
OneT <- matrix(1, nrow = T,ncol = 1)#
Eyen <- diag(n)#
Onen <- matrix(1, nrow = n, ncol = 1)#
OneN <- matrix(1, nrow = N, ncol = 1)#
EyeN <- diag(N)#
ZAlpha <- OneN#
ZBeta <- kronecker(t, Onen)#
ZDelta <- kronecker(OneT, Eyen)#
ZEta <- kronecker(t, Eyen)#
#
###Initial values#
SigmaEpsilon2 <- #
Tau2 <- 5#
Kappa2 <- 1#
Alpha <- 23#
Beta <- 0#
YStar1 <- YStarWide[, 1]#
Delta <- YStar1 - mean(YStar1)#
Eta <- matrix(0, nrow = n, ncol = 1)#
#
###Adjacency matrix#
source("/Users/Sam/Documents/Sam/School/Dissertation/Models/Likelihoods/Glaucoma_R_Functions/adjacency.R")#
# source("/home/users/berchuck/Dissertation/Files/adjacency.R")#
W <- adj.mat(Brigid)#
W <- W[-blind_spot, -blind_spot] #Remove blind spot 26,35#
Dw <- diag(apply(W, 1, sum))#
WStar <- Dw - W#
#
###Hyperparameters#
AlphaSigma <- 5.6#
BetaSigma <- 43.2#
MuAlpha <- 24#
SigmaAlpha2 <- 18^2 #
AlphaBeta <- 0 #
BetaBeta <- 5#
AlphaTau <- AlphaKappa <- 1 / 2#
BetaTau <- BetaKappa <- 0.01 / 2#
#
###Tobit information#
Tobit <- YStar == 0#
NBelow <- sum(Tobit)#
WhichTobit <- which(Tobit)#
#
###MCMC objects#
NSims <- 100000#
NBurn <- 12000#
NThin <- 40#
NTotal <- NBurn + NSims#
WhichKeep <- NBurn + (1:(NSims / NThin)) * NThin#
NKeep <- length(WhichKeep)#
#
###Pilot adaptation objects#
NPilot <- 24 #
WhichPilotAdapt <- (1:NPilot ) * NBurn / NPilot#
PilotAdaptDenominator <- WhichPilotAdapt[1]#
#
###Metropolis object#
TuningParameter <- 0.001#
Acceptance <- 0#
#
###Create storage object#
RawSamples <- matrix(nrow = 6 + 2 * n, ncol = NKeep)#
#
###Time MCMC sampler#
Begin <- Sys.time()#
#
###Begin sampler#
for (s in 1:NTotal) {#
	###Tobit step#
	if (NBelow == 0) Y <- YStar#
	if (NBelow != 0) {#
		Y <- YStar#
		MuTobit <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Y[Tobit] <- rtnorm(NBelow, MuTobit[NBelow], sqrt(SigmaEpsilon2), upper = 0)#
	}#
	###Alpha full conditional#
	VarAlpha <- chol2inv(chol(t(ZAlpha) %*% ZAlpha / SigmaEpsilon2 + 1 / SigmaAlpha2))#
	Gamma <- Y - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	MeanAlpha <- VarAlpha %*% (t(ZAlpha) %*% Gamma / SigmaEpsilon2 + MuAlpha / SigmaAlpha2)#
	Alpha <- rnorm(1, MeanAlpha, sqrt(VarAlpha))#
	###Delta full conditional#
	CovDelta <- chol2inv(chol(t(ZDelta) %*% ZDelta / SigmaEpsilon2 + WStar / Tau2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZEta %*% Eta#
	MeanDelta <- CovDelta %*% (t(ZDelta) %*% Gamma / SigmaEpsilon2)#
	Delta <- matrix(rmvnorm(1, MeanDelta, CovDelta), ncol = 1)#
	Delta <- (Delta - mean(Delta)) # sum to zero constraint#
#
	###Eta full conditional#
	CovEta <- chol2inv(chol(t(ZEta) %*% ZEta / SigmaEpsilon2 + WStar / Kappa2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta#
	MeanEta <- CovEta %*% (t(ZEta) %*% Gamma / SigmaEpsilon2)#
	Eta <- matrix(rmvnorm(1, MeanEta, CovEta), ncol = 1)#
	Eta <- (Eta - mean(Eta)) # sum to zero constraint#
#
	###SigmaEpsilon2 full conditional#
	AlphaSigmaNew <- AlphaSigma + N / 2#
	Residuals <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	BetaSigmaNew <- BetaSigma + t(Residuals) %*% Residuals / 2#
	SigmaEpsilon2 <- rigamma(1, AlphaSigmaNew, BetaSigmaNew)#
#
	###Tau2 full conditional#
	AlphaTauNew <- AlphaTau + (n - 1) / 2 #
	BetaTauNew <- BetaTau + t(Delta) %*% WStar %*% Delta / 2#
	Tau2 <- rigamma(1, AlphaTauNew, BetaTauNew)#
	if ((s <= NBurn) & (Tau2 >= 20)) Tau2 <- 20 # Tau2 is truncated above at 20 in the burn-in phase.#
#
	##Kappa2 Full Conditional#
	AlphaKappaNew <- AlphaKappa + (n - 1) / 2#
	BetaKappaNew <- BetaKappa + t(Eta) %*% WStar %*% Eta / 2#
	Kappa2 <- rigamma(1, AlphaKappaNew, BetaKappaNew)#
#
	###Metropolis step for Beta#
		###Sample proposal	#
    	ProposalBeta <- rnorm(1, Beta, sqrt(TuningParameter))#
#
    	###Likelihood Component#
    	MeanProposal <- Alpha * ZAlpha + ProposalBeta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Mean <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Component1A <- lndMvn(Y, MeanProposal, SigmaEpsilon2 * EyeN)#
    	Component1B <- lndMvn(Y, Mean, SigmaEpsilon2 * EyeN)#
        Component1 <- Component1A - Component1B#
#
    	###Prior components    	#
    	Component2A <- dcauchy(ProposalBeta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2B <- dcauchy(Beta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2 <- Component2A - Component2B#
#
    	###Log acceptance ratio#
    	LogR <- Component1 + Component2#
#
   	 	###Metropolis update#
    	RandU <- runif(1)#
    	if (log(RandU) < LogR) {#
#
     		###Keep count of acceptances#
      		Acceptance <- Acceptance + 1#
#
      		###Update parameters output#
      		Beta <- ProposalBeta#
#
    	}#
	##Compute Deviance for DIC#
	if (s %in% WhichKeep) {#
    	MeanDev <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Dev1 <- -2 * sum(dnorm(YStar[!Tobit], MeanDev[!Tobit], sqrt(SigmaEpsilon2), log = TRUE))#
		Dev2 <- -2 * sum(pnorm(MeanDev[Tobit] / sqrt(SigmaEpsilon2), 0, 1, lower.tail = FALSE, log.p = TRUE))	#
		Dev <- Dev1 + Dev2	#
	}#
#
    ###Pilot adaptation#
    if (s %in% WhichPilotAdapt) {#
    	AcceptancePct <- Acceptance / PilotAdaptDenominator#
    	if (AcceptancePct >= 0.90) TuningParameter <- TuningParameter * 1.3#
		if ((AcceptancePct >= 0.75) & (AcceptancePct < 0.90)) TuningParameter <- TuningParameter * 1.2#
		if ((AcceptancePct >= 0.45) & (AcceptancePct < 0.75)) TuningParameter <- TuningParameter * 1.1#
		if ((AcceptancePct <= 0.25) & (AcceptancePct > 0.15)) TuningParameter <- TuningParameter * 0.9#
		if ((AcceptancePct <= 0.15) & (AcceptancePct > 0.10)) TuningParameter <- TuningParameter * 0.8#
		if (AcceptancePct <= 0.10) TuningParameter <- TuningParameter * 0.7#
		Acceptance <- 0#
    }#
	###Store samples#
	if (s %in% WhichKeep) {#
 		Save <- c(Alpha, Beta, SigmaEpsilon2, Tau2, Kappa2, Dev, as.numeric(Delta), as.numeric(Eta))#
  		RawSamples[ , which(s == WhichKeep)] <- Save#
	}#
	##Verbose#
	if (s %in% WhichPilotAdapt) {#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / NBurn) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
	}#
	if (s %in% WhichKeep) {#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / (NTotal - NBurn)) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
	}#
#
	###Time MCMC Sampler#
	if (s == NTotal) {#
		After <- Sys.time()#
		RunTime <- After - Begin#
		cat(paste0("Run Time: ", round(RunTime, digits = 2), " ", attr(RunTime, "unit")))#
		cat("######################################################################################################\n")#
#
	}#
###End MCMC Sampler	#
}
###Set seed#
set.seed(54)#
#
###Load packages#
suppressMessages(library(mvtnorm)) #multivariate normal#
suppressMessages(library(pscl)) #inverse gamma#
suppressMessages(library(msm)) #truncated normal#
suppressMessages(library(coda)) #mcmc#
suppressMessages(library(bayesm)) #lndMvn#
#
###Define the blind spot#
blind_spot <- c(26, 35)#
###Read in Data for ID = 101, Eye = Left#
glaucoma <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/all_josh.csv")#
# glaucoma <- read.csv("/home/users/berchuck/Dissertation/Files/all_josh.csv")#
dat <- glaucoma[(glaucoma$patid == patid) & (glaucoma$eye == eye), ]#
dat <- dat[order(dat$field), ]#
dat <- dat[!dat$point_ID %in% blind_spot, ] # remove blind spot 26, 35#
temp <- cbind(dat$sens_raw, dat$day)#
#
###Data objects#
n <- length(unique(dat$point_ID))#
T <- length(unique(dat$field))#
N <- n * T#
YStar <- matrix(temp[, 1], nrow = N, ncol = 1) #no scale#
YStarWide <- matrix(YStar, ncol = T)#
t <- matrix(unique(dat$day), nrow = T, ncol = 1) / 365 #scale to years#
t <- ((t - mean(t)) / sd(t)) * 0.5 # center at zero with sd of 0.5.#
#
###Matrix objects#
OneT <- matrix(1, nrow = T,ncol = 1)#
Eyen <- diag(n)#
Onen <- matrix(1, nrow = n, ncol = 1)#
OneN <- matrix(1, nrow = N, ncol = 1)#
EyeN <- diag(N)#
ZAlpha <- OneN#
ZBeta <- kronecker(t, Onen)#
ZDelta <- kronecker(OneT, Eyen)#
ZEta <- kronecker(t, Eyen)#
#
###Initial values#
SigmaEpsilon2 <- #
Tau2 <- 5#
Kappa2 <- 1#
Alpha <- 23#
Beta <- 0#
YStar1 <- YStarWide[, 1]#
Delta <- YStar1 - mean(YStar1)#
Eta <- matrix(0, nrow = n, ncol = 1)#
#
###Adjacency matrix#
source("/Users/Sam/Documents/Sam/School/Dissertation/Models/Likelihoods/Glaucoma_R_Functions/adjacency.R")#
# source("/home/users/berchuck/Dissertation/Files/adjacency.R")#
W <- adj.mat(Brigid)#
W <- W[-blind_spot, -blind_spot] #Remove blind spot 26,35#
Dw <- diag(apply(W, 1, sum))#
WStar <- Dw - W#
#
###Hyperparameters#
AlphaSigma <- 5.6#
BetaSigma <- 43.2#
MuAlpha <- 24#
SigmaAlpha2 <- 18^2 #
AlphaBeta <- 0 #
BetaBeta <- 5#
AlphaTau <- AlphaKappa <- 1 / 2#
BetaTau <- BetaKappa <- 0.01 / 2#
#
###Tobit information#
Tobit <- YStar == 0#
NBelow <- sum(Tobit)#
WhichTobit <- which(Tobit)#
#
###MCMC objects#
NSims <- 100000#
NBurn <- 12000#
NThin <- 40#
NTotal <- NBurn + NSims#
WhichKeep <- NBurn + (1:(NSims / NThin)) * NThin#
NKeep <- length(WhichKeep)#
#
###Pilot adaptation objects#
NPilot <- 24 #
WhichPilotAdapt <- (1:NPilot ) * NBurn / NPilot#
PilotAdaptDenominator <- WhichPilotAdapt[1]#
#
###Metropolis object#
TuningParameter <- 0.001#
Acceptance <- 0#
#
###Create storage object#
RawSamples <- matrix(nrow = 6 + 2 * n, ncol = NKeep)#
#
###Time MCMC sampler#
Begin <- Sys.time()#
#
###Begin sampler#
for (s in 1:NTotal) {#
	###Tobit step#
	if (NBelow == 0) Y <- YStar#
	if (NBelow != 0) {#
		Y <- YStar#
		MuTobit <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Y[Tobit] <- rtnorm(NBelow, MuTobit[NBelow], sqrt(SigmaEpsilon2), upper = 0)#
	}#
	###Alpha full conditional#
	VarAlpha <- chol2inv(chol(t(ZAlpha) %*% ZAlpha / SigmaEpsilon2 + 1 / SigmaAlpha2))#
	Gamma <- Y - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	MeanAlpha <- VarAlpha %*% (t(ZAlpha) %*% Gamma / SigmaEpsilon2 + MuAlpha / SigmaAlpha2)#
	Alpha <- rnorm(1, MeanAlpha, sqrt(VarAlpha))#
	###Delta full conditional#
	CovDelta <- chol2inv(chol(t(ZDelta) %*% ZDelta / SigmaEpsilon2 + WStar / Tau2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZEta %*% Eta#
	MeanDelta <- CovDelta %*% (t(ZDelta) %*% Gamma / SigmaEpsilon2)#
	Delta <- matrix(rmvnorm(1, MeanDelta, CovDelta), ncol = 1)#
	Delta <- (Delta - mean(Delta)) # sum to zero constraint#
#
	###Eta full conditional#
	CovEta <- chol2inv(chol(t(ZEta) %*% ZEta / SigmaEpsilon2 + WStar / Kappa2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta#
	MeanEta <- CovEta %*% (t(ZEta) %*% Gamma / SigmaEpsilon2)#
	Eta <- matrix(rmvnorm(1, MeanEta, CovEta), ncol = 1)#
	Eta <- (Eta - mean(Eta)) # sum to zero constraint#
#
	###SigmaEpsilon2 full conditional#
	AlphaSigmaNew <- AlphaSigma + N / 2#
	Residuals <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	BetaSigmaNew <- BetaSigma + t(Residuals) %*% Residuals / 2#
	SigmaEpsilon2 <- rigamma(1, AlphaSigmaNew, BetaSigmaNew)#
#
	###Tau2 full conditional#
	AlphaTauNew <- AlphaTau + (n - 1) / 2 #
	BetaTauNew <- BetaTau + t(Delta) %*% WStar %*% Delta / 2#
	Tau2 <- rigamma(1, AlphaTauNew, BetaTauNew)#
	if ((s <= NBurn) & (Tau2 >= 20)) Tau2 <- 20 # Tau2 is truncated above at 20 in the burn-in phase.#
#
	##Kappa2 Full Conditional#
	AlphaKappaNew <- AlphaKappa + (n - 1) / 2#
	BetaKappaNew <- BetaKappa + t(Eta) %*% WStar %*% Eta / 2#
	Kappa2 <- rigamma(1, AlphaKappaNew, BetaKappaNew)#
#
	###Metropolis step for Beta#
		###Sample proposal	#
    	ProposalBeta <- rnorm(1, Beta, sqrt(TuningParameter))#
#
    	###Likelihood Component#
    	MeanProposal <- Alpha * ZAlpha + ProposalBeta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Mean <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Component1A <- lndMvn(Y, MeanProposal, SigmaEpsilon2 * EyeN)#
    	Component1B <- lndMvn(Y, Mean, SigmaEpsilon2 * EyeN)#
        Component1 <- Component1A - Component1B#
#
    	###Prior components    	#
    	Component2A <- dcauchy(ProposalBeta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2B <- dcauchy(Beta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2 <- Component2A - Component2B#
#
    	###Log acceptance ratio#
    	LogR <- Component1 + Component2#
#
   	 	###Metropolis update#
    	RandU <- runif(1)#
    	if (log(RandU) < LogR) {#
#
     		###Keep count of acceptances#
      		Acceptance <- Acceptance + 1#
#
      		###Update parameters output#
      		Beta <- ProposalBeta#
#
    	}#
	##Compute Deviance for DIC#
	if (s %in% WhichKeep) {#
    	MeanDev <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Dev1 <- -2 * sum(dnorm(YStar[!Tobit], MeanDev[!Tobit], sqrt(SigmaEpsilon2), log = TRUE))#
		Dev2 <- -2 * sum(pnorm(MeanDev[Tobit] / sqrt(SigmaEpsilon2), 0, 1, lower.tail = FALSE, log.p = TRUE))	#
		Dev <- Dev1 + Dev2	#
	}#
#
    ###Pilot adaptation#
    if (s %in% WhichPilotAdapt) {#
    	AcceptancePct <- Acceptance / PilotAdaptDenominator#
    	if (AcceptancePct >= 0.90) TuningParameter <- TuningParameter * 1.3#
		if ((AcceptancePct >= 0.75) & (AcceptancePct < 0.90)) TuningParameter <- TuningParameter * 1.2#
		if ((AcceptancePct >= 0.45) & (AcceptancePct < 0.75)) TuningParameter <- TuningParameter * 1.1#
		if ((AcceptancePct <= 0.25) & (AcceptancePct > 0.15)) TuningParameter <- TuningParameter * 0.9#
		if ((AcceptancePct <= 0.15) & (AcceptancePct > 0.10)) TuningParameter <- TuningParameter * 0.8#
		if (AcceptancePct <= 0.10) TuningParameter <- TuningParameter * 0.7#
		cat("######################################################################################################\n")#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / PilotAdaptDenominator) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
		Acceptance <- 0#
    }#
	###Store samples#
	if (s %in% WhichKeep) {#
 		Save <- c(Alpha, Beta, SigmaEpsilon2, Tau2, Kappa2, Dev, as.numeric(Delta), as.numeric(Eta))#
  		RawSamples[ , which(s == WhichKeep)] <- Save#
	}#
	##Verbose#
	if (s %in% WhichPilotAdapt) {#
	}#
	if (s %in% WhichKeep) {#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / (NTotal - NBurn)) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
	}#
#
	###Time MCMC Sampler#
	if (s == NTotal) {#
		After <- Sys.time()#
		RunTime <- After - Begin#
		cat(paste0("Run Time: ", round(RunTime, digits = 2), " ", attr(RunTime, "unit")))#
		cat("######################################################################################################\n")#
#
	}#
###End MCMC Sampler	#
}
Acceptance
PilotAdaptDenominator
round(Acceptance / PilotAdaptDenominator) * 100
Acceptance
PilotAdaptDenominator
round(Acceptance / PilotAdaptDenominator)
round(Acceptance/PilotAdaptDenominator)
round(Acceptance/PilotAdaptDenominator,2)
###Set seed#
set.seed(54)#
#
###Load packages#
suppressMessages(library(mvtnorm)) #multivariate normal#
suppressMessages(library(pscl)) #inverse gamma#
suppressMessages(library(msm)) #truncated normal#
suppressMessages(library(coda)) #mcmc#
suppressMessages(library(bayesm)) #lndMvn#
#
###Define the blind spot#
blind_spot <- c(26, 35)#
###Read in Data for ID = 101, Eye = Left#
glaucoma <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/all_josh.csv")#
# glaucoma <- read.csv("/home/users/berchuck/Dissertation/Files/all_josh.csv")#
dat <- glaucoma[(glaucoma$patid == patid) & (glaucoma$eye == eye), ]#
dat <- dat[order(dat$field), ]#
dat <- dat[!dat$point_ID %in% blind_spot, ] # remove blind spot 26, 35#
temp <- cbind(dat$sens_raw, dat$day)#
#
###Data objects#
n <- length(unique(dat$point_ID))#
T <- length(unique(dat$field))#
N <- n * T#
YStar <- matrix(temp[, 1], nrow = N, ncol = 1) #no scale#
YStarWide <- matrix(YStar, ncol = T)#
t <- matrix(unique(dat$day), nrow = T, ncol = 1) / 365 #scale to years#
t <- ((t - mean(t)) / sd(t)) * 0.5 # center at zero with sd of 0.5.#
#
###Matrix objects#
OneT <- matrix(1, nrow = T,ncol = 1)#
Eyen <- diag(n)#
Onen <- matrix(1, nrow = n, ncol = 1)#
OneN <- matrix(1, nrow = N, ncol = 1)#
EyeN <- diag(N)#
ZAlpha <- OneN#
ZBeta <- kronecker(t, Onen)#
ZDelta <- kronecker(OneT, Eyen)#
ZEta <- kronecker(t, Eyen)#
#
###Initial values#
SigmaEpsilon2 <- #
Tau2 <- 5#
Kappa2 <- 1#
Alpha <- 23#
Beta <- 0#
YStar1 <- YStarWide[, 1]#
Delta <- YStar1 - mean(YStar1)#
Eta <- matrix(0, nrow = n, ncol = 1)#
#
###Adjacency matrix#
source("/Users/Sam/Documents/Sam/School/Dissertation/Models/Likelihoods/Glaucoma_R_Functions/adjacency.R")#
# source("/home/users/berchuck/Dissertation/Files/adjacency.R")#
W <- adj.mat(Brigid)#
W <- W[-blind_spot, -blind_spot] #Remove blind spot 26,35#
Dw <- diag(apply(W, 1, sum))#
WStar <- Dw - W#
#
###Hyperparameters#
AlphaSigma <- 5.6#
BetaSigma <- 43.2#
MuAlpha <- 24#
SigmaAlpha2 <- 18^2 #
AlphaBeta <- 0 #
BetaBeta <- 5#
AlphaTau <- AlphaKappa <- 1 / 2#
BetaTau <- BetaKappa <- 0.01 / 2#
#
###Tobit information#
Tobit <- YStar == 0#
NBelow <- sum(Tobit)#
WhichTobit <- which(Tobit)#
#
###MCMC objects#
NSims <- 100000#
NBurn <- 12000#
NThin <- 40#
NTotal <- NBurn + NSims#
WhichKeep <- NBurn + (1:(NSims / NThin)) * NThin#
NKeep <- length(WhichKeep)#
#
###Pilot adaptation objects#
NPilot <- 24 #
WhichPilotAdapt <- (1:NPilot ) * NBurn / NPilot#
PilotAdaptDenominator <- WhichPilotAdapt[1]#
#
###Metropolis object#
TuningParameter <- 0.001#
Acceptance <- 0#
#
###Create storage object#
RawSamples <- matrix(nrow = 6 + 2 * n, ncol = NKeep)#
#
###Time MCMC sampler#
Begin <- Sys.time()#
#
###Begin sampler#
for (s in 1:NTotal) {#
	###Tobit step#
	if (NBelow == 0) Y <- YStar#
	if (NBelow != 0) {#
		Y <- YStar#
		MuTobit <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Y[Tobit] <- rtnorm(NBelow, MuTobit[NBelow], sqrt(SigmaEpsilon2), upper = 0)#
	}#
	###Alpha full conditional#
	VarAlpha <- chol2inv(chol(t(ZAlpha) %*% ZAlpha / SigmaEpsilon2 + 1 / SigmaAlpha2))#
	Gamma <- Y - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	MeanAlpha <- VarAlpha %*% (t(ZAlpha) %*% Gamma / SigmaEpsilon2 + MuAlpha / SigmaAlpha2)#
	Alpha <- rnorm(1, MeanAlpha, sqrt(VarAlpha))#
	###Delta full conditional#
	CovDelta <- chol2inv(chol(t(ZDelta) %*% ZDelta / SigmaEpsilon2 + WStar / Tau2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZEta %*% Eta#
	MeanDelta <- CovDelta %*% (t(ZDelta) %*% Gamma / SigmaEpsilon2)#
	Delta <- matrix(rmvnorm(1, MeanDelta, CovDelta), ncol = 1)#
	Delta <- (Delta - mean(Delta)) # sum to zero constraint#
#
	###Eta full conditional#
	CovEta <- chol2inv(chol(t(ZEta) %*% ZEta / SigmaEpsilon2 + WStar / Kappa2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta#
	MeanEta <- CovEta %*% (t(ZEta) %*% Gamma / SigmaEpsilon2)#
	Eta <- matrix(rmvnorm(1, MeanEta, CovEta), ncol = 1)#
	Eta <- (Eta - mean(Eta)) # sum to zero constraint#
#
	###SigmaEpsilon2 full conditional#
	AlphaSigmaNew <- AlphaSigma + N / 2#
	Residuals <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	BetaSigmaNew <- BetaSigma + t(Residuals) %*% Residuals / 2#
	SigmaEpsilon2 <- rigamma(1, AlphaSigmaNew, BetaSigmaNew)#
#
	###Tau2 full conditional#
	AlphaTauNew <- AlphaTau + (n - 1) / 2 #
	BetaTauNew <- BetaTau + t(Delta) %*% WStar %*% Delta / 2#
	Tau2 <- rigamma(1, AlphaTauNew, BetaTauNew)#
	if ((s <= NBurn) & (Tau2 >= 20)) Tau2 <- 20 # Tau2 is truncated above at 20 in the burn-in phase.#
#
	##Kappa2 Full Conditional#
	AlphaKappaNew <- AlphaKappa + (n - 1) / 2#
	BetaKappaNew <- BetaKappa + t(Eta) %*% WStar %*% Eta / 2#
	Kappa2 <- rigamma(1, AlphaKappaNew, BetaKappaNew)#
#
	###Metropolis step for Beta#
		###Sample proposal	#
    	ProposalBeta <- rnorm(1, Beta, sqrt(TuningParameter))#
#
    	###Likelihood Component#
    	MeanProposal <- Alpha * ZAlpha + ProposalBeta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Mean <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Component1A <- lndMvn(Y, MeanProposal, SigmaEpsilon2 * EyeN)#
    	Component1B <- lndMvn(Y, Mean, SigmaEpsilon2 * EyeN)#
        Component1 <- Component1A - Component1B#
#
    	###Prior components    	#
    	Component2A <- dcauchy(ProposalBeta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2B <- dcauchy(Beta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2 <- Component2A - Component2B#
#
    	###Log acceptance ratio#
    	LogR <- Component1 + Component2#
#
   	 	###Metropolis update#
    	RandU <- runif(1)#
    	if (log(RandU) < LogR) {#
#
     		###Keep count of acceptances#
      		Acceptance <- Acceptance + 1#
#
      		###Update parameters output#
      		Beta <- ProposalBeta#
#
    	}#
	##Compute Deviance for DIC#
	if (s %in% WhichKeep) {#
    	MeanDev <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Dev1 <- -2 * sum(dnorm(YStar[!Tobit], MeanDev[!Tobit], sqrt(SigmaEpsilon2), log = TRUE))#
		Dev2 <- -2 * sum(pnorm(MeanDev[Tobit] / sqrt(SigmaEpsilon2), 0, 1, lower.tail = FALSE, log.p = TRUE))	#
		Dev <- Dev1 + Dev2	#
	}#
#
    ###Pilot adaptation#
    if (s %in% WhichPilotAdapt) {#
    	AcceptancePct <- Acceptance / PilotAdaptDenominator#
    	if (AcceptancePct >= 0.90) TuningParameter <- TuningParameter * 1.3#
		if ((AcceptancePct >= 0.75) & (AcceptancePct < 0.90)) TuningParameter <- TuningParameter * 1.2#
		if ((AcceptancePct >= 0.45) & (AcceptancePct < 0.75)) TuningParameter <- TuningParameter * 1.1#
		if ((AcceptancePct <= 0.25) & (AcceptancePct > 0.15)) TuningParameter <- TuningParameter * 0.9#
		if ((AcceptancePct <= 0.15) & (AcceptancePct > 0.10)) TuningParameter <- TuningParameter * 0.8#
		if (AcceptancePct <= 0.10) TuningParameter <- TuningParameter * 0.7#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / PilotAdaptDenominator, 2) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
		Acceptance <- 0#
    }#
	###Store samples#
	if (s %in% WhichKeep) {#
 		Save <- c(Alpha, Beta, SigmaEpsilon2, Tau2, Kappa2, Dev, as.numeric(Delta), as.numeric(Eta))#
  		RawSamples[ , which(s == WhichKeep)] <- Save#
	}#
	##Verbose#
	if (s %in% WhichKeep) {#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / (NTotal - NBurn), 2) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
	}#
#
	###Time MCMC Sampler#
	if (s == NTotal) {#
		After <- Sys.time()#
		RunTime <- After - Begin#
		cat(paste0("Run Time: ", round(RunTime, digits = 2), " ", attr(RunTime, "unit")))#
		cat("######################################################################################################\n")#
#
	}#
###End MCMC Sampler	#
}
###Set seed#
set.seed(54)#
#
###Load packages#
suppressMessages(library(mvtnorm)) #multivariate normal#
suppressMessages(library(pscl)) #inverse gamma#
suppressMessages(library(msm)) #truncated normal#
suppressMessages(library(coda)) #mcmc#
suppressMessages(library(bayesm)) #lndMvn#
#
###Define the blind spot#
blind_spot <- c(26, 35)#
###Read in Data for ID = 101, Eye = Left#
glaucoma <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/all_josh.csv")#
# glaucoma <- read.csv("/home/users/berchuck/Dissertation/Files/all_josh.csv")#
dat <- glaucoma[(glaucoma$patid == patid) & (glaucoma$eye == eye), ]#
dat <- dat[order(dat$field), ]#
dat <- dat[!dat$point_ID %in% blind_spot, ] # remove blind spot 26, 35#
temp <- cbind(dat$sens_raw, dat$day)#
#
###Data objects#
n <- length(unique(dat$point_ID))#
T <- length(unique(dat$field))#
N <- n * T#
YStar <- matrix(temp[, 1], nrow = N, ncol = 1) #no scale#
YStarWide <- matrix(YStar, ncol = T)#
t <- matrix(unique(dat$day), nrow = T, ncol = 1) / 365 #scale to years#
t <- ((t - mean(t)) / sd(t)) * 0.5 # center at zero with sd of 0.5.#
#
###Matrix objects#
OneT <- matrix(1, nrow = T,ncol = 1)#
Eyen <- diag(n)#
Onen <- matrix(1, nrow = n, ncol = 1)#
OneN <- matrix(1, nrow = N, ncol = 1)#
EyeN <- diag(N)#
ZAlpha <- OneN#
ZBeta <- kronecker(t, Onen)#
ZDelta <- kronecker(OneT, Eyen)#
ZEta <- kronecker(t, Eyen)#
#
###Initial values#
SigmaEpsilon2 <- 1#
Tau2 <- 5#
Kappa2 <- 1#
Alpha <- 23#
Beta <- 0#
YStar1 <- YStarWide[, 1]#
Delta <- YStar1 - mean(YStar1)#
Eta <- matrix(0, nrow = n, ncol = 1)#
#
###Adjacency matrix#
source("/Users/Sam/Documents/Sam/School/Dissertation/Models/Likelihoods/Glaucoma_R_Functions/adjacency.R")#
# source("/home/users/berchuck/Dissertation/Files/adjacency.R")#
W <- adj.mat(Brigid)#
W <- W[-blind_spot, -blind_spot] #Remove blind spot 26,35#
Dw <- diag(apply(W, 1, sum))#
WStar <- Dw - W#
#
###Hyperparameters#
AlphaSigma <- 5.6#
BetaSigma <- 43.2#
MuAlpha <- 24#
SigmaAlpha2 <- 18^2 #
AlphaBeta <- 0 #
BetaBeta <- 5#
AlphaTau <- AlphaKappa <- 1 / 2#
BetaTau <- BetaKappa <- 0.01 / 2#
#
###Tobit information#
Tobit <- YStar == 0#
NBelow <- sum(Tobit)#
WhichTobit <- which(Tobit)#
#
###MCMC objects#
NSims <- 100000#
NBurn <- 12000#
NThin <- 40#
NTotal <- NBurn + NSims#
WhichKeep <- NBurn + (1:(NSims / NThin)) * NThin#
NKeep <- length(WhichKeep)#
#
###Pilot adaptation objects#
NPilot <- 24 #
WhichPilotAdapt <- (1:NPilot ) * NBurn / NPilot#
PilotAdaptDenominator <- WhichPilotAdapt[1]#
#
###Metropolis object#
TuningParameter <- 0.001#
Acceptance <- 0#
#
###Create storage object#
RawSamples <- matrix(nrow = 6 + 2 * n, ncol = NKeep)#
#
###Time MCMC sampler#
Begin <- Sys.time()#
#
###Begin sampler#
for (s in 1:NTotal) {#
	###Tobit step#
	if (NBelow == 0) Y <- YStar#
	if (NBelow != 0) {#
		Y <- YStar#
		MuTobit <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Y[Tobit] <- rtnorm(NBelow, MuTobit[NBelow], sqrt(SigmaEpsilon2), upper = 0)#
	}#
	###Alpha full conditional#
	VarAlpha <- chol2inv(chol(t(ZAlpha) %*% ZAlpha / SigmaEpsilon2 + 1 / SigmaAlpha2))#
	Gamma <- Y - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	MeanAlpha <- VarAlpha %*% (t(ZAlpha) %*% Gamma / SigmaEpsilon2 + MuAlpha / SigmaAlpha2)#
	Alpha <- rnorm(1, MeanAlpha, sqrt(VarAlpha))#
	###Delta full conditional#
	CovDelta <- chol2inv(chol(t(ZDelta) %*% ZDelta / SigmaEpsilon2 + WStar / Tau2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZEta %*% Eta#
	MeanDelta <- CovDelta %*% (t(ZDelta) %*% Gamma / SigmaEpsilon2)#
	Delta <- matrix(rmvnorm(1, MeanDelta, CovDelta), ncol = 1)#
	Delta <- (Delta - mean(Delta)) # sum to zero constraint#
#
	###Eta full conditional#
	CovEta <- chol2inv(chol(t(ZEta) %*% ZEta / SigmaEpsilon2 + WStar / Kappa2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta#
	MeanEta <- CovEta %*% (t(ZEta) %*% Gamma / SigmaEpsilon2)#
	Eta <- matrix(rmvnorm(1, MeanEta, CovEta), ncol = 1)#
	Eta <- (Eta - mean(Eta)) # sum to zero constraint#
#
	###SigmaEpsilon2 full conditional#
	AlphaSigmaNew <- AlphaSigma + N / 2#
	Residuals <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	BetaSigmaNew <- BetaSigma + t(Residuals) %*% Residuals / 2#
	SigmaEpsilon2 <- rigamma(1, AlphaSigmaNew, BetaSigmaNew)#
#
	###Tau2 full conditional#
	AlphaTauNew <- AlphaTau + (n - 1) / 2 #
	BetaTauNew <- BetaTau + t(Delta) %*% WStar %*% Delta / 2#
	Tau2 <- rigamma(1, AlphaTauNew, BetaTauNew)#
	if ((s <= NBurn) & (Tau2 >= 20)) Tau2 <- 20 # Tau2 is truncated above at 20 in the burn-in phase.#
#
	##Kappa2 Full Conditional#
	AlphaKappaNew <- AlphaKappa + (n - 1) / 2#
	BetaKappaNew <- BetaKappa + t(Eta) %*% WStar %*% Eta / 2#
	Kappa2 <- rigamma(1, AlphaKappaNew, BetaKappaNew)#
#
	###Metropolis step for Beta#
		###Sample proposal	#
    	ProposalBeta <- rnorm(1, Beta, sqrt(TuningParameter))#
#
    	###Likelihood Component#
    	MeanProposal <- Alpha * ZAlpha + ProposalBeta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Mean <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Component1A <- lndMvn(Y, MeanProposal, SigmaEpsilon2 * EyeN)#
    	Component1B <- lndMvn(Y, Mean, SigmaEpsilon2 * EyeN)#
        Component1 <- Component1A - Component1B#
#
    	###Prior components    	#
    	Component2A <- dcauchy(ProposalBeta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2B <- dcauchy(Beta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2 <- Component2A - Component2B#
#
    	###Log acceptance ratio#
    	LogR <- Component1 + Component2#
#
   	 	###Metropolis update#
    	RandU <- runif(1)#
    	if (log(RandU) < LogR) {#
#
     		###Keep count of acceptances#
      		Acceptance <- Acceptance + 1#
#
      		###Update parameters output#
      		Beta <- ProposalBeta#
#
    	}#
	##Compute Deviance for DIC#
	if (s %in% WhichKeep) {#
    	MeanDev <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Dev1 <- -2 * sum(dnorm(YStar[!Tobit], MeanDev[!Tobit], sqrt(SigmaEpsilon2), log = TRUE))#
		Dev2 <- -2 * sum(pnorm(MeanDev[Tobit] / sqrt(SigmaEpsilon2), 0, 1, lower.tail = FALSE, log.p = TRUE))	#
		Dev <- Dev1 + Dev2	#
	}#
#
    ###Pilot adaptation#
    if (s %in% WhichPilotAdapt) {#
    	AcceptancePct <- Acceptance / PilotAdaptDenominator#
    	if (AcceptancePct >= 0.90) TuningParameter <- TuningParameter * 1.3#
		if ((AcceptancePct >= 0.75) & (AcceptancePct < 0.90)) TuningParameter <- TuningParameter * 1.2#
		if ((AcceptancePct >= 0.45) & (AcceptancePct < 0.75)) TuningParameter <- TuningParameter * 1.1#
		if ((AcceptancePct <= 0.25) & (AcceptancePct > 0.15)) TuningParameter <- TuningParameter * 0.9#
		if ((AcceptancePct <= 0.15) & (AcceptancePct > 0.10)) TuningParameter <- TuningParameter * 0.8#
		if (AcceptancePct <= 0.10) TuningParameter <- TuningParameter * 0.7#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / PilotAdaptDenominator, 2) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
		Acceptance <- 0#
    }#
	###Store samples#
	if (s %in% WhichKeep) {#
 		Save <- c(Alpha, Beta, SigmaEpsilon2, Tau2, Kappa2, Dev, as.numeric(Delta), as.numeric(Eta))#
  		RawSamples[ , which(s == WhichKeep)] <- Save#
	}#
	##Verbose#
	if (s %in% WhichKeep) {#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / (NTotal - NBurn), 2) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
	}#
#
	###Time MCMC Sampler#
	if (s == NTotal) {#
		After <- Sys.time()#
		RunTime <- After - Begin#
		cat(paste0("Run Time: ", round(RunTime, digits = 2), " ", attr(RunTime, "unit")))#
		cat("######################################################################################################\n")#
#
	}#
###End MCMC Sampler	#
}
\
MeanParameters <- apply(RawSamples, 1, mean)#
	AlphaMean <- MeanParameters[1]#
	BetaMean <- MeanParameters[2]#
	SigmaEpsilon2Mean <- MeanParameters[3]#
	DBar <- MeanParameters[6]#
	DeltaMean <- MeanParameters[(1:n) + 6]#
	EtaMean <- MeanParameters[(1:n) + 6 + 52]
EtaMean
DeltaMean
DBar
MeanParameters
(1:n) + 6 + 52
AlphaMean <- MeanParameters[1]
AlphaMean
BetaMean <- MeanParameters[2]
BetaMean
SigmaEpsilon2Mean <- MeanParameters[3]
SigmaEpsilon2Mean
DBar <- MeanParameters[6]
DBar
DeltaMean <- MeanParameters[(1:n) + 6]#
	EtaMean <- MeanParameters[(1:n) + 6 + 52]
MeanDev <- AlphaMean * ZAlpha + BetaMean * ZBeta + ZDelta %*% DeltaMean + ZEta %*% EtaMean
MeanDev
Dev1 <- -2 * sum(dnorm(YStar[!Tobit], MeanDev[!Tobit], sqrt(SigmaEpsilon2Mean), log = TRUE))
Dev2 <- -2 * sum(pnorm(MeanDev[Tobit] / sqrt(SigmaEpsilon2Mean), 0, 1, lower.tail = FALSE, log.p = TRUE))
DHat <- Dev1 + Dev2
pD <- DBar - DHat#
	DIC <- DBar + pD
pD
DIC
Diagnostics <- list(DIC, pD)#
	names(Diagnostics)<-c("DIC","pD")#
	Diagnostics<-unlist(Diagnostics)#
	names(Diagnostics)<-c("DIC","pD")
Diagnostics
Alpha <- matrix(RawSamples[1, ], ncol = 1)
Alpha
Beta <- matrix(RawSamples[2, ], ncol = 1)
Beta
SigmaEpsilon2 <- matrix(RawSamples[3, ], ncol = 1)
Alpha <- matrix(RawSamples[1, ], ncol = 1)#
Beta <- matrix(RawSamples[2, ], ncol = 1)#
SigmaEpsilon2 <- matrix(RawSamples[3, ], ncol = 1)#
Tau2 <- matrix(RawSamples[4, ], ncol = 1)#
Kappa2 <- matrix(RawSamples[5, ], ncol = 1)
Tau2
Kappa2
Delta <- matrix(RawSamples[(1:n) + 6, ], ncol = n)#
Eta <- matrix(RawSamples[(1:n) + 6 + 52, ], ncol = n)
Delta
Eta
###Unload RawSamples#
Alpha <- matrix(RawSamples[1, ], ncol = 1)#
Beta <- matrix(RawSamples[2, ], ncol = 1)#
SigmaEpsilon2 <- matrix(RawSamples[3, ], ncol = 1)#
Tau2 <- matrix(RawSamples[4, ], ncol = 1)#
Kappa2 <- matrix(RawSamples[5, ], ncol = 1)#
Delta <- matrix(RawSamples[(1:n) + 6, ], ncol = n)#
Eta <- matrix(RawSamples[(1:n) + 6 + 52, ], ncol = n)
Out <- list(Alpha, Beta, SigmaEpsilon2, Tau2, Kappa2, Delta, Eta, Diagnostics)
Diagnostics
Alpha
Beta
SigmaEpsilon2
Out <- list(Alpha, Beta, SigmaEpsilon2, Tau2, Kappa2, Delta, Eta, Diagnostics)
Tau2
Kappa2
Delta
names(out)<-c("Alpha", "Beta", "SigmaEpsilon2", "Tau2", "Kappa2", "Delta", "Eta", "Diagnostics")
sprog <- function(patid = 101, eye = "L") {#
#
###Set seed#
set.seed(54)#
#
###Load packages#
suppressMessages(library(mvtnorm)) #multivariate normal#
suppressMessages(library(pscl)) #inverse gamma#
suppressMessages(library(msm)) #truncated normal#
suppressMessages(library(coda)) #mcmc#
suppressMessages(library(bayesm)) #lndMvn#
#
###Define the blind spot#
blind_spot <- c(26, 35)#
###Read in Data for ID = 101, Eye = Left#
glaucoma <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/all_josh.csv")#
# glaucoma <- read.csv("/home/users/berchuck/Dissertation/Files/all_josh.csv")#
dat <- glaucoma[(glaucoma$patid == patid) & (glaucoma$eye == eye), ]#
dat <- dat[order(dat$field), ]#
dat <- dat[!dat$point_ID %in% blind_spot, ] # remove blind spot 26, 35#
temp <- cbind(dat$sens_raw, dat$day)#
#
###Data objects#
n <- length(unique(dat$point_ID))#
T <- length(unique(dat$field))#
N <- n * T#
YStar <- matrix(temp[, 1], nrow = N, ncol = 1) #no scale#
YStarWide <- matrix(YStar, ncol = T)#
t <- matrix(unique(dat$day), nrow = T, ncol = 1) / 365 #scale to years#
t <- ((t - mean(t)) / sd(t)) * 0.5 # center at zero with sd of 0.5.#
#
###Matrix objects#
OneT <- matrix(1, nrow = T,ncol = 1)#
Eyen <- diag(n)#
Onen <- matrix(1, nrow = n, ncol = 1)#
OneN <- matrix(1, nrow = N, ncol = 1)#
EyeN <- diag(N)#
ZAlpha <- OneN#
ZBeta <- kronecker(t, Onen)#
ZDelta <- kronecker(OneT, Eyen)#
ZEta <- kronecker(t, Eyen)#
#
###Initial values#
SigmaEpsilon2 <- 1#
Tau2 <- 5#
Kappa2 <- 1#
Alpha <- 23#
Beta <- 0#
YStar1 <- YStarWide[, 1]#
Delta <- YStar1 - mean(YStar1)#
Eta <- matrix(0, nrow = n, ncol = 1)#
#
###Adjacency matrix#
source("/Users/Sam/Documents/Sam/School/Dissertation/Models/Likelihoods/Glaucoma_R_Functions/adjacency.R")#
# source("/home/users/berchuck/Dissertation/Files/adjacency.R")#
W <- adj.mat(Brigid)#
W <- W[-blind_spot, -blind_spot] #Remove blind spot 26,35#
Dw <- diag(apply(W, 1, sum))#
WStar <- Dw - W#
#
###Hyperparameters#
AlphaSigma <- 5.6#
BetaSigma <- 43.2#
MuAlpha <- 24#
SigmaAlpha2 <- 18^2 #
AlphaBeta <- 0 #
BetaBeta <- 5#
AlphaTau <- AlphaKappa <- 1 / 2#
BetaTau <- BetaKappa <- 0.01 / 2#
#
###Tobit information#
Tobit <- YStar == 0#
NBelow <- sum(Tobit)#
WhichTobit <- which(Tobit)#
#
###MCMC objects#
NSims <- 100000#
NBurn <- 12000#
NThin <- 40#
NTotal <- NBurn + NSims#
WhichKeep <- NBurn + (1:(NSims / NThin)) * NThin#
NKeep <- length(WhichKeep)#
#
###Pilot adaptation objects#
NPilot <- 24 #
WhichPilotAdapt <- (1:NPilot ) * NBurn / NPilot#
PilotAdaptDenominator <- WhichPilotAdapt[1]#
#
###Metropolis object#
TuningParameter <- 0.001#
Acceptance <- 0#
#
###Create storage object#
RawSamples <- matrix(nrow = 6 + 2 * n, ncol = NKeep)#
#
###Time MCMC sampler#
Begin <- Sys.time()#
#
###Begin sampler#
for (s in 1:NTotal) {#
	###Tobit step#
	if (NBelow == 0) Y <- YStar#
	if (NBelow != 0) {#
		Y <- YStar#
		MuTobit <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Y[Tobit] <- rtnorm(NBelow, MuTobit[NBelow], sqrt(SigmaEpsilon2), upper = 0)#
	}#
	###Alpha full conditional#
	VarAlpha <- chol2inv(chol(t(ZAlpha) %*% ZAlpha / SigmaEpsilon2 + 1 / SigmaAlpha2))#
	Gamma <- Y - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	MeanAlpha <- VarAlpha %*% (t(ZAlpha) %*% Gamma / SigmaEpsilon2 + MuAlpha / SigmaAlpha2)#
	Alpha <- rnorm(1, MeanAlpha, sqrt(VarAlpha))#
	###Delta full conditional#
	CovDelta <- chol2inv(chol(t(ZDelta) %*% ZDelta / SigmaEpsilon2 + WStar / Tau2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZEta %*% Eta#
	MeanDelta <- CovDelta %*% (t(ZDelta) %*% Gamma / SigmaEpsilon2)#
	Delta <- matrix(rmvnorm(1, MeanDelta, CovDelta), ncol = 1)#
	Delta <- (Delta - mean(Delta)) # sum to zero constraint#
#
	###Eta full conditional#
	CovEta <- chol2inv(chol(t(ZEta) %*% ZEta / SigmaEpsilon2 + WStar / Kappa2))#
	Gamma <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta#
	MeanEta <- CovEta %*% (t(ZEta) %*% Gamma / SigmaEpsilon2)#
	Eta <- matrix(rmvnorm(1, MeanEta, CovEta), ncol = 1)#
	Eta <- (Eta - mean(Eta)) # sum to zero constraint#
#
	###SigmaEpsilon2 full conditional#
	AlphaSigmaNew <- AlphaSigma + N / 2#
	Residuals <- Y - Alpha * ZAlpha - Beta * ZBeta - ZDelta %*% Delta - ZEta %*% Eta#
	BetaSigmaNew <- BetaSigma + t(Residuals) %*% Residuals / 2#
	SigmaEpsilon2 <- rigamma(1, AlphaSigmaNew, BetaSigmaNew)#
#
	###Tau2 full conditional#
	AlphaTauNew <- AlphaTau + (n - 1) / 2 #
	BetaTauNew <- BetaTau + t(Delta) %*% WStar %*% Delta / 2#
	Tau2 <- rigamma(1, AlphaTauNew, BetaTauNew)#
	# if ((s <= NBurn) & (Tau2 >= 20)) Tau2 <- 20 # Tau2 is truncated above at 20 in the burn-in phase.#
	if (Tau2 >= 20) Tau2 <- 20 # Tau2 is truncated above at 20.#
#
	##Kappa2 Full Conditional#
	AlphaKappaNew <- AlphaKappa + (n - 1) / 2#
	BetaKappaNew <- BetaKappa + t(Eta) %*% WStar %*% Eta / 2#
	Kappa2 <- rigamma(1, AlphaKappaNew, BetaKappaNew)#
#
	###Metropolis step for Beta#
		###Sample proposal	#
    	ProposalBeta <- rnorm(1, Beta, sqrt(TuningParameter))#
#
    	###Likelihood Component#
    	MeanProposal <- Alpha * ZAlpha + ProposalBeta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Mean <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
    	Component1A <- lndMvn(Y, MeanProposal, SigmaEpsilon2 * EyeN)#
    	Component1B <- lndMvn(Y, Mean, SigmaEpsilon2 * EyeN)#
        Component1 <- Component1A - Component1B#
#
    	###Prior components    	#
    	Component2A <- dcauchy(ProposalBeta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2B <- dcauchy(Beta, AlphaBeta, BetaBeta, log = TRUE)#
    	Component2 <- Component2A - Component2B#
#
    	###Log acceptance ratio#
    	LogR <- Component1 + Component2#
#
   	 	###Metropolis update#
    	RandU <- runif(1)#
    	if (log(RandU) < LogR) {#
#
     		###Keep count of acceptances#
      		Acceptance <- Acceptance + 1#
#
      		###Update parameters output#
      		Beta <- ProposalBeta#
#
    	}#
	##Compute Deviance for DIC#
	if (s %in% WhichKeep) {#
    	MeanDev <- Alpha * ZAlpha + Beta * ZBeta + ZDelta %*% Delta + ZEta %*% Eta#
		Dev1 <- -2 * sum(dnorm(YStar[!Tobit], MeanDev[!Tobit], sqrt(SigmaEpsilon2), log = TRUE))#
		Dev2 <- -2 * sum(pnorm(MeanDev[Tobit] / sqrt(SigmaEpsilon2), 0, 1, lower.tail = FALSE, log.p = TRUE))	#
		Dev <- Dev1 + Dev2	#
	}#
#
    ###Pilot adaptation#
    if (s %in% WhichPilotAdapt) {#
    	AcceptancePct <- Acceptance / PilotAdaptDenominator#
    	if (AcceptancePct >= 0.90) TuningParameter <- TuningParameter * 1.3#
		if ((AcceptancePct >= 0.75) & (AcceptancePct < 0.90)) TuningParameter <- TuningParameter * 1.2#
		if ((AcceptancePct >= 0.45) & (AcceptancePct < 0.75)) TuningParameter <- TuningParameter * 1.1#
		if ((AcceptancePct <= 0.25) & (AcceptancePct > 0.15)) TuningParameter <- TuningParameter * 0.9#
		if ((AcceptancePct <= 0.15) & (AcceptancePct > 0.10)) TuningParameter <- TuningParameter * 0.8#
		if (AcceptancePct <= 0.10) TuningParameter <- TuningParameter * 0.7#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / PilotAdaptDenominator, 2) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
		Acceptance <- 0#
    }#
	###Store samples#
	if (s %in% WhichKeep) {#
 		Save <- c(Alpha, Beta, SigmaEpsilon2, Tau2, Kappa2, Dev, as.numeric(Delta), as.numeric(Eta))#
  		RawSamples[ , which(s == WhichKeep)] <- Save#
	}#
	##Verbose#
	if (s %in% WhichKeep) {#
		cat(paste0("Completed Percentage: ", round((s / NTotal) * 100, digits = 0), "%\n"))#
		cat(paste0("Alpha: ",round(Alpha, digits = 3)),"\n")#
		cat(paste0("Beta: ",round(Beta, digits = 3),", Acceptance: ",round(Acceptance / (NTotal - NBurn), 2) * 100,"%\n"))#
		cat(paste0("Sigma2: ",round(SigmaEpsilon2, digits = 3)),"\n")#
		cat(paste0("Tau2: ",round(Tau2, digits = 3)),"\n")#
		cat(paste0("Kappa2: ", round(Kappa2, digits = 3)),"\n")#
		cat(paste0("Deviance: ", round(Dev, digits = 3)),"\n")#
		cat("######################################################################################################\n")#
	}#
#
	###Time MCMC Sampler#
	if (s == NTotal) {#
		After <- Sys.time()#
		RunTime <- After - Begin#
		cat(paste0("Run Time: ", round(RunTime, digits = 2), " ", attr(RunTime, "unit")))#
		cat("######################################################################################################\n")#
#
	}#
###End MCMC Sampler	#
}#
#
###Compute Diagnostics#
#
	###Compute posterior means for DIC#
	MeanParameters <- apply(RawSamples, 1, mean)#
	AlphaMean <- MeanParameters[1]#
	BetaMean <- MeanParameters[2]#
	SigmaEpsilon2Mean <- MeanParameters[3]#
	DBar <- MeanParameters[6]#
	DeltaMean <- MeanParameters[(1:n) + 6]#
	EtaMean <- MeanParameters[(1:n) + 6 + 52]#
	###DHat#
	MeanDev <- AlphaMean * ZAlpha + BetaMean * ZBeta + ZDelta %*% DeltaMean + ZEta %*% EtaMean#
	Dev1 <- -2 * sum(dnorm(YStar[!Tobit], MeanDev[!Tobit], sqrt(SigmaEpsilon2Mean), log = TRUE))#
	Dev2 <- -2 * sum(pnorm(MeanDev[Tobit] / sqrt(SigmaEpsilon2Mean), 0, 1, lower.tail = FALSE, log.p = TRUE))	#
	DHat <- Dev1 + Dev2	#
#
	###DIC, pD#
	pD <- DBar - DHat#
	DIC <- DBar + pD#
	###Summarize diagnostics#
	Diagnostics <- list(DIC, pD)#
	Diagnostics <- unlist(Diagnostics)#
	names(Diagnostics) <- c("DIC", "pD")#
#
###Unload RawSamples#
Alpha <- matrix(RawSamples[1, ], ncol = 1)#
Beta <- matrix(RawSamples[2, ], ncol = 1)#
SigmaEpsilon2 <- matrix(RawSamples[3, ], ncol = 1)#
Tau2 <- matrix(RawSamples[4, ], ncol = 1)#
Kappa2 <- matrix(RawSamples[5, ], ncol = 1)#
Delta <- matrix(RawSamples[(1:n) + 6, ], ncol = n)#
Eta <- matrix(RawSamples[(1:n) + 6 + 52, ], ncol = n)#
#
###Return objects#
Out <- list(Alpha, Beta, SigmaEpsilon2, Tau2, Kappa2, Delta, Eta, Diagnostics)#
names(Out)<-c("Alpha", "Beta", "SigmaEpsilon2", "Tau2", "Kappa2", "Delta", "Eta", "Diagnostics")#
return(Out)#
}
reg <- brigid(patid = 101, eye = "L")
reg <- sprog(patid = 101, eye = "L")
reg <- sprog(patid = 101, eye = "R")
traceplot(as.mcmc(reg$Beta))
rm(list = ls())#
#
###Load data#
glaucoma <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/all_josh.csv", header = TRUE)#
class <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/class_josh.csv", header = TRUE)#
#
###Packages to be used#
library(pROC) #roc#
library(xtable) #xtable#
#
#####################################################################
#####################################################################
### Create PLR, mean slope and max day metrics#
#####################################################################
#####################################################################
#
###Format data to get eye information#
glaucoma<-glaucoma[order(glaucoma$eye),]#
glaucoma<-glaucoma[order(glaucoma$patid),]#
Eyes<-data.frame(unique(cbind(glaucoma$patid,glaucoma$eye)))#
Eyes[,2]<-factor(Eyes[,2])#
levels(Eyes[,2])<-c("L","R")#
neyes<-dim(Eyes)[1]#
###Storage objects#
P1 <- P2 <- P3 <- P4 <- Maxday <- MS <- Nu <- numeric(length = dim(Eyes)[1])#
###Begin calculating diagnostic metrics#
for (i in 1:neyes) {#
	###Set eye information#
	patid <- Eyes[i, 1]#
	eye <- Eyes[i, 2]#
	dat <- glaucoma[(glaucoma$patid == patid) & (glaucoma$eye == eye), ]#
	day <- unique(dat$day)#
#
	###Calculate PLR#
	Nu[i] <- nu <- length(unique(dat$field))#
	sens <- matrix(dat$sens_raw, ncol = 54) / 10#
	# sens <- matrix(dat$sens_raw, ncol = 54)#
	sens <- sens[, -c(26, 35)]#
	pvalues <- sort(apply(sens, 2, f <- function(x) summary(lm(x ~ I(day / 365)))$coef[2, 4]))[1:4]#
	# pvalues <- sort(apply(sens, 2, f <- function(x) summary(lm(x ~ day))$coef[2, 4]))[1:4]#
	P1[i] <- pvalues[1]#
	P2[i] <- pvalues[2]#
	P3[i] <- pvalues[3]#
	P4[i] <- pvalues[4]#
#
	###Calculate maxday#
	Maxday[i] <- unique(day)[nu] / 365#
	# Maxday[i] <- unique(day)[nu]#
	###Calculate mean slope#
	MS[i] <- mean(apply(sens, 2, f <- function(x) summary(lm(x ~ I(day / 365)))$coef[2]))#
	# MS[i] <- mean(apply(sens, 2, f <- function(x) summary(lm(x ~ day))$coef[2]))#
#
	###Output iteration#
	cat("Iteration: ", i," / ",neyes,"\n")#
#
}#
###Save PLR metrics#
PLR <- data.frame(cbind(Eyes, Nu, P1, P2, P3, P4, Maxday, MS))#
PLR[, -2] <- apply(PLR[, -2], 2, as.character)#
PLR[, -2] <- apply(PLR[, -2], 2, as.numeric)#
colnames(PLR)[1:2] <- c("patid", "eye")#
save(PLR, file = "/Users/Sam/Documents/Sam/School/Dissertation/Models/Boundary\ Models/Paper2/Output/PLR.RData")     #
#####################################################################
#####################################################################
### Load ST CV and SPROG output created on the cluster
load("/Users/Sam/Documents/Sam/School/Dissertation/Models/Boundary\ Models/Paper2/Cluster/ProcessBrigidCauchy.RData") # SPROG
ProcessBrigid
load("/Users/Sam/Documents/Sam/School/Dissertation/Models/Boundary\ Models/Paper2/Cluster/ProcessBrigidCauchy.RData") # SPROG#
SPROG <- ProcessBrigid[, 4]#
#
###Load ST CV original#
# STCV <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Models/Boundary\ Models/Paper2/Cluster/CVout250.csv")[, -1] # STCV#
# STCVMean <- STCV$cvalpha_mean#
# STCVSd <- STCV$cvalpha_sd#
#
###STCV based on EXP#
load("/Users/Sam/Documents/Sam/School/Dissertation/Models/Boundary\ Models/Paper1/DataApplication/Process/PatientLevel_Process.RData")#
PatientLevel_Process <- data.frame(PatientLevel_Process)#
STCVMean <- PatientLevel_Process$cv_mean#
STCVSd <- sqrt(PatientLevel_Process$cv_var)#
#
###STCV based on AR1#
# load("/Users/Sam/Documents/Sam/School/Dissertation/Models/Boundary\ Models/OPMV/Longleaf/ProcessAR1.RData")#
# STCVMean <- as.numeric(ProcessAR1[ , 5])	#
# STCVSd <- sqrt(as.numeric(ProcessAR1[ , 6]))#
#####################################################################
#####################################################################
### Set the clinical assessment of progression (gold standard)#
#####################################################################
#####################################################################
#
# Prog <- STCV$class#
Prog <- PatientLevel_Process$prog#
# Prog <- as.numeric(ProcessAR1[ , 4])#
#
#####################################################################
#####################################################################
### Fit logistic regression models of glaucoma progression#
#####################################################################
#####################################################################
#
###EM regression#
regEM <- glm(Prog ~ MS * Maxday, family = "binomial")#
#
###Marginal regressions#
regP1 <- glm(Prog ~ P1, family = "binomial")#
regP2 <- glm(Prog ~ P2, family = "binomial")#
regP3 <- glm(Prog ~ P3, family = "binomial")#
regP4 <- glm(Prog ~ P4, family = "binomial")#
regSPROG <- glm(Prog ~ SPROG, family = "binomial")#
regDM1 <- glm(Prog ~ STCVMean, family = "binomial")#
regDM2 <- glm(Prog ~ STCVMean * STCVSd, family = "binomial")#
#
###Add-on regressions#
regEMplusP1 <- glm(Prog ~ MS * Maxday + P1, family = "binomial")#
regEMplusP2 <- glm(Prog ~ MS * Maxday + P2, family = "binomial")#
regEMplusP3 <- glm(Prog ~ MS * Maxday + P3, family = "binomial")#
regEMplusP4 <- glm(Prog ~ MS * Maxday + P4, family = "binomial")#
regEMplusSPROG <- glm(Prog ~ MS * Maxday + SPROG, family = "binomial")#
regEMplusDM1 <- glm(Prog ~ MS * Maxday + STCVMean, family = "binomial")#
regEMplusDM2 <- glm(Prog ~ MS * Maxday + STCVMean * STCVSd, family = "binomial")#
#
###Save regression objects#
save(regEM, file = "/Users/Sam/Documents/Sam/School/Dissertation/Models/Boundary\ Models/Paper2/Output/Regressions/regEM.RData")#
save(regDM1, file = "/Users/Sam/Documents/Sam/School/Dissertation/Models/Boundary\ Models/Paper2/Output/Regressions/regDM1.RData")#
save(regDM2, file = "/Users/Sam/Documents/Sam/School/Dissertation/Models/Boundary\ Models/Paper2/Output/Regressions/regDM2.RData")#
save(regEMplusDM1, file = "/Users/Sam/Documents/Sam/School/Dissertation/Models/Boundary\ Models/Paper2/Output/Regressions/regEMplusDM1.RData")#
save(regEMplusDM2, file = "/Users/Sam/Documents/Sam/School/Dissertation/Models/Boundary\ Models/Paper2/Output/Regressions/regEMplusDM2.RData")#
#
#####################################################################
#####################################################################
### Create metrics Table.#
#####################################################################
#####################################################################
#
###Collect regression objects#
regs <- list(regEM, regP1, regP2, regP3, regP4, regSPROG, regDM1, regDM2)#
regsEMplus <- list(regEM, regEMplusP1, regEMplusP2, regEMplusP3, regEMplusP4, regEMplusSPROG, regEMplusDM1, regEMplusDM2)#
#
###Calculate AUC#
rocs <- rocsEMplus <- list()#
for (i in 1:length(regs)) {#
	reg <- regs[[i]]#
	if (length(predict(reg)) == 191) rocs[[i]] <- roc(Prog[!is.na(Prog)], predict(reg))#
	if (length(predict(reg)) == 188) rocs[[i]] <- roc(Prog[!is.na(Prog) & !is.na(P1)], predict(reg))#
	regEMplus <- regsEMplus[[i]]#
	if (length(predict(regEMplus)) == 191) rocsEMplus[[i]] <- roc(Prog[!is.na(Prog)], predict(regEMplus))#
	if (length(predict(regEMplus)) == 188) rocsEMplus[[i]] <- roc(Prog[!is.na(Prog) & !is.na(P1)], predict(regEMplus))#
}#
#
###Calculate pAUC#
rocsPartial <- rocsEMplusPartial <- list()#
for (i in 1:length(regs)) {#
	reg <- regs[[i]]#
	if (length(predict(reg)) == 191) rocsPartial[[i]] <- roc(Prog[!is.na(Prog)], predict(reg), partial.auc = c(0.85, 1), partial.auc.correct = FALSE, partial.auc.focus = "sp")#
	if (length(predict(reg)) == 188) rocsPartial[[i]] <- roc(Prog[!is.na(Prog) & !is.na(P1)], predict(reg), partial.auc = c(0.85,1), partial.auc.correct = FALSE, partial.auc.focus = "sp")#
	regEMplus <- regsEMplus[[i]]#
	if (length(predict(regEMplus)) == 191) rocsEMplusPartial[[i]] <- roc(Prog[!is.na(Prog)], predict(regEMplus), partial.auc = c(0.85, 1), partial.auc.correct = FALSE, partial.auc.focus = "sp")#
	if (length(predict(regEMplus)) == 188) rocsEMplusPartial[[i]] <- roc(Prog[!is.na(Prog) & !is.na(P1)], predict(regEMplus), partial.auc = c(0.85, 1), partial.auc.correct = FALSE, partial.auc.focus = "sp")#
}#
#
###Calculate AUC and pAUC p-value#
# pvalueAUC <- pvaluepAUC <- numeric(length = length(regs))#
# for (i in 1:length(regs)) {#
	# pvalueAUC[i] <- roc.test(rocs[[1]], rocsEMplus[[i]], method = "bootstrap")$p.value#
	# pvaluepAUC[i] <- roc.test(rocsPartial[[1]], rocsEMplusPartial[[i]], method = "bootstrap")$p.value#
# }#
#
###Marginal p-value#
regNULL <- glm(Prog ~ 1, "binomial")#
pvalues1 <- c(anova(regNULL, regEM,test = "Chisq")$"Pr(>Chi)"[2], anova(glm(Prog[!is.na(P1)] ~ 1, "binomial"), glm(Prog[!is.na(P1)] ~ P1[!is.na(P1)], "binomial"), test = "Chisq")$"Pr(>Chi)"[2], anova(glm(Prog[!is.na(P1)] ~ 1, "binomial"), glm(Prog[!is.na(P1)] ~ P2[!is.na(P1)], "binomial"), test = "Chisq")$"Pr(>Chi)"[2], anova(glm(Prog[!is.na(P1)] ~ 1, "binomial"), glm(Prog[!is.na(P1)] ~ P3[!is.na(P1)], "binomial"), test = "Chisq")$"Pr(>Chi)"[2], anova(glm(Prog[!is.na(P1)] ~ 1, "binomial"), glm(Prog[!is.na(P1)] ~ P4[!is.na(P1)], "binomial"), test = "Chisq")$"Pr(>Chi)"[2], anova(regNULL, regSPROG, test = "Chisq")$"Pr(>Chi)"[2], anova(regNULL, regDM1, test = "Chisq")$"Pr(>Chi)"[2], anova(regNULL, regDM2, test = "Chisq")$"Pr(>Chi)"[2])#
#
###Add-on p-value#
pvalues2 <- c(anova(regEM,regEM,test="Chisq")$"Pr(>Chi)"[2], anova(glm(Prog[!is.na(P1)] ~ Maxday[!is.na(P1)] * MS[!is.na(P1)], "binomial"), glm(Prog[!is.na(P1)]~Maxday[!is.na(P1)] * MS[!is.na(P1)] + P1[!is.na(P1)], "binomial"), test = "Chisq")$"Pr(>Chi)"[2], anova(glm(Prog[!is.na(P1)] ~ Maxday[!is.na(P1)] * MS[!is.na(P1)], "binomial"), glm(Prog[!is.na(P1)]~Maxday[!is.na(P1)] * MS[!is.na(P1)] + P2[!is.na(P1)], "binomial"), test = "Chisq")$"Pr(>Chi)"[2], anova(glm(Prog[!is.na(P1)] ~ Maxday[!is.na(P1)] * MS[!is.na(P1)], "binomial"), glm(Prog[!is.na(P1)]~Maxday[!is.na(P1)] * MS[!is.na(P1)] + P3[!is.na(P1)], "binomial"), test = "Chisq")$"Pr(>Chi)"[2], anova(glm(Prog[!is.na(P1)] ~ Maxday[!is.na(P1)] * MS[!is.na(P1)], "binomial"), glm(Prog[!is.na(P1)]~Maxday[!is.na(P1)] * MS[!is.na(P1)] + P4[!is.na(P1)], "binomial"), test = "Chisq")$"Pr(>Chi)"[2], anova(regEM,regEMplusSPROG,test="Chisq")$"Pr(>Chi)"[2], anova(regEM,regEMplusDM1,test="Chisq")$"Pr(>Chi)"[2], anova(regEM,regEMplusDM2,test="Chisq")$"Pr(>Chi)"[2])#
#
###Summarize and create table #
TableOut <- cbind(cbind(unlist(lapply(regs, AIC)), unlist(lapply(rocs, auc)), unlist(lapply(rocsPartial, f <- function(x) auc(x, partial.auc = c(0.85, 1), partial.auc.correct = FALSE, partial.auc.focus = "sp"))) /0.15, pvalues1), cbind(unlist(lapply(regsEMplus, AIC)), unlist(lapply(rocsEMplus, auc)), unlist(lapply(rocsEMplusPartial, f <- function(x) auc(x, partial.auc = c(0.85, 1), partial.auc.correct = FALSE, partial.auc.focus = "sp"))) / 0.15), pvalues2)#
colnames(TableOut)<-c("AIC","AUC","pAUC","p-value","AIC","AUC","pAUC","p-value")#
rownames(TableOut)<-c("EM","P1","P2","P3","P4","SPROG","DM1","DM2")#
DigitsMat <- matrix(c(rep(0, 8), rep(2, 8), rep(2, 8), rep(2, 8), rep(3, 8), rep(2, 8), rep(2, 8), rep(2, 8), rep(3, 8)), nrow = 8, ncol = 9)#
print(xtable(TableOut, digits= DigitsMat), sanitize.text.function = function(x) {x})#
#
#####################################################################
#####################################################################
### Create correlation Table.#
#####################################################################
#####################################################################
#
###Combine objects#
X <- cbind(EM = MS * Maxday, P1, P2, P3, P4, SPROG, DM1 = STCVMean, DM2 = STCVMean * STCVSd)#
#
###Calculate correlation matrix#
Xcor <- cor(X, use = "complete.obs")#
#
###Calculate Pearson correlation tests#
Xtest <- Xcor#
for (i in 1:dim(X)[2]) {#
	for (j in i:dim(X)[2]) {	#
		Xtest[i, j] <- cor.test(X[, i], X[, j])$p.value#
	}#
}#
#
###Create Table for Paper 2#
xtable(Xtest,digits=c(0,rep(2,8)))#
#
#####################################################################
#####################################################################
### Create ROC Figure.
plot(1, 1, type = "n", xlim = c(0, 1), ylim = c(0, 1), xlab = "1 - Specificity", ylab = "Sensitivity", main = "", asp = 1)#
abline(a = 0, b = 1, col = "gray")#
#
###ROC curve for EM#
rocEM <- roc(Prog[!is.na(Prog)], predict(regEM, type = "response"))#
lines(1 - rocEM$spec, rocEM$sens, col = 1)#
#
###ROC curve for EM + DM1#
rocEMplusDM1 <- roc(Prog[!is.na(Prog)], predict(regEMplusDM1, type = "response"))#
lines(1 - rocEMplusDM1$spec, rocEMplusDM1$sens, col = 2)#
#
###ROC curve for EM + DM2#
rocEMplusDM2 <- roc(Prog[!is.na(Prog)], predict(regEMplusDM2, type = "response"))#
lines(1 - rocEMplusDM2$spec, rocEMplusDM2 $sens, col = 3)#
#
###ROC curve for SPROG#
rocSPROG <- roc(Prog[!is.na(Prog)], predict(regSPROG, type = "response"))#
lines(1 - rocSPROG$spec, rocSPROG$sens, col = 4)#
#
###ROC curve for P1#
rocP1 <- roc(regP1$y, predict(regP1, type = "response"))#
lines(1 - rocP1$spec, rocP1$sens, col = 5)#
#
##Legend#
legend("bottomright", legend = paste(c("EM      (" ,"EM + DM1 (","EM + DM2  (","SPROG     (", "P1 ("),round(c(auc(rocEM),auc(rocEMplusDM1),auc(rocEMplusDM2),auc(rocSPROG),auc(rocP1)),digits=2),", ",round(c(AIC(regEM),AIC(regEMplusDM1),AIC(regEMplusDM2),AIC(regSPROG),AIC(regP1)),digits=2),")",sep=""),lty=rep(1,5),col=1:5)
q()
###Logical function to check for colors#
  areColors <- function(x) {#
    sapply(x, function(X) {#
      tryCatch(is.matrix(col2rgb(X)),#
               error = function(e) FALSE)#
    })#
  }
areColors
!any(areColors(col.line))
!any(areColors(1))
!any(areColors(3))
!any(areColors("red"))
!any(areColors("asdf"))
!any(areColors("addfsa"))
reg.line = TRUE
!is.logical(reg.line)
###Function for plotting a time series of data at each location on the visual field#
#'#
#' PlotVfTimeSeries#
#'#
#' Plots a time series at each location of the Humphrey Field Analyzer-II visual field .#
#'#
#' @param Y a time series variable to be plotted.#
#'#
#' @param Location a variable corresponding to the location on the visual field#
#'  that the time series variable was observed.#
#'#
#' @param Time a variable corresponding to the time that the time series variable#
#'  was observed.#
#'#
#' @param main an overall title for the plot.#
#'#
#' @param xlab a title for the x axis.#
#'#
#' @param ylab a title for the y axis.#
#'#
#' @param col.line color for the regression line, either character string corresponding#
#'  to a color or a integer (default = "red").#
#'#
#' @param reg.line logical, determining if there are regressiong lines printed (default = TRUE).#
#'#
#' @details \code{PlotVfTimeSeries} is used in the application of glaucoma progression.#
#'  In each cell is the observed DLS at each location over visits, with the red line#
#'  representing a linear regression trend.#
#'#
#' @examples#
#' data(VFSeries)#
#' PlotVfTimeSeries(Y = VFSeries$DLS,#
#'                   Location = VFSeries$Location,#
#'                   Time = VFSeries$Time,#
#'                   main = "Visual field sensitivity time series \n at each location",#
#'                   xlab = "Days from baseline visit",#
#'                   ylab = "Differential light sensitivity (dB)")#
#'#
#'#
#' @author Samuel I. Berchuck#
#'#
#' @export#
PlotVfTimeSeries <- function(Y, Location, Time,#
                             main = "Visual field sensitivity time series \n at each location",#
                             xlab = "Time from first visit (days)",#
                             ylab = "Sensitivity (dB)",#
                             col.line = "red",#
                             reg.line = TRUE) {#
  ###Logical function to check for colors#
  areColors <- function(x) {#
    sapply(x, function(X) {#
      tryCatch(is.matrix(col2rgb(X)),#
               error = function(e) FALSE)#
    })#
  }#
#
  ###Check Inputs#
  if (missing(Y)) stop('"Y" is missing')#
  if (missing(Location)) stop('"Location" is missing')#
  if (missing(Time)) stop('"Time" is missing')#
  if (!is.character(main)) stop('"main" must be a character string')#
  if (!is.character(xlab)) stop('"xlab" must be a character string')#
  if (!is.character(ylab)) stop('"ylab" must be a character string')#
  if (!any(areColors(col.line))) stop('"col.line" can only include colors')#
  if (!is.logical(reg.line)) stop('"reg.line" must be a logical')#
  ###Function inputs#
  # Y <- YObserved#
  # Location <- Location#
  # Time <- Time#
  # main = "Visual field sensitivity time series \n at each location"#
  # xlab = "Time from first visit (days)"#
  # ylab = "Sensitivity (dB)"#
  pardefault <- suppressWarnings(par(no.readonly = T))#
#
  ###Collect and sort data#
  VF <- data.frame(cbind(Location, Time, Y))#
  VF <- VF[order(VF$Time), ]#
  VF <- VF[order(VF$Location), ]#
#
  ###Compute Summary Statistics#
  max.VF <- max(abs(range(Y)))#
  max.Time <- max(abs(Time))#
  y_breaks <- round(seq(0, 40, by = 10))#
  x_breaks <- round(seq(0, 100*(max.Time%/%100 + as.logical(max.Time%%100)), by = 100)) #Round up to the nearest 100th#
#
  ###Create layout matrix#
  layout.matrix<-matrix(c(0,0,0,1,2,3,4,0,0,#
                          0,0,5,6,7,8,9,10,0,#
                          0,11,12,13,14,15,16,17,18,#
                          19,20,21,22,23,24,25,26,27,#
                          28,29,30,31,32,33,34,35,36,#
                          0,37,38,39,40,41,42,43,44,#
                          0,0,45,46,47,48,49,50,0,#
                          0,0,0,51,52,53,54,0,0),nrow=8,ncol=9,byrow=TRUE)#
  pp<-layout(layout.matrix,rep(1,3),rep(1,9),TRUE)#
  # layout.show(pp)#
#
  ###Clarify Blind Spot#
  all <- 1 : max(VF$Location)#
  blind_spot <- c(26, 35)#
  remaining <- all[-blind_spot]#
#
  ###Plot Time Series at Each Location#
  par(mar = c(0, 0, 0, 0), oma = c(5, 10, 10, 5), mgp = c(3, 1, 0))#
  for (i in 1 : max(VF$Location)) {#
    if (i %in% remaining) {#
      ph <- VF[VF$Location == i, ]#
      plot(ph[ , 2], ph[ , 3], type = "l", xaxt = "n", yaxt = "n", xlim = c(0, max.Time), ylim = c(0, 40))#
      points(ph[ , 2], ph[ , 3], pch = ".")#
      abline(lm(ph[ , 3] ~ ph[ , 2]), col = col.line)#
    }#
    if (i %in% blind_spot) plot(ph[ , 2], ph[ , 3], type = "n", xaxt = "n", yaxt = "n", xlim = c(0, max.Time), ylim = c(0, 40))#
    if (i %in% c(52, 54)) axis(1, at = x_breaks)#
    if (i %in% c(1, 3)) axis(3, at = x_breaks)#
    if (i %in% c(5, 19, 37, 51)) axis(2, at = y_breaks, las = 2)#
    if (i %in% c(4, 18, 36, 50)) axis(4, at = y_breaks, las = 2)#
  }#
#
  ###Add Title#
  title(main = list(main, cex = 2.5, col = "black", font = 2),#
        xlab = list(xlab, cex = 2, col = "black", font = 1),#
        ylab = list(ylab, cex = 2, col = "black", font = 1), outer = TRUE)#
#
  ###Return par to default#
  suppressMessages(par(pardefault))#
#
###End function#
}
###Function for plotting a time series of data at each location on the visual field#
#'#
#' PlotVfTimeSeries#
#'#
#' Plots a time series at each location of the Humphrey Field Analyzer-II visual field .#
#'#
#' @param Y a time series variable to be plotted.#
#'#
#' @param Location a variable corresponding to the location on the visual field#
#'  that the time series variable was observed.#
#'#
#' @param Time a variable corresponding to the time that the time series variable#
#'  was observed.#
#'#
#' @param main an overall title for the plot.#
#'#
#' @param xlab a title for the x axis.#
#'#
#' @param ylab a title for the y axis.#
#'#
#' @param col.line color for the regression line, either character string corresponding#
#'  to a color or a integer (default = "red").#
#'#
#' @param reg.line logical, determining if there are regressiong lines printed (default = TRUE).#
#'#
#' @details \code{PlotVfTimeSeries} is used in the application of glaucoma progression.#
#'  In each cell is the observed DLS at each location over visits, with the red line#
#'  representing a linear regression trend.#
#'#
#' @examples#
#' data(VFSeries)#
#' PlotVfTimeSeries(Y = VFSeries$DLS,#
#'                   Location = VFSeries$Location,#
#'                   Time = VFSeries$Time,#
#'                   main = "Visual field sensitivity time series \n at each location",#
#'                   xlab = "Days from baseline visit",#
#'                   ylab = "Differential light sensitivity (dB)")#
#'#
#'#
#' @author Samuel I. Berchuck#
#'#
#' @export#
PlotVfTimeSeries <- function(Y, Location, Time,#
                             main = "Visual field sensitivity time series \n at each location",#
                             xlab = "Time from first visit (days)",#
                             ylab = "Sensitivity (dB)",#
                             col.line = "red",#
                             reg.line = TRUE) {#
  ###Logical function to check for colors#
  areColors <- function(x) {#
    sapply(x, function(X) {#
      tryCatch(is.matrix(col2rgb(X)),#
               error = function(e) FALSE)#
    })#
  }#
#
  ###Check Inputs#
  if (missing(Y)) stop('"Y" is missing')#
  if (missing(Location)) stop('"Location" is missing')#
  if (missing(Time)) stop('"Time" is missing')#
  if (!is.character(main)) stop('"main" must be a character string')#
  if (!is.character(xlab)) stop('"xlab" must be a character string')#
  if (!is.character(ylab)) stop('"ylab" must be a character string')#
  if (!any(areColors(col.line))) stop('"col.line" can only include colors')#
  if (!is.logical(reg.line)) stop('"reg.line" must be a logical')#
  ###Function inputs#
  # Y <- YObserved#
  # Location <- Location#
  # Time <- Time#
  # main = "Visual field sensitivity time series \n at each location"#
  # xlab = "Time from first visit (days)"#
  # ylab = "Sensitivity (dB)"#
  pardefault <- suppressWarnings(par(no.readonly = T))#
#
  ###Collect and sort data#
  VF <- data.frame(cbind(Location, Time, Y))#
  VF <- VF[order(VF$Time), ]#
  VF <- VF[order(VF$Location), ]#
#
  ###Compute Summary Statistics#
  max.VF <- max(abs(range(Y)))#
  max.Time <- max(abs(Time))#
  y_breaks <- round(seq(0, 40, by = 10))#
  x_breaks <- round(seq(0, 100*(max.Time%/%100 + as.logical(max.Time%%100)), by = 100)) #Round up to the nearest 100th#
#
  ###Create layout matrix#
  layout.matrix<-matrix(c(0,0,0,1,2,3,4,0,0,#
                          0,0,5,6,7,8,9,10,0,#
                          0,11,12,13,14,15,16,17,18,#
                          19,20,21,22,23,24,25,26,27,#
                          28,29,30,31,32,33,34,35,36,#
                          0,37,38,39,40,41,42,43,44,#
                          0,0,45,46,47,48,49,50,0,#
                          0,0,0,51,52,53,54,0,0),nrow=8,ncol=9,byrow=TRUE)#
  pp<-layout(layout.matrix,rep(1,3),rep(1,9),TRUE)#
  # layout.show(pp)#
#
  ###Clarify Blind Spot#
  all <- 1 : max(VF$Location)#
  blind_spot <- c(26, 35)#
  remaining <- all[-blind_spot]#
#
  ###Plot Time Series at Each Location#
  par(mar = c(0, 0, 0, 0), oma = c(5, 10, 10, 5), mgp = c(3, 1, 0))#
  for (i in 1 : max(VF$Location)) {#
    if (i %in% remaining) {#
      ph <- VF[VF$Location == i, ]#
      plot(ph[ , 2], ph[ , 3], type = "l", xaxt = "n", yaxt = "n", xlim = c(0, max.Time), ylim = c(0, 40))#
      points(ph[ , 2], ph[ , 3], pch = ".")#
      if (reg.line) abline(lm(ph[ , 3] ~ ph[ , 2]), col = col.line)#
    }#
    if (i %in% blind_spot) plot(ph[ , 2], ph[ , 3], type = "n", xaxt = "n", yaxt = "n", xlim = c(0, max.Time), ylim = c(0, 40))#
    if (i %in% c(52, 54)) axis(1, at = x_breaks)#
    if (i %in% c(1, 3)) axis(3, at = x_breaks)#
    if (i %in% c(5, 19, 37, 51)) axis(2, at = y_breaks, las = 2)#
    if (i %in% c(4, 18, 36, 50)) axis(4, at = y_breaks, las = 2)#
  }#
#
  ###Add Title#
  title(main = list(main, cex = 2.5, col = "black", font = 2),#
        xlab = list(xlab, cex = 2, col = "black", font = 1),#
        ylab = list(ylab, cex = 2, col = "black", font = 1), outer = TRUE)#
#
  ###Return par to default#
  suppressMessages(par(pardefault))#
#
###End function#
}
PlotVfTimeSeries(Y = VFSeries$DLS,#
                 Location = VFSeries$Location,#
                 Time = VFSeries$Time,#
                 main = "Visual field sensitivity time series \n at each location",#
                 xlab = "Days from baseline visit",#
                 ylab = "Differential light sensitivity (dB)")
PlotVfTimeSeries(Y = VFSeries$DLS,#
                 Location = VFSeries$Location,#
                 Time = VFSeries$Time,#
                 main = "Visual field sensitivity time series \n at each location",#
                 xlab = "Days from baseline visit",#
                 ylab = "Differential light sensitivity (dB)", col.line = "black")
PlotVfTimeSeries(Y = VFSeries$DLS,#
                 Location = VFSeries$Location,#
                 Time = VFSeries$Time,#
                 main = "Visual field sensitivity time series \n at each location",#
                 xlab = "Days from baseline visit",#
                 ylab = "Differential light sensitivity (dB)", col.line = "black", line = FALSE)
PlotVfTimeSeries(Y = VFSeries$DLS,#
                 Location = VFSeries$Location,#
                 Time = VFSeries$Time,#
                 main = "Visual field sensitivity time series \n at each location",#
                 xlab = "Days from baseline visit",#
                 ylab = "Differential light sensitivity (dB)")
help(PlotVFTimeSeries)
PlotVfTimeSeries(Y = VFSeries$DLS,#
                 Location = VFSeries$Location,#
                 Time = VFSeries$Time,#
                 main = "Visual field sensitivity time series \n at each location",#
                 xlab = "Days from baseline visit",#
                 ylab = "Differential light sensitivity (dB)", col.line = "black", reg.line = FALSE)
line.type = 1
###Function for plotting a time series of data at each location on the visual field#
#'#
#' PlotVfTimeSeries#
#'#
#' Plots a time series at each location of the Humphrey Field Analyzer-II visual field .#
#'#
#' @param Y a time series variable to be plotted.#
#'#
#' @param Location a variable corresponding to the location on the visual field#
#'  that the time series variable was observed.#
#'#
#' @param Time a variable corresponding to the time that the time series variable#
#'  was observed.#
#'#
#' @param main an overall title for the plot.#
#'#
#' @param xlab a title for the x axis.#
#'#
#' @param ylab a title for the y axis.#
#'#
#' @param line.col color for the regression line, either character string corresponding#
#'  to a color or a integer (default = "red").#
#'#
#' @param line.reg logical, determines if there are regression lines printed (default = TRUE).#
#'#
#' @param line.type integer, specifies the type of regression line printed (default = 1).#
#'#
#' @details \code{PlotVfTimeSeries} is used in the application of glaucoma progression.#
#'  In each cell is the observed DLS at each location over visits, with the red line#
#'  representing a linear regression trend.#
#'#
#' @examples#
#' data(VFSeries)#
#' PlotVfTimeSeries(Y = VFSeries$DLS,#
#'                   Location = VFSeries$Location,#
#'                   Time = VFSeries$Time,#
#'                   main = "Visual field sensitivity time series \n at each location",#
#'                   xlab = "Days from baseline visit",#
#'                   ylab = "Differential light sensitivity (dB)")#
#'#
#'#
#' @author Samuel I. Berchuck#
#'#
#' @export#
PlotVfTimeSeries <- function(Y, Location, Time,#
                             main = "Visual field sensitivity time series \n at each location",#
                             xlab = "Time from first visit (days)",#
                             ylab = "Sensitivity (dB)",#
                             line.col = "red",#
                             line.reg = TRUE,#
                             line.type = 1) {#
  ###Logical function to check for colors#
  areColors <- function(x) {#
    sapply(x, function(X) {#
      tryCatch(is.matrix(col2rgb(X)),#
               error = function(e) FALSE)#
    })#
  }#
#
  ###Check Inputs#
  if (missing(Y)) stop('"Y" is missing')#
  if (missing(Location)) stop('"Location" is missing')#
  if (missing(Time)) stop('"Time" is missing')#
  if (!is.character(main)) stop('"main" must be a character string')#
  if (!is.character(xlab)) stop('"xlab" must be a character string')#
  if (!is.character(ylab)) stop('"ylab" must be a character string')#
  if (!any(areColors(line.col))) stop('"line.col" can only include colors')#
  if (!is.logical(line.reg)) stop('"line.reg" must be a logical')#
  if (!is.integer(line.type)) stop('"line.type" must be an integer')#
#
  ###Function inputs#
  # Y <- YObserved#
  # Location <- Location#
  # Time <- Time#
  # main = "Visual field sensitivity time series \n at each location"#
  # xlab = "Time from first visit (days)"#
  # ylab = "Sensitivity (dB)"#
  pardefault <- suppressWarnings(par(no.readonly = T))#
#
  ###Collect and sort data#
  VF <- data.frame(cbind(Location, Time, Y))#
  VF <- VF[order(VF$Time), ]#
  VF <- VF[order(VF$Location), ]#
#
  ###Compute Summary Statistics#
  max.VF <- max(abs(range(Y)))#
  max.Time <- max(abs(Time))#
  y_breaks <- round(seq(0, 40, by = 10))#
  x_breaks <- round(seq(0, 100*(max.Time%/%100 + as.logical(max.Time%%100)), by = 100)) #Round up to the nearest 100th#
#
  ###Create layout matrix#
  layout.matrix<-matrix(c(0,0,0,1,2,3,4,0,0,#
                          0,0,5,6,7,8,9,10,0,#
                          0,11,12,13,14,15,16,17,18,#
                          19,20,21,22,23,24,25,26,27,#
                          28,29,30,31,32,33,34,35,36,#
                          0,37,38,39,40,41,42,43,44,#
                          0,0,45,46,47,48,49,50,0,#
                          0,0,0,51,52,53,54,0,0),nrow=8,ncol=9,byrow=TRUE)#
  pp<-layout(layout.matrix,rep(1,3),rep(1,9),TRUE)#
  # layout.show(pp)#
#
  ###Clarify Blind Spot#
  all <- 1 : max(VF$Location)#
  blind_spot <- c(26, 35)#
  remaining <- all[-blind_spot]#
#
  ###Plot Time Series at Each Location#
  par(mar = c(0, 0, 0, 0), oma = c(5, 10, 10, 5), mgp = c(3, 1, 0))#
  for (i in 1 : max(VF$Location)) {#
    if (i %in% remaining) {#
      ph <- VF[VF$Location == i, ]#
      plot(ph[ , 2], ph[ , 3], type = "l", xaxt = "n", yaxt = "n", xlim = c(0, max.Time), ylim = c(0, 40))#
      points(ph[ , 2], ph[ , 3], pch = ".")#
      if (line.reg) abline(lm(ph[ , 3] ~ ph[ , 2]), col = line.col, lty = line.type)#
    }#
    if (i %in% blind_spot) plot(ph[ , 2], ph[ , 3], type = "n", xaxt = "n", yaxt = "n", xlim = c(0, max.Time), ylim = c(0, 40))#
    if (i %in% c(52, 54)) axis(1, at = x_breaks)#
    if (i %in% c(1, 3)) axis(3, at = x_breaks)#
    if (i %in% c(5, 19, 37, 51)) axis(2, at = y_breaks, las = 2)#
    if (i %in% c(4, 18, 36, 50)) axis(4, at = y_breaks, las = 2)#
  }#
#
  ###Add Title#
  title(main = list(main, cex = 2.5, col = "black", font = 2),#
        xlab = list(xlab, cex = 2, col = "black", font = 1),#
        ylab = list(ylab, cex = 2, col = "black", font = 1), outer = TRUE)#
#
  ###Return par to default#
  suppressMessages(par(pardefault))#
#
###End function#
}
PlotVfTimeSeries(Y = VFSeries$DLS,#
                 Location = VFSeries$Location,#
                 Time = VFSeries$Time,#
                 main = "Visual field sensitivity time series \n at each location",#
                 xlab = "Days from baseline visit",#
                 ylab = "Differential light sensitivity (dB)")
line.type = 1
!is.integer(line.type)
is.integer(line.type)
line.type
is.integer(line.type)
is.integer(1)
is.integer(2)
help(is.integer)
is.wholenumber <- function(x, tol = .Machine$double.eps^0.5) abs(x - round(x)) < tol
is.wholenumber(1)
###Function for plotting a time series of data at each location on the visual field#
#'#
#' PlotVfTimeSeries#
#'#
#' Plots a time series at each location of the Humphrey Field Analyzer-II visual field .#
#'#
#' @param Y a time series variable to be plotted.#
#'#
#' @param Location a variable corresponding to the location on the visual field#
#'  that the time series variable was observed.#
#'#
#' @param Time a variable corresponding to the time that the time series variable#
#'  was observed.#
#'#
#' @param main an overall title for the plot.#
#'#
#' @param xlab a title for the x axis.#
#'#
#' @param ylab a title for the y axis.#
#'#
#' @param line.col color for the regression line, either character string corresponding#
#'  to a color or a integer (default = "red").#
#'#
#' @param line.reg logical, determines if there are regression lines printed (default = TRUE).#
#'#
#' @param line.type integer, specifies the type of regression line printed (default = 1).#
#'#
#' @details \code{PlotVfTimeSeries} is used in the application of glaucoma progression.#
#'  In each cell is the observed DLS at each location over visits, with the red line#
#'  representing a linear regression trend.#
#'#
#' @examples#
#' data(VFSeries)#
#' PlotVfTimeSeries(Y = VFSeries$DLS,#
#'                   Location = VFSeries$Location,#
#'                   Time = VFSeries$Time,#
#'                   main = "Visual field sensitivity time series \n at each location",#
#'                   xlab = "Days from baseline visit",#
#'                   ylab = "Differential light sensitivity (dB)")#
#'#
#'#
#' @author Samuel I. Berchuck#
#'#
#' @export#
PlotVfTimeSeries <- function(Y, Location, Time,#
                             main = "Visual field sensitivity time series \n at each location",#
                             xlab = "Time from first visit (days)",#
                             ylab = "Sensitivity (dB)",#
                             line.col = "red",#
                             line.reg = TRUE,#
                             line.type = 1) {#
  ###Logical function to check for colors#
  areColors <- function(x) {#
    sapply(x, function(X) {#
      tryCatch(is.matrix(col2rgb(X)),#
               error = function(e) FALSE)#
    })#
  }#
#
  ###Check Inputs#
  is.wholenumber <- function(x, tol = .Machine$double.eps^0.5) abs(x - round(x)) < tol#
  if (missing(Y)) stop('"Y" is missing')#
  if (missing(Location)) stop('"Location" is missing')#
  if (missing(Time)) stop('"Time" is missing')#
  if (!is.character(main)) stop('"main" must be a character string')#
  if (!is.character(xlab)) stop('"xlab" must be a character string')#
  if (!is.character(ylab)) stop('"ylab" must be a character string')#
  if (!any(areColors(line.col))) stop('"line.col" can only include colors')#
  if (!is.logical(line.reg)) stop('"line.reg" must be a logical')#
  if (!is.wholenumber(line.type)) stop('"line.type" must be an integer')#
#
  ###Function inputs#
  # Y <- YObserved#
  # Location <- Location#
  # Time <- Time#
  # main = "Visual field sensitivity time series \n at each location"#
  # xlab = "Time from first visit (days)"#
  # ylab = "Sensitivity (dB)"#
  pardefault <- suppressWarnings(par(no.readonly = T))#
#
  ###Collect and sort data#
  VF <- data.frame(cbind(Location, Time, Y))#
  VF <- VF[order(VF$Time), ]#
  VF <- VF[order(VF$Location), ]#
#
  ###Compute Summary Statistics#
  max.VF <- max(abs(range(Y)))#
  max.Time <- max(abs(Time))#
  y_breaks <- round(seq(0, 40, by = 10))#
  x_breaks <- round(seq(0, 100*(max.Time%/%100 + as.logical(max.Time%%100)), by = 100)) #Round up to the nearest 100th#
#
  ###Create layout matrix#
  layout.matrix<-matrix(c(0,0,0,1,2,3,4,0,0,#
                          0,0,5,6,7,8,9,10,0,#
                          0,11,12,13,14,15,16,17,18,#
                          19,20,21,22,23,24,25,26,27,#
                          28,29,30,31,32,33,34,35,36,#
                          0,37,38,39,40,41,42,43,44,#
                          0,0,45,46,47,48,49,50,0,#
                          0,0,0,51,52,53,54,0,0),nrow=8,ncol=9,byrow=TRUE)#
  pp<-layout(layout.matrix,rep(1,3),rep(1,9),TRUE)#
  # layout.show(pp)#
#
  ###Clarify Blind Spot#
  all <- 1 : max(VF$Location)#
  blind_spot <- c(26, 35)#
  remaining <- all[-blind_spot]#
#
  ###Plot Time Series at Each Location#
  par(mar = c(0, 0, 0, 0), oma = c(5, 10, 10, 5), mgp = c(3, 1, 0))#
  for (i in 1 : max(VF$Location)) {#
    if (i %in% remaining) {#
      ph <- VF[VF$Location == i, ]#
      plot(ph[ , 2], ph[ , 3], type = "l", xaxt = "n", yaxt = "n", xlim = c(0, max.Time), ylim = c(0, 40))#
      points(ph[ , 2], ph[ , 3], pch = ".")#
      if (line.reg) abline(lm(ph[ , 3] ~ ph[ , 2]), col = line.col, lty = line.type)#
    }#
    if (i %in% blind_spot) plot(ph[ , 2], ph[ , 3], type = "n", xaxt = "n", yaxt = "n", xlim = c(0, max.Time), ylim = c(0, 40))#
    if (i %in% c(52, 54)) axis(1, at = x_breaks)#
    if (i %in% c(1, 3)) axis(3, at = x_breaks)#
    if (i %in% c(5, 19, 37, 51)) axis(2, at = y_breaks, las = 2)#
    if (i %in% c(4, 18, 36, 50)) axis(4, at = y_breaks, las = 2)#
  }#
#
  ###Add Title#
  title(main = list(main, cex = 2.5, col = "black", font = 2),#
        xlab = list(xlab, cex = 2, col = "black", font = 1),#
        ylab = list(ylab, cex = 2, col = "black", font = 1), outer = TRUE)#
#
  ###Return par to default#
  suppressMessages(par(pardefault))#
#
###End function#
}
PlotVfTimeSeries(Y = VFSeries$DLS,#
                 Location = VFSeries$Location,#
                 Time = VFSeries$Time,#
                 main = "Visual field sensitivity time series \n at each location",#
                 xlab = "Days from baseline visit",#
                 ylab = "Differential light sensitivity (dB)")
PlotVfTimeSeries(Y = VFSeries$DLS,#
                 Location = VFSeries$Location,#
                 Time = VFSeries$Time,#
                 main = "Visual field sensitivity time series \n at each location",#
                 xlab = "Days from baseline visit",#
                 ylab = "Differential light sensitivity (dB)", line.col = "black")
PlotVfTimeSeries(Y = VFSeries$DLS,#
                 Location = VFSeries$Location,#
                 Time = VFSeries$Time,#
                 main = "Visual field sensitivity time series \n at each location",#
                 xlab = "Days from baseline visit",#
                 ylab = "Differential light sensitivity (dB)", line.col = "black", line.type = 2)
PlotVfTimeSeries(Y = VFSeries$DLS,#
                 Location = VFSeries$Location,#
                 Time = VFSeries$Time,#
                 main = "Visual field sensitivity time series \n at each location",#
                 xlab = "Days from baseline visit",#
                 ylab = "Differential light sensitivity (dB)", line.col = "black", line.type = 3)
PlotVfTimeSeries(Y = VFSeries$DLS,#
                 Location = VFSeries$Location,#
                 Time = VFSeries$Time,#
                 main = "Visual field sensitivity time series \n at each location",#
                 xlab = "Days from baseline visit",#
                 ylab = "Differential light sensitivity (dB)", line.col = "black", line.type = 4)
rm(list = ls())#
#
###Read in data (Brigid)#
glaucoma <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/all_josh.csv")#
class <- read.csv("/Users/Sam/Documents/Sam/School/Dissertation/Data/Brigid\ Data/class_josh.csv")#
#
###Format data#
glaucoma <- glaucoma[order(glaucoma$field), ]#
glaucoma <- glaucoma[order(glaucoma$eye), ]#
glaucoma <- glaucoma[order(glaucoma$patid), ]#
Eyes <- data.frame(unique(cbind(glaucoma$patid, glaucoma$eye)))#
Eyes[, 2] <- factor(Eyes[,2])#
levels(Eyes[ ,2]) <- c("L", "R")#
neyes <- dim(Eyes)[1]#
#
###Coefficient of variation function#
CV <- function(x) sd(x) / mean(x)#
#
###Loop over eyes#
MeanCV <- SdCV <- Prog <- NULL#
for (i in 1:neyes) {#
	###Subset to eye#
	id <- Eyes[i, 1]	#
	eye <- as.character(Eyes[i, 2])#
	dat <- glaucoma[(glaucoma$patid == id) & (glaucoma$eye == eye), ] #subset to id and eye#
	dat <- dat[order(dat$field), ] #sort by visit#
	dat <- dat[!dat$point_ID %in% c(26, 35), ] #remove blind spot locations#
	Nu <- length(unique(dat$field)) #number of visits#
	M <- length(unique(dat$point_ID)) #number of locations#
	YWide <- matrix(dat$sens_raw, nrow = M, ncol = Nu) / 10#
#
	###Calculate Mean and Sd CV#
	MeanCV <- c(MeanCV, CV(apply(YWide, 2, mean)))#
#
	###Logistic Regression#
	temp<-class[class$patid == id, ]#
	if (eye=="L") prog<-temp[1,3]#
	if (eye=="R") prog<-temp[1,2]#
	Prog <- c(Prog, prog)#
#
}#
#
###Create Metrics#
Metrics <- data.frame(Patid = Eyes[ ,1], Left = 1*(Eyes[ , 2] == "L"), Prog, MeanCV)
Metrics
library(pROC)#
#
dat[order(dat$MeanCV),]
Metrics
dat[order(dat$MeanCV),]
dat[order(dat$MeanCV), ]
Metrics[order(Metrics$MeanCV), ]
NBurn=1000
NSims=10000
NThin=1
NBurn=10000
NSims=250000
NThin=25
NSims<-NSims#
NBurn<-NBurn#
NThin<-NThin#
NTotal<-NBurn+NSims
NKeep<-NBurn+(1:(NSims/NThin))*NThin
NKeep
WhichKeep<-NBurn+(1:(NSims/NThin))*NThin
WhichKeep
NKeep<-length(WhichKeep)
NKeep
RawSamples<-matrix(nrow=3,ncol=NKeep)
RawSamples
WhichKeep
s<-10025
which(s==WhichKeep)
####Lee and Mitchell 2011 Space Only Model #
SpaceOnly<-function(#
	Y=Yreduced, #m x 1 matrix of sensitivity outcome#
	Z=Zreduced, #a x 1 matrix of dissimilarity differences, where a is number of adjacnecy edges #
	W=Wreduced, #m x m adjacnecy matrix#
	Inits=list(mu=2.5,tau2=1,alpha=1/2), #Initial values of parameters #
	Hypers=list(mu=10e4,tau2=c(3,1),alpha=10), #Prior hyperparameters #
	delta=1, #Metropolis tuning parameter#
	rho=0.99, #Fixed propriety parameter#
	Scale=10, #Scale for the outcome#
	NBurn=10000, #Number of burn-in samples #
	NSims=250000, #Number of simulations after burn-in#
	NThin=25, #Interval for which samples are saved after burn-in, must be a factor of NSims#
	Proposal="normal", #"normal" or "uniform"#
	Verbose=FALSE) {#
#
###Set Seed#
set.seed(54)#
#
###Load Libraries#
suppressMessages(library(mvtnorm)) #multivariate normal#
suppressMessages(library(pscl)) #inverse gamma#
suppressMessages(library(msm)) #truncated normal#
suppressMessages(library(coda)) #mcmc#
#
###Format Data#
YScaled<-Y/Scale #scale#
m<-length(YScaled)#
#
###Math Objects#
Onem<-matrix(rep(1,m),nrow=m,ncol=1)#
Eyem<-diag(m)#
#
###Fixed Adjacency Matrix#
Wfixed<-W#
#
###Compute Adjacent Edge Locations Boolean#
AdjacentEdgesBoolean<-obtainBooleanEdges(Wfixed)#
#
###Create Global Objects For W.alpha Function#
assign("Z",Z,envir=globalenv())#
assign("Wfixed",Wfixed,envir=globalenv())#
assign("AdjacentEdgesBoolean", AdjacentEdgesBoolean,envir=globalenv())#
#
###Hyperparameters#
ATau2<-Hypers$tau2[1]#
BTau2<-Hypers$tau2[2]#
SigmaMu2<-Hypers$mu[1]#
BAlpha<-Hypers$alpha#
#
###Initial Values#
#
	##Parameters #
	mu<-Inits$mu#
	tau2<-Inits$tau2#
	alpha<-Inits$alpha#
#
	##Covariance Objects#
	WAlpha<-W.alpha(alpha)#
	DwAlpha<-diag(apply(WAlpha,1,sum))#
	WStar<-DwAlpha-WAlpha#
	Q<-rho*WStar+(1-rho)*Eyem#
	QInverse<-chol2inv(chol(Q))#
#
###Alpha Metropolis Objects#
delta<-delta#
AcceptanceAlphaCounter<-0#
PilotAdaptationCounter<-1#
#
###Tobit Information#
TobitBoolean<-Y==0#
NumberOfZeros<-sum(TobitBoolean)#
LocationOfZeros<-which(TobitBoolean) #
YStarNonZero<-YScaled[!TobitBoolean]#
#
###MCMC Objects#
NSims<-NSims#
NBurn<-NBurn#
NThin<-NThin#
NTotal<-NBurn+NSims#
WhichKeep<-NBurn+(1:(NSims/NThin))*NThin#
NKeep<-length(WhichKeep)#
OutProgress<-(1:10)*NTotal/10#
PilotAdapt<-(1:10)*NBurn/10#
#
###Output Matrix#
RawSamples<-matrix(nrow=3,ncol=NKeep)#
#
###Time MCMC Sampler#
BeginTime<-Sys.time()#
#
###Begin Gibbs Sampler#
for (s in 1:NTotal) {#
	##Tobit Step#
	if (NumberOfZeros==0) YStar<-YScaled#
	if (NumberOfZeros!=0) {#
		#Initiate Tobit Step#
		YStar<-YScaled#
		#Loop over Latent Observations (since they are dependent, can't vectorize)#
		for (i in 1:NumberOfZeros) {#
			#Subset over Tobit Observations#
			ZeroLocationIndex<-LocationOfZeros[i]#
			#Compute Conditional Mean and Variance#
			ConditionalVariance<-tau2/(rho*(WAlpha[ZeroLocationIndex,]%*%Onem)+(1-rho))#
			ConditionalMean<-ConditionalVariance*((rho*(WAlpha[ZeroLocationIndex,]%*%YStar)+(1-rho)*mu)/tau2)#
			#Sample Latent Variable From Full Conditional#
			YStar[ZeroLocationIndex]<-rtnorm(1,ConditionalMean,sqrt(ConditionalVariance),upper=0)#
		}#
	}#
	##Full Conditional For Mu#
	MuVariance<-1/(t(Onem)%*%Q%*%Onem/tau2+1/SigmaMu2)#
	MuMean<-MuVariance*(t(Onem)%*%Q%*%YStar/tau2)#
	mu<-rnorm(1,MuMean,sqrt(MuVariance))#
	##Full Conditional For Tau2#
	ShapeTau2<-ATau2+m/2#
	Residuals<-(YStar-mu*Onem)#
	ScaleTau2<-BTau2+t(Residuals)%*%Q%*%Residuals/2#
	tau2<-rigamma(1,ShapeTau2,ScaleTau2)#
#
	##Reflecting Uniform Proposal#
	if (Proposal=="uniform") {#
	##Metropolis Step For Alpha#
		#Sample Proposal Value Of Alpha#
		AlphaProposal<-runif(1,alpha-delta,alpha+delta)	#
		if (AlphaProposal<=0) AlphaProposal<-abs(AlphaProposal)#
		if (AlphaProposal>BAlpha) AlphaProposal<-(2*BAlpha-AlphaProposal)#
		#Compute W(alpha) and Covariance Proposal#
		WAlphaProposal<-W.alpha(AlphaProposal)#
		DwAlphaProposal<-diag(apply(WAlphaProposal,1,sum))#
		WStarProposal<-DwAlphaProposal-WAlphaProposal#
		QProposal<-rho*WStarProposal+(1-rho)*Eyem#
		#Compute Log Acceptance Ratio#
		LogR<-(1/2)*log(det(QProposal)/det(Q))-(1/(2*tau2))*t(Residuals)%*%(QProposal-Q)%*%Residuals#
		#Metropolis Update#
		if (log(runif(1))<LogR) {#
			#Update Alpha#
			alpha<-AlphaProposal#
			AcceptanceAlphaCounter<-AcceptanceAlphaCounter+1#
			#Update Covariance and W#
			WAlpha<-WAlphaProposal#
			Q<-QProposal#
			QInverse<-chol2inv(chol(Q))#
#
		}#
	}	#
#
	##Normal Transformation Proposal#
	if (Proposal=="normal") {#
	##Metropolis Step For Alpha#
		#Transform Current State#
		Theta<-log(alpha/(BAlpha-alpha))#
		#Sample Transformed Proposal#
		ThetaProposal<-rnorm(1,Theta,delta)	#
		#Compute Alpha Proposal#
		AlphaProposal<-(BAlpha*exp(ThetaProposal))/(1+exp(ThetaProposal))#
		#Compute W(alpha) and Covariance Proposal#
		WAlphaProposal<-W.alpha(AlphaProposal)#
		DwAlphaProposal<-diag(apply(WAlphaProposal,1,sum))#
		WStarProposal<-DwAlphaProposal-WAlphaProposal#
		QProposal<-rho*WStarProposal+(1-rho)*Eyem#
		#Compute Log Acceptance Ratio#
		LogR<-(1/2)*log(det(QProposal)/det(Q))-(1/(2*tau2))*t(Residuals)%*%(QProposal-Q)%*%Residuals+(ThetaProposal-Theta)+2*log((1+exp(Theta))/(1+exp(ThetaProposal)))	#
		#Metropolis Update#
		if (log(runif(1))<LogR) {#
			#Update Alpha#
			alpha<-AlphaProposal#
			AcceptanceAlphaCounter<-AcceptanceAlphaCounter+1#
			#Update Covariance and W#
			WAlpha<-WAlphaProposal#
			Q<-QProposal#
			QInverse<-chol2inv(chol(Q))#
#
		}#
	}	#
#
	##Manage Acceptance Rates (Pilot Adaptation)#
	if (s %in% PilotAdapt) {#
#
		###Metropolis Tuning Parameter#
		AcceptancePct<-round(AcceptanceAlphaCounter/PilotAdaptationCounter,digits=2)#
		if (AcceptancePct>=0.90) delta<-delta+delta*0.3#
		if (AcceptancePct>=0.75) delta<-delta+delta*0.2#
		if (AcceptancePct>=0.45) delta<-delta+delta*0.1#
		if (AcceptancePct<=0.25) delta<-delta-delta*0.1#
		if (AcceptancePct<=0.15) delta<-delta-delta*0.2#
		if (AcceptancePct<=0.10) delta<-delta-delta*0.3	#
		AcceptanceAlphaCounter<-PilotAdaptationCounter<-0#
#
	}#
	PilotAdaptationCounter<-PilotAdaptationCounter+1#
#
	if (s%in%WhichKeep) {#
		Save <- c(mu,tau2,alpha)#
		RawSamples[which(s==WhichKeep),Save]#
	}			#
#
	##Output Verbose#
	if (Verbose) {#
		cat(paste("Completed Percentage: ",round((s/NTotal)*100,digits=0),"%\n",sep=""))#
		cat(paste("Mu: ",round(mu,digits=3),"\n",sep=""))#
		cat(paste("Tau2: ",round(tau2,digits=3),"\n",sep=""))#
		cat(paste("Alpha: ",round(alpha,digits=3),", Acceptance Rate: ",round(AcceptanceAlphaCounter/PilotAdaptationCounter,digits=2),"\n",sep=""))#
		cat(rep("-",75),"\n",sep="")#
	}#
#
	##Output Progress#
	if (!Verbose) {#
		if (s%in%OutProgress) cat(round(100*(s/NTotal)), "%..  ", sep="")	#
		if (s==1) {#
			cat(rep("-",75),"\n",sep="")#
			cat("0%..  ", sep="")#
		}#
	}#
	##Output MCMC Sampler Run Time#
	if (s==NTotal) {#
		EndTime<-Sys.time()#
		RunTime<-EndTime-BeginTime#
		cat(paste("Run Time: ",round(RunTime,digits=2)," ",attr(RunTime,"unit"),"\n",sep=""))#
		cat(rep("-",75),"\n",sep="")#
	}#
#
###End MCMC Sampler#
}#
#
###Read in Posterior MCMC Samples#
Mu<-RawSamples[1,]#
Alpha<-RawSamples[3,]#
Tau2<-RawSamples[2,]#
#
###Create MCMC Objects#
MuMCMC<-as.mcmc(Mu)#
AlphaMCMC<-as.mcmc(Alpha)#
Tau2MCMC<-as.mcmc(Tau2)#
#
###Summarize Metropolis Output#
Metropolis<-list(round(AcceptanceAlphaCounter/PilotAdaptationCounter,digits=2),delta)#
names(Metropolis)<-c("accpetance","delta")#
#
###Summarize Output and Return#
FinalObject<-list(MuMCMC,Tau2MCMC,AlphaMCMC,Metropolis)#
names(FinalObject)<-c("mu","tau2","alpha","metropolis")#
return(FinalObject)#
#
####End Space Only Function#
}
###Start with a clean workspace#
rm(list = ls())#
#
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
###Source scripts used to fit space only model to create SpaceCV metric#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/adjacency.R")) # Original Adjacency Matrix#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/opticnervedegree.R")) # Optic Nerve Degree (object called degree)#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/ComputeDissimilarityMetric.R")) # Dissimilarity Metric Functions#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/W.alpha.R")) # W(alpha) Function#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/SpaceOnly2.R")) # Lee and Mitchell 2011 Space Model
###Start with a clean workspace#
rm(list = ls())#
#
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
###Source scripts used to fit space only model to create SpaceCV metric#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/adjacency.R")) # Original Adjacency Matrix#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/opticnervedegree.R")) # Optic Nerve Degree (object called degree)#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/ComputeDissimilarityMetric.R")) # Dissimilarity Metric Functions#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/W.alpha.R")) # W(alpha) Function#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/SpaceOnly2.R")) # Lee and Mitchell 2011 Space Model#
###Define blind spot#
blind_spot <- c(26, 35)#
###Create original adjacency matrix (same as HFAII_Queen in womblR package)#
Woriginal <- adj.mat(Queen)#
Wreduced <- Woriginal[-blind_spot, -blind_spot]#
###Load dissimilarity metric#
scale_degree <- 100#
Zoriginal <- obtainDissimilarityMetric(degree / scale_degree, Woriginal)#
Zreduced <- obtainDissimilarityMetric(degree[-blind_spot] / scale_degree, Wreduced)#
###Get phi upper bound#
b_alpha <- log(0.50) / -min(Zreduced[Zreduced>0]) #original definition used by Lee and Mitchell#
###Load visual field data for example patient#
load(paste0(berchuck.dir, "/berchuck_code/Data/VFSeries.RData"))#
VFSeries <- VFSeries[order(VFSeries$Location), ] # sort by location#
VFSeries <- VFSeries[order(VFSeries$Visit), ] # sort by visit#
VFSeries <- VFSeries[!VFSeries$Location %in% blind_spot, ] # remove blind spot locations#
###Set Data Objects#
Nu <- length(unique(VFSeries$Visit))#
M <- length(unique(VFSeries$Location))#
Ymat <- matrix(VFSeries$DLS, nrow = M, ncol = Nu)#
###MCMC Sampler Inputs#
NSims <- 250000#
NBurn <- 10000#
NThin <- 25#
###Create MCMC output folder#
# system(paste0("mkdir ", berchuck.dir, "/berchuck_code/SpaceOnlyModel/RawMCMC"))#
#
###Must Loop over visits (since this is a space only model)#
Reg<-list()#
for (i in 1:Nu) {#
	###Set Data at Visit i#
	Yreduced <- matrix(Ymat[,i], nrow=M, ncol=1)#
#
	###Fit OPOV (i.e. Duncan Lee Model)#
	Reg[[i]]<-SpaceOnly(Y = Yreduced, Z = Zreduced, W = Wreduced, Inits = list(mu = 2.5, tau2 = 1, alpha = 1/2),#
						Hypers = list(mu = 10e4, tau2 = c(0.001, 0.001), alpha = b_alpha),#
						delta = 1, NSims = NSims, NBurn = NBurn, NThin = NThin))#
	cat(paste0("Model: ", i," of ", Nu,"\n"))#
#
}
###Start with a clean workspace#
rm(list = ls())#
#
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
###Source scripts used to fit space only model to create SpaceCV metric#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/adjacency.R")) # Original Adjacency Matrix#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/opticnervedegree.R")) # Optic Nerve Degree (object called degree)#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/ComputeDissimilarityMetric.R")) # Dissimilarity Metric Functions#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/W.alpha.R")) # W(alpha) Function#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/SpaceOnly2.R")) # Lee and Mitchell 2011 Space Model#
###Define blind spot#
blind_spot <- c(26, 35)#
###Create original adjacency matrix (same as HFAII_Queen in womblR package)#
Woriginal <- adj.mat(Queen)#
Wreduced <- Woriginal[-blind_spot, -blind_spot]#
###Load dissimilarity metric#
scale_degree <- 100#
Zoriginal <- obtainDissimilarityMetric(degree / scale_degree, Woriginal)#
Zreduced <- obtainDissimilarityMetric(degree[-blind_spot] / scale_degree, Wreduced)#
###Get phi upper bound#
b_alpha <- log(0.50) / -min(Zreduced[Zreduced>0]) #original definition used by Lee and Mitchell#
###Load visual field data for example patient#
load(paste0(berchuck.dir, "/berchuck_code/Data/VFSeries.RData"))#
VFSeries <- VFSeries[order(VFSeries$Location), ] # sort by location#
VFSeries <- VFSeries[order(VFSeries$Visit), ] # sort by visit#
VFSeries <- VFSeries[!VFSeries$Location %in% blind_spot, ] # remove blind spot locations#
###Set Data Objects#
Nu <- length(unique(VFSeries$Visit))#
M <- length(unique(VFSeries$Location))#
Ymat <- matrix(VFSeries$DLS, nrow = M, ncol = Nu)#
###MCMC Sampler Inputs#
NSims <- 250000#
NBurn <- 10000#
NThin <- 25#
###Create MCMC output folder#
# system(paste0("mkdir ", berchuck.dir, "/berchuck_code/SpaceOnlyModel/RawMCMC"))#
#
###Must Loop over visits (since this is a space only model)#
Reg<-list()#
for (i in 1:Nu) {#
	###Set Data at Visit i#
	Yreduced <- matrix(Ymat[,i], nrow=M, ncol=1)#
#
	###Fit OPOV (i.e. Duncan Lee Model)#
	Reg[[i]]<-SpaceOnly(Y = Yreduced, Z = Zreduced, W = Wreduced, Inits = list(mu = 2.5, tau2 = 1, alpha = 1/2),#
						Hypers = list(mu = 10e4, tau2 = c(0.001, 0.001), alpha = b_alpha),#
						delta = 1, NSims = NSims, NBurn = NBurn, NThin = NThin)#
	cat(paste0("Model: ", i," of ", Nu,"\n"))#
#
}
suppressMessages(library(mvtnorm)) #multivariate normal
###Start with a clean workspace#
rm(list = ls())#
#
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
###Source scripts used to fit space only model to create SpaceCV metric#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/adjacency.R")) # Original Adjacency Matrix#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/opticnervedegree.R")) # Optic Nerve Degree (object called degree)#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/ComputeDissimilarityMetric.R")) # Dissimilarity Metric Functions#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/W.alpha.R")) # W(alpha) Function#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/SpaceOnly2.R")) # Lee and Mitchell 2011 Space Model#
###Define blind spot#
blind_spot <- c(26, 35)#
###Create original adjacency matrix (same as HFAII_Queen in womblR package)#
Woriginal <- adj.mat(Queen)#
Wreduced <- Woriginal[-blind_spot, -blind_spot]#
###Load dissimilarity metric#
scale_degree <- 100#
Zoriginal <- obtainDissimilarityMetric(degree / scale_degree, Woriginal)#
Zreduced <- obtainDissimilarityMetric(degree[-blind_spot] / scale_degree, Wreduced)#
###Get phi upper bound#
b_alpha <- log(0.50) / -min(Zreduced[Zreduced>0]) #original definition used by Lee and Mitchell#
###Load visual field data for example patient#
load(paste0(berchuck.dir, "/berchuck_code/Data/VFSeries.RData"))#
VFSeries <- VFSeries[order(VFSeries$Location), ] # sort by location#
VFSeries <- VFSeries[order(VFSeries$Visit), ] # sort by visit#
VFSeries <- VFSeries[!VFSeries$Location %in% blind_spot, ] # remove blind spot locations#
###Set Data Objects#
Nu <- length(unique(VFSeries$Visit))#
M <- length(unique(VFSeries$Location))#
Ymat <- matrix(VFSeries$DLS, nrow = M, ncol = Nu)#
###MCMC Sampler Inputs#
NSims <- 250000#
NBurn <- 10000#
NThin <- 25#
###Create MCMC output folder#
# system(paste0("mkdir ", berchuck.dir, "/berchuck_code/SpaceOnlyModel/RawMCMC"))#
#
###Must Loop over visits (since this is a space only model)#
Reg<-list()#
for (i in 1:Nu) {#
	###Set Data at Visit i#
	Yreduced <- matrix(Ymat[,i], nrow=M, ncol=1)#
#
	###Fit OPOV (i.e. Duncan Lee Model)#
	# Reg[[i]]<-SpaceOnly(Y = Yreduced, Z = Zreduced, W = Wreduced, Inits = list(mu = 2.5, tau2 = 1, alpha = 1/2),#
						# Hypers = list(mu = 10e4, tau2 = c(0.001, 0.001), alpha = b_alpha),#
						# delta = 1, NSims = NSims, NBurn = NBurn, NThin = NThin,#
						# Output = paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/RawMCMC/"))#
	Reg[[i]]<-SpaceOnly(Y = Yreduced, Z = Zreduced, W = Wreduced, Inits = list(mu = 2.5, tau2 = 1, alpha = 1/2),#
						Hypers = list(mu = 10e4, tau2 = c(0.001, 0.001), alpha = b_alpha),#
						delta = 1, NSims = NSims, NBurn = NBurn, NThin = NThin)#
	cat(paste0("Model: ", i," of ", Nu,"\n"))#
#
}#
#
###Delete MCMC output folder#
# #!!!!!!!!#
# #!!!!!!!!#
# #Note: this step can depend on your operating system: #
# system(paste0("rm -rf ", berchuck.dir, "/berchuck_code/SpaceOnlyModel/RawMCMC")) # Mac OSX / Linux#
# system(paste0("rd /s /q", berchuck.dir, "/berchuck_code/SpaceOnlyModel/RawMCMC")) # Windows Command Prompt#
# system(paste0("rd -r", berchuck.dir, "/berchuck_code/SpaceOnlyModel/RawMCMC")) # Windows Powershell#
# ###If none of these work you can manually delete the RawMCMC folder!#
#
###Coefficient of variation function#
cv <- function(x) sd(x) / mean(x)#
#
###Obtain the posterior distribution of the CV of alpha#
Alpha <- NULL#
for (i in 1:Nu) Alpha<-cbind(Alpha, Reg[[i]]$alpha)#
CVAlpha<-apply(Alpha, 1, cv)#
#
###Calculate SpaceCV#
SpaceCV <- mean(CVAlpha)#
#
###Save SpaceCV#
save(SpaceCV, file = paste0(berchuck.dir, "/berchuck_code/DataApplication/Output/SpaceCV.RData"))
s
###Start with a clean workspace#
rm(list = ls())#
#
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
###Source scripts used to fit space only model to create SpaceCV metric#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/adjacency.R")) # Original Adjacency Matrix#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/opticnervedegree.R")) # Optic Nerve Degree (object called degree)#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/ComputeDissimilarityMetric.R")) # Dissimilarity Metric Functions#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/W.alpha.R")) # W(alpha) Function#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/SpaceOnly2.R")) # Lee and Mitchell 2011 Space Model#
###Define blind spot#
blind_spot <- c(26, 35)#
###Create original adjacency matrix (same as HFAII_Queen in womblR package)#
Woriginal <- adj.mat(Queen)#
Wreduced <- Woriginal[-blind_spot, -blind_spot]#
###Load dissimilarity metric#
scale_degree <- 100#
Zoriginal <- obtainDissimilarityMetric(degree / scale_degree, Woriginal)#
Zreduced <- obtainDissimilarityMetric(degree[-blind_spot] / scale_degree, Wreduced)#
###Get phi upper bound#
b_alpha <- log(0.50) / -min(Zreduced[Zreduced>0]) #original definition used by Lee and Mitchell#
###Load visual field data for example patient#
load(paste0(berchuck.dir, "/berchuck_code/Data/VFSeries.RData"))#
VFSeries <- VFSeries[order(VFSeries$Location), ] # sort by location#
VFSeries <- VFSeries[order(VFSeries$Visit), ] # sort by visit#
VFSeries <- VFSeries[!VFSeries$Location %in% blind_spot, ] # remove blind spot locations#
###Set Data Objects#
Nu <- length(unique(VFSeries$Visit))#
M <- length(unique(VFSeries$Location))#
Ymat <- matrix(VFSeries$DLS, nrow = M, ncol = Nu)#
###MCMC Sampler Inputs#
NSims <- 250000#
NBurn <- 10000#
NThin <- 25#
###Create MCMC output folder#
# system(paste0("mkdir ", berchuck.dir, "/berchuck_code/SpaceOnlyModel/RawMCMC"))#
#
###Must Loop over visits (since this is a space only model)#
Reg<-list()
i<-1
###Set Data at Visit i#
	Yreduced <- matrix(Ymat[,i], nrow=M, ncol=1)#
#
	###Fit OPOV (i.e. Duncan Lee Model)#
	# Reg[[i]]<-SpaceOnly(Y = Yreduced, Z = Zreduced, W = Wreduced, Inits = list(mu = 2.5, tau2 = 1, alpha = 1/2),#
						# Hypers = list(mu = 10e4, tau2 = c(0.001, 0.001), alpha = b_alpha),#
						# delta = 1, NSims = NSims, NBurn = NBurn, NThin = NThin,#
						# Output = paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/RawMCMC/"))
Y = Yreduced
Z = Zreduced
W = Wreduced
Inits = list(mu = 2.5, tau2 = 1, alpha = 1/2)
Hypers = list(mu = 10e4, tau2 = c(0.001, 0.001), alpha = b_alpha)
delta = 1
NSims = NSims
NBurn = NBurn
NThin = NThin
Proposal="normal"
Verbose=FALSE
rho=0.99
###Set Seed#
set.seed(54)#
#
###Load Libraries#
suppressMessages(library(mvtnorm)) #multivariate normal#
suppressMessages(library(pscl)) #inverse gamma#
suppressMessages(library(msm)) #truncated normal#
suppressMessages(library(coda)) #mcmc#
#
###Format Data#
YScaled<-Y/Scale #scale#
m<-length(YScaled)
Scale=10
###Format Data#
YScaled<-Y/Scale #scale#
m<-length(YScaled)
###Math Objects#
Onem<-matrix(rep(1,m),nrow=m,ncol=1)#
Eyem<-diag(m)#
#
###Fixed Adjacency Matrix#
Wfixed<-W
###Compute Adjacent Edge Locations Boolean#
AdjacentEdgesBoolean<-obtainBooleanEdges(Wfixed)#
#
###Create Global Objects For W.alpha Function#
assign("Z",Z,envir=globalenv())#
assign("Wfixed",Wfixed,envir=globalenv())#
assign("AdjacentEdgesBoolean", AdjacentEdgesBoolean,envir=globalenv())
###Hyperparameters#
ATau2<-Hypers$tau2[1]#
BTau2<-Hypers$tau2[2]#
SigmaMu2<-Hypers$mu[1]#
BAlpha<-Hypers$alpha#
#
###Initial Values#
#
	##Parameters #
	mu<-Inits$mu#
	tau2<-Inits$tau2#
	alpha<-Inits$alpha#
#
	##Covariance Objects#
	WAlpha<-W.alpha(alpha)#
	DwAlpha<-diag(apply(WAlpha,1,sum))#
	WStar<-DwAlpha-WAlpha#
	Q<-rho*WStar+(1-rho)*Eyem#
	QInverse<-chol2inv(chol(Q))
###Alpha Metropolis Objects#
delta<-delta#
AcceptanceAlphaCounter<-0#
PilotAdaptationCounter<-1#
#
###Tobit Information#
TobitBoolean<-Y==0#
NumberOfZeros<-sum(TobitBoolean)#
LocationOfZeros<-which(TobitBoolean) #
YStarNonZero<-YScaled[!TobitBoolean]#
#
###MCMC Objects#
NSims<-NSims#
NBurn<-NBurn#
NThin<-NThin#
NTotal<-NBurn+NSims#
WhichKeep<-NBurn+(1:(NSims/NThin))*NThin#
NKeep<-length(WhichKeep)#
OutProgress<-(1:10)*NTotal/10#
PilotAdapt<-(1:10)*NBurn/10
###Output Matrix#
RawSamples<-matrix(nrow=3,ncol=NKeep)#
#
###Time MCMC Sampler#
BeginTime<-Sys.time()
###Begin Gibbs Sampler#
for (s in 1:NTotal) {#
	##Tobit Step#
	if (NumberOfZeros==0) YStar<-YScaled#
	if (NumberOfZeros!=0) {#
		#Initiate Tobit Step#
		YStar<-YScaled#
		#Loop over Latent Observations (since they are dependent, can't vectorize)#
		for (i in 1:NumberOfZeros) {#
			#Subset over Tobit Observations#
			ZeroLocationIndex<-LocationOfZeros[i]#
			#Compute Conditional Mean and Variance#
			ConditionalVariance<-tau2/(rho*(WAlpha[ZeroLocationIndex,]%*%Onem)+(1-rho))#
			ConditionalMean<-ConditionalVariance*((rho*(WAlpha[ZeroLocationIndex,]%*%YStar)+(1-rho)*mu)/tau2)#
			#Sample Latent Variable From Full Conditional#
			YStar[ZeroLocationIndex]<-rtnorm(1,ConditionalMean,sqrt(ConditionalVariance),upper=0)#
		}#
	}#
	##Full Conditional For Mu#
	MuVariance<-1/(t(Onem)%*%Q%*%Onem/tau2+1/SigmaMu2)#
	MuMean<-MuVariance*(t(Onem)%*%Q%*%YStar/tau2)#
	mu<-rnorm(1,MuMean,sqrt(MuVariance))#
	##Full Conditional For Tau2#
	ShapeTau2<-ATau2+m/2#
	Residuals<-(YStar-mu*Onem)#
	ScaleTau2<-BTau2+t(Residuals)%*%Q%*%Residuals/2#
	tau2<-rigamma(1,ShapeTau2,ScaleTau2)#
#
	##Reflecting Uniform Proposal#
	if (Proposal=="uniform") {#
	##Metropolis Step For Alpha#
		#Sample Proposal Value Of Alpha#
		AlphaProposal<-runif(1,alpha-delta,alpha+delta)	#
		if (AlphaProposal<=0) AlphaProposal<-abs(AlphaProposal)#
		if (AlphaProposal>BAlpha) AlphaProposal<-(2*BAlpha-AlphaProposal)#
		#Compute W(alpha) and Covariance Proposal#
		WAlphaProposal<-W.alpha(AlphaProposal)#
		DwAlphaProposal<-diag(apply(WAlphaProposal,1,sum))#
		WStarProposal<-DwAlphaProposal-WAlphaProposal#
		QProposal<-rho*WStarProposal+(1-rho)*Eyem#
		#Compute Log Acceptance Ratio#
		LogR<-(1/2)*log(det(QProposal)/det(Q))-(1/(2*tau2))*t(Residuals)%*%(QProposal-Q)%*%Residuals#
		#Metropolis Update#
		if (log(runif(1))<LogR) {#
			#Update Alpha#
			alpha<-AlphaProposal#
			AcceptanceAlphaCounter<-AcceptanceAlphaCounter+1#
			#Update Covariance and W#
			WAlpha<-WAlphaProposal#
			Q<-QProposal#
			QInverse<-chol2inv(chol(Q))#
#
		}#
	}	#
#
	##Normal Transformation Proposal#
	if (Proposal=="normal") {#
	##Metropolis Step For Alpha#
		#Transform Current State#
		Theta<-log(alpha/(BAlpha-alpha))#
		#Sample Transformed Proposal#
		ThetaProposal<-rnorm(1,Theta,delta)	#
		#Compute Alpha Proposal#
		AlphaProposal<-(BAlpha*exp(ThetaProposal))/(1+exp(ThetaProposal))#
		#Compute W(alpha) and Covariance Proposal#
		WAlphaProposal<-W.alpha(AlphaProposal)#
		DwAlphaProposal<-diag(apply(WAlphaProposal,1,sum))#
		WStarProposal<-DwAlphaProposal-WAlphaProposal#
		QProposal<-rho*WStarProposal+(1-rho)*Eyem#
		#Compute Log Acceptance Ratio#
		LogR<-(1/2)*log(det(QProposal)/det(Q))-(1/(2*tau2))*t(Residuals)%*%(QProposal-Q)%*%Residuals+(ThetaProposal-Theta)+2*log((1+exp(Theta))/(1+exp(ThetaProposal)))	#
		#Metropolis Update#
		if (log(runif(1))<LogR) {#
			#Update Alpha#
			alpha<-AlphaProposal#
			AcceptanceAlphaCounter<-AcceptanceAlphaCounter+1#
			#Update Covariance and W#
			WAlpha<-WAlphaProposal#
			Q<-QProposal#
			QInverse<-chol2inv(chol(Q))#
#
		}#
	}	#
#
	##Manage Acceptance Rates (Pilot Adaptation)#
	if (s %in% PilotAdapt) {#
#
		###Metropolis Tuning Parameter#
		AcceptancePct<-round(AcceptanceAlphaCounter/PilotAdaptationCounter,digits=2)#
		if (AcceptancePct>=0.90) delta<-delta+delta*0.3#
		if (AcceptancePct>=0.75) delta<-delta+delta*0.2#
		if (AcceptancePct>=0.45) delta<-delta+delta*0.1#
		if (AcceptancePct<=0.25) delta<-delta-delta*0.1#
		if (AcceptancePct<=0.15) delta<-delta-delta*0.2#
		if (AcceptancePct<=0.10) delta<-delta-delta*0.3	#
		AcceptanceAlphaCounter<-PilotAdaptationCounter<-0#
#
	}#
	PilotAdaptationCounter<-PilotAdaptationCounter+1#
#
	if (s%in%WhichKeep) {#
		Save <- c(mu,tau2,alpha)#
		RawSamples[which(s==WhichKeep),Save]#
	}			#
#
	##Output Verbose#
	if (Verbose) {#
		cat(paste("Completed Percentage: ",round((s/NTotal)*100,digits=0),"%\n",sep=""))#
		cat(paste("Mu: ",round(mu,digits=3),"\n",sep=""))#
		cat(paste("Tau2: ",round(tau2,digits=3),"\n",sep=""))#
		cat(paste("Alpha: ",round(alpha,digits=3),", Acceptance Rate: ",round(AcceptanceAlphaCounter/PilotAdaptationCounter,digits=2),"\n",sep=""))#
		cat(rep("-",75),"\n",sep="")#
	}#
#
	##Output Progress#
	if (!Verbose) {#
		if (s%in%OutProgress) cat(round(100*(s/NTotal)), "%..  ", sep="")	#
		if (s==1) {#
			cat(rep("-",75),"\n",sep="")#
			cat("0%..  ", sep="")#
		}#
	}#
	##Output MCMC Sampler Run Time#
	if (s==NTotal) {#
		EndTime<-Sys.time()#
		RunTime<-EndTime-BeginTime#
		cat(paste("Run Time: ",round(RunTime,digits=2)," ",attr(RunTime,"unit"),"\n",sep=""))#
		cat(rep("-",75),"\n",sep="")#
	}#
#
###End MCMC Sampler#
}
s
WhichKeep
RawSamples
which(s==WhichKeep)
RawSamples[, which(s==WhichKeep)]
if (s%in%WhichKeep) RawSamples[, which(s==WhichKeep)] <- c(mu,tau2,alpha)
####Lee and Mitchell 2011 Space Only Model #
SpaceOnly<-function(#
	Y=Yreduced, #m x 1 matrix of sensitivity outcome#
	Z=Zreduced, #a x 1 matrix of dissimilarity differences, where a is number of adjacnecy edges #
	W=Wreduced, #m x m adjacnecy matrix#
	Inits=list(mu=2.5,tau2=1,alpha=1/2), #Initial values of parameters #
	Hypers=list(mu=10e4,tau2=c(3,1),alpha=10), #Prior hyperparameters #
	delta=1, #Metropolis tuning parameter#
	rho=0.99, #Fixed propriety parameter#
	Scale=10, #Scale for the outcome#
	NBurn=10000, #Number of burn-in samples #
	NSims=250000, #Number of simulations after burn-in#
	NThin=25, #Interval for which samples are saved after burn-in, must be a factor of NSims#
	Proposal="normal", #"normal" or "uniform"#
	Verbose=FALSE) {#
#
###Set Seed#
set.seed(54)#
#
###Load Libraries#
suppressMessages(library(mvtnorm)) #multivariate normal#
suppressMessages(library(pscl)) #inverse gamma#
suppressMessages(library(msm)) #truncated normal#
suppressMessages(library(coda)) #mcmc#
#
###Format Data#
YScaled<-Y/Scale #scale#
m<-length(YScaled)#
#
###Math Objects#
Onem<-matrix(rep(1,m),nrow=m,ncol=1)#
Eyem<-diag(m)#
#
###Fixed Adjacency Matrix#
Wfixed<-W#
#
###Compute Adjacent Edge Locations Boolean#
AdjacentEdgesBoolean<-obtainBooleanEdges(Wfixed)#
#
###Create Global Objects For W.alpha Function#
assign("Z",Z,envir=globalenv())#
assign("Wfixed",Wfixed,envir=globalenv())#
assign("AdjacentEdgesBoolean", AdjacentEdgesBoolean,envir=globalenv())#
#
###Hyperparameters#
ATau2<-Hypers$tau2[1]#
BTau2<-Hypers$tau2[2]#
SigmaMu2<-Hypers$mu[1]#
BAlpha<-Hypers$alpha#
#
###Initial Values#
#
	##Parameters #
	mu<-Inits$mu#
	tau2<-Inits$tau2#
	alpha<-Inits$alpha#
#
	##Covariance Objects#
	WAlpha<-W.alpha(alpha)#
	DwAlpha<-diag(apply(WAlpha,1,sum))#
	WStar<-DwAlpha-WAlpha#
	Q<-rho*WStar+(1-rho)*Eyem#
	QInverse<-chol2inv(chol(Q))#
#
###Alpha Metropolis Objects#
delta<-delta#
AcceptanceAlphaCounter<-0#
PilotAdaptationCounter<-1#
#
###Tobit Information#
TobitBoolean<-Y==0#
NumberOfZeros<-sum(TobitBoolean)#
LocationOfZeros<-which(TobitBoolean) #
YStarNonZero<-YScaled[!TobitBoolean]#
#
###MCMC Objects#
NSims<-NSims#
NBurn<-NBurn#
NThin<-NThin#
NTotal<-NBurn+NSims#
WhichKeep<-NBurn+(1:(NSims/NThin))*NThin#
NKeep<-length(WhichKeep)#
OutProgress<-(1:10)*NTotal/10#
PilotAdapt<-(1:10)*NBurn/10#
#
###Output Matrix#
RawSamples<-matrix(nrow=3,ncol=NKeep)#
#
###Time MCMC Sampler#
BeginTime<-Sys.time()#
#
###Begin Gibbs Sampler#
for (s in 1:NTotal) {#
	##Tobit Step#
	if (NumberOfZeros==0) YStar<-YScaled#
	if (NumberOfZeros!=0) {#
		#Initiate Tobit Step#
		YStar<-YScaled#
		#Loop over Latent Observations (since they are dependent, can't vectorize)#
		for (i in 1:NumberOfZeros) {#
			#Subset over Tobit Observations#
			ZeroLocationIndex<-LocationOfZeros[i]#
			#Compute Conditional Mean and Variance#
			ConditionalVariance<-tau2/(rho*(WAlpha[ZeroLocationIndex,]%*%Onem)+(1-rho))#
			ConditionalMean<-ConditionalVariance*((rho*(WAlpha[ZeroLocationIndex,]%*%YStar)+(1-rho)*mu)/tau2)#
			#Sample Latent Variable From Full Conditional#
			YStar[ZeroLocationIndex]<-rtnorm(1,ConditionalMean,sqrt(ConditionalVariance),upper=0)#
		}#
	}#
	##Full Conditional For Mu#
	MuVariance<-1/(t(Onem)%*%Q%*%Onem/tau2+1/SigmaMu2)#
	MuMean<-MuVariance*(t(Onem)%*%Q%*%YStar/tau2)#
	mu<-rnorm(1,MuMean,sqrt(MuVariance))#
	##Full Conditional For Tau2#
	ShapeTau2<-ATau2+m/2#
	Residuals<-(YStar-mu*Onem)#
	ScaleTau2<-BTau2+t(Residuals)%*%Q%*%Residuals/2#
	tau2<-rigamma(1,ShapeTau2,ScaleTau2)#
#
	##Reflecting Uniform Proposal#
	if (Proposal=="uniform") {#
	##Metropolis Step For Alpha#
		#Sample Proposal Value Of Alpha#
		AlphaProposal<-runif(1,alpha-delta,alpha+delta)	#
		if (AlphaProposal<=0) AlphaProposal<-abs(AlphaProposal)#
		if (AlphaProposal>BAlpha) AlphaProposal<-(2*BAlpha-AlphaProposal)#
		#Compute W(alpha) and Covariance Proposal#
		WAlphaProposal<-W.alpha(AlphaProposal)#
		DwAlphaProposal<-diag(apply(WAlphaProposal,1,sum))#
		WStarProposal<-DwAlphaProposal-WAlphaProposal#
		QProposal<-rho*WStarProposal+(1-rho)*Eyem#
		#Compute Log Acceptance Ratio#
		LogR<-(1/2)*log(det(QProposal)/det(Q))-(1/(2*tau2))*t(Residuals)%*%(QProposal-Q)%*%Residuals#
		#Metropolis Update#
		if (log(runif(1))<LogR) {#
			#Update Alpha#
			alpha<-AlphaProposal#
			AcceptanceAlphaCounter<-AcceptanceAlphaCounter+1#
			#Update Covariance and W#
			WAlpha<-WAlphaProposal#
			Q<-QProposal#
			QInverse<-chol2inv(chol(Q))#
#
		}#
	}	#
#
	##Normal Transformation Proposal#
	if (Proposal=="normal") {#
	##Metropolis Step For Alpha#
		#Transform Current State#
		Theta<-log(alpha/(BAlpha-alpha))#
		#Sample Transformed Proposal#
		ThetaProposal<-rnorm(1,Theta,delta)	#
		#Compute Alpha Proposal#
		AlphaProposal<-(BAlpha*exp(ThetaProposal))/(1+exp(ThetaProposal))#
		#Compute W(alpha) and Covariance Proposal#
		WAlphaProposal<-W.alpha(AlphaProposal)#
		DwAlphaProposal<-diag(apply(WAlphaProposal,1,sum))#
		WStarProposal<-DwAlphaProposal-WAlphaProposal#
		QProposal<-rho*WStarProposal+(1-rho)*Eyem#
		#Compute Log Acceptance Ratio#
		LogR<-(1/2)*log(det(QProposal)/det(Q))-(1/(2*tau2))*t(Residuals)%*%(QProposal-Q)%*%Residuals+(ThetaProposal-Theta)+2*log((1+exp(Theta))/(1+exp(ThetaProposal)))	#
		#Metropolis Update#
		if (log(runif(1))<LogR) {#
			#Update Alpha#
			alpha<-AlphaProposal#
			AcceptanceAlphaCounter<-AcceptanceAlphaCounter+1#
			#Update Covariance and W#
			WAlpha<-WAlphaProposal#
			Q<-QProposal#
			QInverse<-chol2inv(chol(Q))#
#
		}#
	}	#
#
	##Manage Acceptance Rates (Pilot Adaptation)#
	if (s %in% PilotAdapt) {#
#
		###Metropolis Tuning Parameter#
		AcceptancePct<-round(AcceptanceAlphaCounter/PilotAdaptationCounter,digits=2)#
		if (AcceptancePct>=0.90) delta<-delta+delta*0.3#
		if (AcceptancePct>=0.75) delta<-delta+delta*0.2#
		if (AcceptancePct>=0.45) delta<-delta+delta*0.1#
		if (AcceptancePct<=0.25) delta<-delta-delta*0.1#
		if (AcceptancePct<=0.15) delta<-delta-delta*0.2#
		if (AcceptancePct<=0.10) delta<-delta-delta*0.3	#
		AcceptanceAlphaCounter<-PilotAdaptationCounter<-0#
#
	}#
	PilotAdaptationCounter<-PilotAdaptationCounter+1#
#
	###Store Samples#
	if (s%in%WhichKeep) RawSamples[, which(s==WhichKeep)] <- c(mu,tau2,alpha)		#
#
	##Output Verbose#
	if (Verbose) {#
		cat(paste("Completed Percentage: ",round((s/NTotal)*100,digits=0),"%\n",sep=""))#
		cat(paste("Mu: ",round(mu,digits=3),"\n",sep=""))#
		cat(paste("Tau2: ",round(tau2,digits=3),"\n",sep=""))#
		cat(paste("Alpha: ",round(alpha,digits=3),", Acceptance Rate: ",round(AcceptanceAlphaCounter/PilotAdaptationCounter,digits=2),"\n",sep=""))#
		cat(rep("-",75),"\n",sep="")#
	}#
#
	##Output Progress#
	if (!Verbose) {#
		if (s%in%OutProgress) cat(round(100*(s/NTotal)), "%..  ", sep="")	#
		if (s==1) {#
			cat(rep("-",75),"\n",sep="")#
			cat("0%..  ", sep="")#
		}#
	}#
	##Output MCMC Sampler Run Time#
	if (s==NTotal) {#
		EndTime<-Sys.time()#
		RunTime<-EndTime-BeginTime#
		cat(paste("Run Time: ",round(RunTime,digits=2)," ",attr(RunTime,"unit"),"\n",sep=""))#
		cat(rep("-",75),"\n",sep="")#
	}#
#
###End MCMC Sampler#
}#
#
###Read in Posterior MCMC Samples#
Mu<-RawSamples[1,]#
Alpha<-RawSamples[3,]#
Tau2<-RawSamples[2,]#
#
###Create MCMC Objects#
MuMCMC<-as.mcmc(Mu)#
AlphaMCMC<-as.mcmc(Alpha)#
Tau2MCMC<-as.mcmc(Tau2)#
#
###Summarize Metropolis Output#
Metropolis<-list(round(AcceptanceAlphaCounter/PilotAdaptationCounter,digits=2),delta)#
names(Metropolis)<-c("accpetance","delta")#
#
###Summarize Output and Return#
FinalObject<-list(MuMCMC,Tau2MCMC,AlphaMCMC,Metropolis)#
names(FinalObject)<-c("mu","tau2","alpha","metropolis")#
return(FinalObject)#
#
####End Space Only Function#
}
###Start with a clean workspace#
rm(list = ls())#
#
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
###Source scripts used to fit space only model to create SpaceCV metric#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/adjacency.R")) # Original Adjacency Matrix#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/opticnervedegree.R")) # Optic Nerve Degree (object called degree)#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/ComputeDissimilarityMetric.R")) # Dissimilarity Metric Functions#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/W.alpha.R")) # W(alpha) Function#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/SpaceOnly2.R")) # Lee and Mitchell 2011 Space Model#
###Define blind spot#
blind_spot <- c(26, 35)#
###Create original adjacency matrix (same as HFAII_Queen in womblR package)#
Woriginal <- adj.mat(Queen)#
Wreduced <- Woriginal[-blind_spot, -blind_spot]#
###Load dissimilarity metric#
scale_degree <- 100#
Zoriginal <- obtainDissimilarityMetric(degree / scale_degree, Woriginal)#
Zreduced <- obtainDissimilarityMetric(degree[-blind_spot] / scale_degree, Wreduced)#
###Get phi upper bound#
b_alpha <- log(0.50) / -min(Zreduced[Zreduced>0]) #original definition used by Lee and Mitchell#
###Load visual field data for example patient#
load(paste0(berchuck.dir, "/berchuck_code/Data/VFSeries.RData"))#
VFSeries <- VFSeries[order(VFSeries$Location), ] # sort by location#
VFSeries <- VFSeries[order(VFSeries$Visit), ] # sort by visit#
VFSeries <- VFSeries[!VFSeries$Location %in% blind_spot, ] # remove blind spot locations#
###Set Data Objects#
Nu <- length(unique(VFSeries$Visit))#
M <- length(unique(VFSeries$Location))#
Ymat <- matrix(VFSeries$DLS, nrow = M, ncol = Nu)#
###MCMC Sampler Inputs#
NSims <- 250000#
NBurn <- 10000#
NThin <- 25#
###Create MCMC output folder#
# system(paste0("mkdir ", berchuck.dir, "/berchuck_code/SpaceOnlyModel/RawMCMC"))#
#
###Must Loop over visits (since this is a space only model)#
Reg<-list()#
for (i in 1:Nu) {#
#
	###Set Data at Visit i#
	Yreduced <- matrix(Ymat[,i], nrow=M, ncol=1)#
#
	###Fit OPOV (i.e. Duncan Lee Model)#
	# Reg[[i]]<-SpaceOnly(Y = Yreduced, Z = Zreduced, W = Wreduced, Inits = list(mu = 2.5, tau2 = 1, alpha = 1/2),#
						# Hypers = list(mu = 10e4, tau2 = c(0.001, 0.001), alpha = b_alpha),#
						# delta = 1, NSims = NSims, NBurn = NBurn, NThin = NThin,#
						# Output = paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/RawMCMC/"))#
	Reg[[i]]<-SpaceOnly(Y = Yreduced, Z = Zreduced, W = Wreduced, Inits = list(mu = 2.5, tau2 = 1, alpha = 1/2),#
						Hypers = list(mu = 10e4, tau2 = c(0.001, 0.001), alpha = b_alpha),#
						delta = 1, NSims = NSims, NBurn = NBurn, NThin = NThin)#
	cat(paste0("Model: ", i," of ", Nu,"\n"))#
#
}
###Coefficient of variation function#
cv <- function(x) sd(x) / mean(x)#
#
###Obtain the posterior distribution of the CV of alpha#
Alpha <- NULL#
for (i in 1:Nu) Alpha<-cbind(Alpha, Reg[[i]]$alpha)#
CVAlpha<-apply(Alpha, 1, cv)#
#
###Calculate SpaceCV#
SpaceCV <- mean(CVAlpha)
SpaceCV
save(SpaceCV, file = paste0(berchuck.dir, "/berchuck_code/DataApplication/Output/SpaceCV.RData"))
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
#
###Load Metrics object#
load(paste0(berchuck.dir, "/berchuck_code/Data/Metrics.RData"))
load(paste0(berchuck.dir, "/berchuck_code/DataApplication/Output/SpaceCV.RData"))
all.equal(metrics$SpaceCV, SpaceCV)
load(paste0(berchuck.dir, "/berchuck_code/Data/Metrics.RData"))
all.equal(metrics$SpaceCV, SpaceCV)
###Start with a clean workspace#
rm(list = ls())#
#
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
#
###Load womblR package#
library(womblR)#
#
###############################################
### Load Metrics object#
###############################################
#
load(paste0(berchuck.dir, "/berchuck_code/Data/Metrics.RData"))#
head(Metrics)#
#
###############################################
### Reproduce Table 1.#
###############################################
#
###Regress each metric against progression#
regMeanCV <- glm(Prog ~ MeanCV, family = "binomial", data = Metrics)#
regSpaceCV <- glm(Prog ~ SpaceCV, family = "binomial", data = Metrics)#
regSTCV <- glm(Prog ~ STCV, family = "binomial", data = Metrics)#
#
###Load xtable library#
library(xtable)#
#
###Output Table 1 material#
xtable(regMeanCV, digits = c(0, 2, 3, 2, 3))#
xtable(regSpaceCV, digits = c(0, 2, 3, 2, 3))#
xtable(regSTCV, digits = c(0, 2, 3, 2, 3))#
#
###############################################
### Reproduce Figure 2.#
###############################################
#
###Load example patient's VF data (This is not the same patient used in the paper, but this code is provides the a repoducible example for created the Figure given VF data.)#
load(paste0(berchuck.dir, "/berchuck_code/Data/VFSeries.RData")) # object VFSeries#
#
###We reproduce Figure 2 using the PlotVFTimeSeries function from the womblR package#
pdf(paste0(berchuck.dir, "/berchuck_code/DataApplication/Results/Figure2.pdf"), height = 6, width = 6)#
PlotVfTimeSeries(Y = VFSeries$DLS,#
                 Location = VFSeries$Location,#
                 Time = VFSeries$Time,#
                 main = "Visual field sensitivity time series \n at each location",#
                 xlab = "Days from baseline visit",#
                 ylab = "Differential light sensitivity (dB)")#
dev.off()#
#
###############################################
### Reproduce Figure 3.#
###############################################
#
###Glaucoma objects from womblR package#
blind_spot <- c(26, 35)#
W <- HFAII_Queen[ -blind_spot, -blind_spot] # visual field adjacency matrix#
DM <- GarwayHeath[-blind_spot] # Garway-Heath angles#
#
###Create figure using PlotAdjacency function from womblR#
pdf(paste0(berchuck.dir, "/berchuck_code/DataApplication/Results/Figure3.pdf"), height = 5.5, width = 5.5)#
PlotAdjacency(W = W, DM = DM, zlim = c(0, 180), Visit = NA, main = "Garway-Heath dissimilarity metric\n across the visual field")#
dev.off()#
#
###############################################
### Reproduce Figure 4.#
###############################################
#
###Set Metrics#
MeanCV <- Metrics$MeanCV#
SpaceCV <- Metrics$SpaceCV#
STCV <- Metrics$STCV#
#
###Pearson correlation tests#
cor.test(MeanCV, SpaceCV)#
cor.test(MeanCV, STCV)#
cor.test(SpaceCV, STCV)#
#
###Create figure#
pdf(paste0(berchuck.dir, "/berchuck_code/DataApplication/Results/Figure4.pdf"), height = 3, width = 8.5)#
par(mfcol = c(1, 3))#
#
##MeanCV vs SpaceCV#
plot(1,1,type = "n", xlab = "Space CV", ylab = "Mean CV", main = "", xlim = c(0.1, 1.24), ylim = c(0, 0.205)) # initialize plot#
points(SpaceCV, MeanCV) #add points#
abline(lm(MeanCV ~ SpaceCV), col = "black", lty = 2) # add regression line#
text(1.13, 0.203, expression(paste(rho~"= 0.026", sep = ""))) # add pearson correlation#
text(1.16, 0.190, "(0.718)") # add pearson correlation p-value#
#
##MeanCV vs STCV#
plot(1,1,type = "n", xlab = "ST CV", ylab = "Mean CV", main = "", xlim = c(0.1, 1.172), ylim = c(0, 0.198))#
points(STCV, MeanCV)#
abline(lm(MeanCV ~ STCV), col = "black", lty = 2)#
text(1.07, 0.197, expression(paste(rho~"= 0.062", sep = "")))#
text(1.09, 0.185, "(0.389)")#
#
##SpaceCV vs STCV#
plot(1,1,type = "n", xlab = "Space CV", ylab = "ST CV", main = "", xlim = c(0.1, 1.209), ylim = c(0.1, 1.166))#
points(SpaceCV, STCV)#
abline(lm(STCV ~ SpaceCV), col = "black", lty = 2)#
text(1.105, 1.155, expression(paste(rho~"= 0.477", sep = "")))#
text(1.12, 1.09, "(<0.001)")#
#
dev.off()
load(paste0(berchuck.dir, "/berchuck_code/Data/Metrics.RData"))
metrics$SpaceCV
ls()
###Start with a clean workspace#
rm(list = ls())
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
#
###Load Metrics object#
load(paste0(berchuck.dir, "/berchuck_code/Data/Metrics.RData"))
ls()
all.equal(Metrics$SpaceCV, SpaceCV)
load(paste0(berchuck.dir, "/berchuck_code/DataApplication/Output/SpaceCV.RData"))
all.equal(Metrics$SpaceCV, SpaceCV)
SpaceCV
Metrics$SpaceCV
metrics <- Metrics[1, ] # our example patient is in row 1
metrics
all.equal(metrics$SpaceCV, SpaceCV)
###Start with a clean workspace#
rm(list = ls())#
#
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
###Source scripts used to fit space only model to create SpaceCV metric#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/adjacency.R")) # Original Adjacency Matrix#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/opticnervedegree.R")) # Optic Nerve Degree (object called degree)#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/ComputeDissimilarityMetric.R")) # Dissimilarity Metric Functions#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/W.alpha.R")) # W(alpha) Function#
source(paste0(berchuck.dir, "/berchuck_code/SpaceOnlyModel/SpaceOnly.R")) # Lee and Mitchell 2011 Space Model#
###Define blind spot#
blind_spot <- c(26, 35)#
###Create original adjacency matrix (same as HFAII_Queen in womblR package)#
Woriginal <- adj.mat(Queen)#
Wreduced <- Woriginal[-blind_spot, -blind_spot]#
###Load dissimilarity metric#
scale_degree <- 100#
Zoriginal <- obtainDissimilarityMetric(degree / scale_degree, Woriginal)#
Zreduced <- obtainDissimilarityMetric(degree[-blind_spot] / scale_degree, Wreduced)#
###Get phi upper bound#
b_alpha <- log(0.50) / -min(Zreduced[Zreduced>0]) #original definition used by Lee and Mitchell#
###Load visual field data for example patient#
load(paste0(berchuck.dir, "/berchuck_code/Data/VFSeries.RData"))#
VFSeries <- VFSeries[order(VFSeries$Location), ] # sort by location#
VFSeries <- VFSeries[order(VFSeries$Visit), ] # sort by visit#
VFSeries <- VFSeries[!VFSeries$Location %in% blind_spot, ] # remove blind spot locations#
###Set Data Objects#
Nu <- length(unique(VFSeries$Visit))#
M <- length(unique(VFSeries$Location))#
Ymat <- matrix(VFSeries$DLS, nrow = M, ncol = Nu)#
###MCMC Sampler Inputs#
NSims <- 250000#
NBurn <- 10000#
NThin <- 25#
#
###Must Loop over visits (since this is a space only model)#
Reg<-list()#
for (i in 1:Nu) {#
#
	###Set Data at Visit i#
	Yreduced <- matrix(Ymat[,i], nrow=M, ncol=1)#
#
	###Fit OPOV (i.e. Duncan Lee Model)#
	Reg[[i]]<-SpaceOnly(Y = Yreduced, Z = Zreduced, W = Wreduced, Inits = list(mu = 2.5, tau2 = 1, alpha = 1/2),#
						Hypers = list(mu = 10e4, tau2 = c(0.001, 0.001), alpha = b_alpha),#
						delta = 1, NSims = NSims, NBurn = NBurn, NThin = NThin)#
	cat(paste0("Model: ", i," of ", Nu,"\n"))#
#
}#
#
###Coefficient of variation function#
cv <- function(x) sd(x) / mean(x)#
#
###Obtain the posterior distribution of the CV of alpha#
Alpha <- NULL#
for (i in 1:Nu) Alpha<-cbind(Alpha, Reg[[i]]$alpha)#
CVAlpha<-apply(Alpha, 1, cv)#
#
###Calculate SpaceCV#
SpaceCV <- mean(CVAlpha)#
#
###Save SpaceCV#
save(SpaceCV, file = paste0(berchuck.dir, "/berchuck_code/DataApplication/Output/SpaceCV.RData"))
###Start with a clean workspace#
rm(list = ls())#
#
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
#
###Load womblR package#
library(womblR)#
#
###Define blind side#
blind_spot <- c(26, 35)#
#
###Format data for STBDwDM MCMC sampler (uses VFSeries from womblR package, which is the same series of VFs used here.)#
VFSeries <- VFSeries[order(VFSeries$Visit), ] #sort by visit#
VFSeries <- VFSeries[!VFSeries$Location %in% blind_spot, ] #remove blind spot locations#
Y <- VFSeries$DLS #assig observed outcome data#
Time <- unique(VFSeries$Time) / 365 #time since first visit#
Nu <- length(Time)#
#
###Create original adjacency matrix#
W <- HFAII_Queen[ -blind_spot, -blind_spot] #Visual field adjacency matrix#
#
###Load Garway-Heath angles for dissimiliarity metric#
DM <- GarwayHeath[-blind_spot] #Uses Garway-Heath angles object "GarwayHeath"#
#
###Compute bounds for phi prior#
TimeDist <- abs(outer(Time, Time, "-"))#
TimeDistVec <- TimeDist[lower.tri(TimeDist)]#
minDiff <- min(TimeDistVec)#
maxDiff <- max(TimeDistVec)#
PhiUpper <- -log(0.01) / minDiff #shortest diff goes down to 1%#
PhiLower <- -log(0.95) / maxDiff #longest diff goes up to 95%#
#
###Initial values#
Starting <- list(Delta = c(3, 0, 0), T = diag(3), Phi = 0.5)#
#
###Hyperparameters#
Hypers <- list(Delta = list(MuDelta = c(3, 0, 0), OmegaDelta = diag(c(1000, 1000, 1))),#
               T = list(Xi = 4, Psi = diag(3)),#
               Phi = list(APhi = PhiLower, BPhi = PhiUpper))#
#
###Metropolis tuners#
Tuning <- list(Theta2 = rep(1, Nu), Theta3 = rep(1, Nu), Phi = 1)#
#
###MCMC inputs#
MCMC <- list(NBurn = 10000, NSims = 250000, NThin = 25, NPilot = 20)#
#
###Fit spatiotemporal boundary dection with dissimilarity metric (Seed = 54 by default)#
reg.STBDwDM <- STBDwDM(Y = Y, DM = DM, W = W, Time = Time, Starting = Starting, Hypers = Hypers, Tuning = Tuning, MCMC = MCMC)#
#
###Coefficient of variation function#
cv <- function(x) sd(x) / mean(x)#
#
###Obtain the posterior distribution of the CV of alpha#
Alpha <- reg.STBDwDM$alpha#
CVAlpha <- apply(Alpha, 1, cv)#
#
###Calculate ST CV#
STCV <- mean(CVAlpha)#
#
###Save ST CV#
save(STCV, file = paste0(berchuck.dir, "/berchuck_code/DataApplication/Output/STCV.RData"))
###Start with a clean workspace#
rm(list = ls())#
#
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
#
###Load womblR package#
library(womblR)#
#
###############################################
### Load Metrics object#
###############################################
#
load(paste0(berchuck.dir, "/berchuck_code/Data/Metrics.RData"))#
head(Metrics)
library(devtools)
install_github("berchuck/womblR")
install_github("berchuck/womblR", force = TRUE)
library(womblR)
head(VFSeries)
rm(list=ls())#
#
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
#
###Load womblR package#
library(devtools)#
install_github("berchuck/womblR")#
library(womblR)#
#
###Load other libraries#
library(Matrix) #load for bdiag function#
library(mvtnorm) #load for rmvnorm function
####################################################################
### Set data objects required for simulation.#
####################################################################
#
###Exponential temporal correlation structure (wraps womblR internal SIGMA function)#
Sigma <- function(phi, day) {#
	temp <- day / 365#
	womblR:::SIGMA(phi, 0, abs(outer(temp, temp, "-")), length(day))#
}#
#
###Glaucoma objects from womblR package#
blind_spot <- c(26, 35) # define blind spot#
W <- HFAII_Queen[ -blind_spot, -blind_spot] # visual field adjacency matrix (from womblR package)#
DM <- GarwayHeath[-blind_spot] # Garway-Heath angles (from womblR package)#
#
###Set data objects#
Rho <- 0.99#
M  <- 52#
EyeM <- diag(M)#
ScaleDM <- 100#
ScaleY <- 10#
WeightsInd <- 0 # continuous#
#
###Obtain the unique locations of adjacencies, AdjacentEdgesBoolean#
AdjacentEdgesBoolean <- (W == 1) & (!lower.tri(W))#
# AdjacentEdgesBoolean <- matrix(which(AdjacentEdgesBoolean) - 1, ncol = 1) # indicates the index of the adjacencies in the adjacency matrix#
#
###Obtain the scaled dissimilarity metric object, Z#
Dist <- function(x, y) pmin(abs(x - y), (360 - pmax(x, y) + pmin(x, y))) #arc length of optic nerve#
DM_Grid <- expand.grid(DM, DM)#
Z_Vector <- Dist(DM_Grid[ , 1], DM_Grid[ , 2])#
Z_Matrix <- matrix(Z_Vector, nrow = dim(W)[1], ncol = dim(W)[1], byrow = TRUE)#
Z <- matrix(Z_Matrix[AdjacentEdgesBoolean] / ScaleDM, ncol = 1) # vector of the scaled dissimilarity metric#
#
####################################################################
### Specify the simulation settings.#
####################################################################
#
###Delta: fixed at a posterior mean of the example patient#
delta <- matrix(c(2.44634552191589982328424, 0.07015085810136499622214, 0.97418292944508000363868), nrow = 3, ncol = 1)#
#
###T: fixed at a posterior mean of the example patient#
TFull <- matrix(0, nrow = 3, ncol = 3)#
TFull[lower.tri(TFull, diag=TRUE)] <- c(0.820415457249000001915817,  0.004111988364394999866114, -0.02815687181061940144722,  0.380455175360999986455823, -0.19165688755208701299360,  0.84042396003099995027696)#
TFull[upper.tri(TFull)] <- TFull[lower.tri(TFull)]#
TDiag <- TFull # diagnoal matrix with no cross-covariance#
TDiag[upper.tri(TDiag)] <- TDiag[lower.tri(TDiag)] <- 0#
#
###Phi: fixed at a posterior mean of the example patient#
phi <- 0.1633080144659999899392 # small value (i.e. temporal correlation)#
phiI <- 100 # very large (i.e. no temporal correlation --> (I)ndependence)#
#
###Number of visits#
NuVec <- c(7, 21) #median and maximum setting#
#
###Number of simulations#
NThetas <- 100 # number of Theta vectors sampled#
NDatas <- 10 # number of datasets sampled/Theta vector#
NTotal <- NThetas * NDatas#
#
###Begin Simulation#
set.seed(54)#
#
###Days of the VF vists (first 9 days are fixed at those from the average patient we used to set delta, T and phi, then we randomly sampled to get the remaining visit days out to 21 visit days)#
day <- c(0, 126, 238, 406, 504, 588, 756, 868, 938) # vistis of the example patient#
WaitingTime <- rpois(NuVec[2] - length(day), mean(diff(day))) # sample the remaining visits#
day <- c(day, cumsum(WaitingTime) + day[9])#
#
###Assemble simulation Settings:#
Settings <- list(#
list(phi = phiI, T = TDiag, Nu = NuVec[[1]]),#
list(phi = phiI, T = TFull, Nu = NuVec[[1]]),#
list(phi = phi, T = TDiag, Nu = NuVec[[1]]),#
list(phi = phi, T = TFull, Nu = NuVec[[1]]),#
list(phi = phiI, T = TDiag, Nu = NuVec[[2]]),#
list(phi = phiI, T = TFull, Nu = NuVec[[2]]),#
list(phi = phi, T = TDiag, Nu = NuVec[[2]]),#
list(phi = phi, T = TFull, Nu = NuVec[[2]]))#
NSettings <- length(Settings)#
#
###Create Object For Simulation Summary#
SimSummary <- matrix(nrow = NTotal * NSettings, ncol = 9)#
SimSummary[, 1] <- c(rep(paste(LETTERS[1:4], "Median", sep = "_"), each = NTotal), rep(paste(LETTERS[1:4], "Maximum", sep = "_"), each = NTotal))#
colnames(SimSummary) <- c("Setting", "TrueCV", "MeanCV", "VarCV", "2.5%CV", "97.5%CV", "Bias", "MSE", "EC")
head(SimSummary)
unique(SimSummary[, 1])
SettingNames <- unique(SimSummary[, 1])
SettingNames
####################################################################
### Set data objects required for simulation.#
####################################################################
#
###Exponential temporal correlation structure (wraps womblR internal SIGMA function)#
Sigma <- function(phi, day) {#
	temp <- day / 365#
	womblR:::SIGMA(phi, 0, abs(outer(temp, temp, "-")), length(day))#
}#
#
###Glaucoma objects from womblR package#
blind_spot <- c(26, 35) # define blind spot#
W <- HFAII_Queen[ -blind_spot, -blind_spot] # visual field adjacency matrix (from womblR package)#
DM <- GarwayHeath[-blind_spot] # Garway-Heath angles (from womblR package)#
#
###Set data objects#
Rho <- 0.99#
M  <- 52#
EyeM <- diag(M)#
ScaleDM <- 100#
ScaleY <- 10#
WeightsInd <- 0 # continuous#
#
###Obtain the unique locations of adjacencies, AdjacentEdgesBoolean#
AdjacentEdgesBoolean <- (W == 1) & (!lower.tri(W))#
# AdjacentEdgesBoolean <- matrix(which(AdjacentEdgesBoolean) - 1, ncol = 1) # indicates the index of the adjacencies in the adjacency matrix#
#
###Obtain the scaled dissimilarity metric object, Z#
Dist <- function(x, y) pmin(abs(x - y), (360 - pmax(x, y) + pmin(x, y))) #arc length of optic nerve#
DM_Grid <- expand.grid(DM, DM)#
Z_Vector <- Dist(DM_Grid[ , 1], DM_Grid[ , 2])#
Z_Matrix <- matrix(Z_Vector, nrow = dim(W)[1], ncol = dim(W)[1], byrow = TRUE)#
Z <- matrix(Z_Matrix[AdjacentEdgesBoolean] / ScaleDM, ncol = 1) # vector of the scaled dissimilarity metric#
#
####################################################################
### Specify the simulation settings.#
####################################################################
#
###Delta: fixed at a posterior mean of the example patient#
delta <- matrix(c(2.44634552191589982328424, 0.07015085810136499622214, 0.97418292944508000363868), nrow = 3, ncol = 1)#
#
###T: fixed at a posterior mean of the example patient#
TFull <- matrix(0, nrow = 3, ncol = 3)#
TFull[lower.tri(TFull, diag=TRUE)] <- c(0.820415457249000001915817,  0.004111988364394999866114, -0.02815687181061940144722,  0.380455175360999986455823, -0.19165688755208701299360,  0.84042396003099995027696)#
TFull[upper.tri(TFull)] <- TFull[lower.tri(TFull)]#
TDiag <- TFull # diagnoal matrix with no cross-covariance#
TDiag[upper.tri(TDiag)] <- TDiag[lower.tri(TDiag)] <- 0#
#
###Phi: fixed at a posterior mean of the example patient#
phi <- 0.1633080144659999899392 # small value (i.e. temporal correlation)#
phiI <- 100 # very large (i.e. no temporal correlation --> (I)ndependence)#
#
###Number of visits#
NuVec <- c(7, 21) #median and maximum setting#
#
###Number of simulations#
NThetas <- 100 # number of Theta vectors sampled#
NDatas <- 10 # number of datasets sampled/Theta vector#
NTotal <- NThetas * NDatas#
#
###Begin Simulation#
set.seed(54)#
#
###Days of the VF vists (first 9 days are fixed at those from the average patient we used to set delta, T and phi, then we randomly sampled to get the remaining visit days out to 21 visit days)#
day <- c(0, 126, 238, 406, 504, 588, 756, 868, 938) # vistis of the example patient#
WaitingTime <- rpois(NuVec[2] - length(day), mean(diff(day))) # sample the remaining visits#
day <- c(day, cumsum(WaitingTime) + day[9])#
#
###Assemble simulation Settings:#
Settings <- list(#
list(phi = phiI, T = TDiag, Nu = NuVec[[1]]),#
list(phi = phiI, T = TFull, Nu = NuVec[[1]]),#
list(phi = phi, T = TDiag, Nu = NuVec[[1]]),#
list(phi = phi, T = TFull, Nu = NuVec[[1]]),#
list(phi = phiI, T = TDiag, Nu = NuVec[[2]]),#
list(phi = phiI, T = TFull, Nu = NuVec[[2]]),#
list(phi = phi, T = TDiag, Nu = NuVec[[2]]),#
list(phi = phi, T = TFull, Nu = NuVec[[2]]))#
NSettings <- length(Settings)#
#
###Create Object For Simulation Summary#
SimSummary <- matrix(nrow = NTotal * NSettings, ncol = 9)#
SimSummary[, 1] <- c(rep(paste(LETTERS[1:4], "Median", sep = "_"), each = NTotal), rep(paste(LETTERS[1:4], "Maximum", sep = "_"), each = NTotal))#
colnames(SimSummary) <- c("Setting", "TrueCV", "MeanCV", "VarCV", "2.5%CV", "97.5%CV", "Bias", "MSE", "EC")#
head(SimSummary)#
SettingNames <- unique(SimSummary[, 1])
i<-1
###Set simulation settings#
	settings <- Settings[[i]]#
	settingName <- SettingNames[i]#
	Nu <- settings$Nu#
	Day <- day[1:Nu]#
	T <- settings$T#
	phi <- settings$phi
settingName
settingName
rm(list=ls())#
#
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
#
###Load womblR package#
library(devtools)#
install_github("berchuck/womblR")#
library(womblR)#
#
###Load other libraries#
library(Matrix) #load for bdiag function#
library(mvtnorm) #load for rmvnorm function#
#
####################################################################
### Set data objects required for simulation.#
####################################################################
#
###Exponential temporal correlation structure (wraps womblR internal SIGMA function)#
Sigma <- function(phi, day) {#
	temp <- day / 365#
	womblR:::SIGMA(phi, 0, abs(outer(temp, temp, "-")), length(day))#
}#
#
###Glaucoma objects from womblR package#
blind_spot <- c(26, 35) # define blind spot#
W <- HFAII_Queen[ -blind_spot, -blind_spot] # visual field adjacency matrix (from womblR package)#
DM <- GarwayHeath[-blind_spot] # Garway-Heath angles (from womblR package)#
#
###Set data objects#
Rho <- 0.99#
M  <- 52#
EyeM <- diag(M)#
ScaleDM <- 100#
ScaleY <- 10#
WeightsInd <- 0 # continuous#
#
###Obtain the unique locations of adjacencies, AdjacentEdgesBoolean#
AdjacentEdgesBoolean <- (W == 1) & (!lower.tri(W))#
# AdjacentEdgesBoolean <- matrix(which(AdjacentEdgesBoolean) - 1, ncol = 1) # indicates the index of the adjacencies in the adjacency matrix#
#
###Obtain the scaled dissimilarity metric object, Z#
Dist <- function(x, y) pmin(abs(x - y), (360 - pmax(x, y) + pmin(x, y))) #arc length of optic nerve#
DM_Grid <- expand.grid(DM, DM)#
Z_Vector <- Dist(DM_Grid[ , 1], DM_Grid[ , 2])#
Z_Matrix <- matrix(Z_Vector, nrow = dim(W)[1], ncol = dim(W)[1], byrow = TRUE)#
Z <- matrix(Z_Matrix[AdjacentEdgesBoolean] / ScaleDM, ncol = 1) # vector of the scaled dissimilarity metric#
#
####################################################################
### Specify the simulation settings.#
####################################################################
#
###Delta: fixed at a posterior mean of the example patient#
delta <- matrix(c(2.44634552191589982328424, 0.07015085810136499622214, 0.97418292944508000363868), nrow = 3, ncol = 1)#
#
###T: fixed at a posterior mean of the example patient#
TFull <- matrix(0, nrow = 3, ncol = 3)#
TFull[lower.tri(TFull, diag=TRUE)] <- c(0.820415457249000001915817,  0.004111988364394999866114, -0.02815687181061940144722,  0.380455175360999986455823, -0.19165688755208701299360,  0.84042396003099995027696)#
TFull[upper.tri(TFull)] <- TFull[lower.tri(TFull)]#
TDiag <- TFull # diagnoal matrix with no cross-covariance#
TDiag[upper.tri(TDiag)] <- TDiag[lower.tri(TDiag)] <- 0#
#
###Phi: fixed at a posterior mean of the example patient#
phi <- 0.1633080144659999899392 # small value (i.e. temporal correlation)#
phiI <- 100 # very large (i.e. no temporal correlation --> (I)ndependence)#
#
###Number of visits#
NuVec <- c(7, 21) #median and maximum setting#
#
###Number of simulations#
NThetas <- 100 # number of Theta vectors sampled#
NDatas <- 10 # number of datasets sampled/Theta vector#
NTotal <- NThetas * NDatas#
#
###Begin Simulation#
set.seed(54)#
#
###Days of the VF vists (first 9 days are fixed at those from the average patient we used to set delta, T and phi, then we randomly sampled to get the remaining visit days out to 21 visit days)#
day <- c(0, 126, 238, 406, 504, 588, 756, 868, 938) # vistis of the example patient#
WaitingTime <- rpois(NuVec[2] - length(day), mean(diff(day))) # sample the remaining visits#
day <- c(day, cumsum(WaitingTime) + day[9])#
#
###Assemble simulation Settings:#
Settings <- list(#
list(phi = phiI, T = TDiag, Nu = NuVec[[1]]),#
list(phi = phiI, T = TFull, Nu = NuVec[[1]]),#
list(phi = phi, T = TDiag, Nu = NuVec[[1]]),#
list(phi = phi, T = TFull, Nu = NuVec[[1]]),#
list(phi = phiI, T = TDiag, Nu = NuVec[[2]]),#
list(phi = phiI, T = TFull, Nu = NuVec[[2]]),#
list(phi = phi, T = TDiag, Nu = NuVec[[2]]),#
list(phi = phi, T = TFull, Nu = NuVec[[2]]))#
NSettings <- length(Settings)#
#
###Create Object For Simulation Summary#
SimSummary <- matrix(nrow = NTotal * NSettings, ncol = 9)#
SimSummary[, 1] <- c(rep(paste(LETTERS[1:4], "Median", sep = "_"), each = NTotal), rep(paste(LETTERS[1:4], "Maximum", sep = "_"), each = NTotal))#
colnames(SimSummary) <- c("Setting", "TrueCV", "MeanCV", "VarCV", "2.5%CV", "97.5%CV", "Bias", "MSE", "EC")#
SettingNames <- unique(SimSummary[, 1])#
#
####################################################################
### Generate data for simulation based on defined settings.#
####################################################################
#
###Begin simulation#
for (i in 1:NSettings) {#
#
  	###Set simulation settings#
	settings <- Settings[[i]]#
	settingName <- SettingNames[i]#
	Nu <- settings$Nu#
	Day <- day[1:Nu]#
	T <- settings$T#
	phi <- settings$phi#
#
	###Sample thetas and assign level 1 parameters#
	Thetas <- mvtnorm::rmvnorm(NThetas, kronecker(delta, rep(1, Nu)), kronecker(T, Sigma(phi, Day)))#
#
	Mu <- Thetas[, 1:Nu]#
	Tau2 <- exp(Thetas[, (Nu + 1):(2 * Nu)]) ^ 2#
	Alpha <- exp(Thetas[, (2 * Nu + 1):(3 * Nu)])#
#
	###Save True CV#
	SimSummary[(1:NTotal) + (i - 1) * NTotal, 2] <- rep(apply(Alpha, 1, f <- function(x) sd(x) / mean(x)), each = NDatas)#
#
	###Simulate data#
	Yout <- matrix(nrow = Nu * M, ncol = NTotal)#
	for (j in 1:NThetas) {#
#
	  ##Set parameters#
		mu <- Mu[j, ]#
		tau2 <- Tau2[j, ]#
		alpha <- Alpha[j, ]#
		WAlphas <- womblR:::WAlphaCube(alpha, Z, W, M, Nu, WeightsInd)#
		JointCovs <- womblR:::JointCovarianceCube(WAlphas, tau2, EyeM, Rho, M, Nu)#
		JointCovsList <- list()#
		for (k in 1:Nu) JointCovsList[[k]] <- JointCovs[ , , k]#
#
		#Joint moments#
		Q <- matrix(Matrix::bdiag(JointCovsList), nrow = Nu * M)#
		JointMean <- kronecker(mu, rep(1, M))#
#
		#Simulate data#
		YStar <- mvtnorm::rmvnorm(NDatas, JointMean, Q)#
		Yout[, (1:NDatas) + (j - 1) * NDatas] <- apply(YStar, 1, f <- function(x) pmax(0, x * ScaleY))#
#
		###Output iteration#
		cat(paste0("Setting: ", settingName, "; Simulation: ", j * NDatas, "\n"))#
#
	#End sims over thetas#
	}#
#
	###Save simulated data for each setting#
	save(Yout, file = paste0(berchuck.dir, "/Data/",settingName, ".RData"))#
#
}#
#
###Save SimSummary Object (Contains TrueCV values)#
SimSummary <- data.frame(SimSummary)#
save(SimSummary, file = paste0(berchuck.dir, "/Summary/SimSummary.RData"))
rm(list=ls())#
#
###Set the location of berchuck_code directory#
berchuck.dir <- "/Users/Sam/Desktop"#
#
###Load womblR package#
library(devtools)#
install_github("berchuck/womblR")#
library(womblR)#
#
###Load other libraries#
library(Matrix) #load for bdiag function#
library(mvtnorm) #load for rmvnorm function#
#
####################################################################
### Set data objects required for simulation.#
####################################################################
#
###Exponential temporal correlation structure (wraps womblR internal SIGMA function)#
Sigma <- function(phi, day) {#
	temp <- day / 365#
	womblR:::SIGMA(phi, 0, abs(outer(temp, temp, "-")), length(day))#
}#
#
###Glaucoma objects from womblR package#
blind_spot <- c(26, 35) # define blind spot#
W <- HFAII_Queen[ -blind_spot, -blind_spot] # visual field adjacency matrix (from womblR package)#
DM <- GarwayHeath[-blind_spot] # Garway-Heath angles (from womblR package)#
#
###Set data objects#
Rho <- 0.99#
M  <- 52#
EyeM <- diag(M)#
ScaleDM <- 100#
ScaleY <- 10#
WeightsInd <- 0 # continuous#
#
###Obtain the unique locations of adjacencies, AdjacentEdgesBoolean#
AdjacentEdgesBoolean <- (W == 1) & (!lower.tri(W))#
# AdjacentEdgesBoolean <- matrix(which(AdjacentEdgesBoolean) - 1, ncol = 1) # indicates the index of the adjacencies in the adjacency matrix#
#
###Obtain the scaled dissimilarity metric object, Z#
Dist <- function(x, y) pmin(abs(x - y), (360 - pmax(x, y) + pmin(x, y))) #arc length of optic nerve#
DM_Grid <- expand.grid(DM, DM)#
Z_Vector <- Dist(DM_Grid[ , 1], DM_Grid[ , 2])#
Z_Matrix <- matrix(Z_Vector, nrow = dim(W)[1], ncol = dim(W)[1], byrow = TRUE)#
Z <- matrix(Z_Matrix[AdjacentEdgesBoolean] / ScaleDM, ncol = 1) # vector of the scaled dissimilarity metric#
#
####################################################################
### Specify the simulation settings.#
####################################################################
#
###Delta: fixed at a posterior mean of the example patient#
delta <- matrix(c(2.44634552191589982328424, 0.07015085810136499622214, 0.97418292944508000363868), nrow = 3, ncol = 1)#
#
###T: fixed at a posterior mean of the example patient#
TFull <- matrix(0, nrow = 3, ncol = 3)#
TFull[lower.tri(TFull, diag=TRUE)] <- c(0.820415457249000001915817,  0.004111988364394999866114, -0.02815687181061940144722,  0.380455175360999986455823, -0.19165688755208701299360,  0.84042396003099995027696)#
TFull[upper.tri(TFull)] <- TFull[lower.tri(TFull)]#
TDiag <- TFull # diagnoal matrix with no cross-covariance#
TDiag[upper.tri(TDiag)] <- TDiag[lower.tri(TDiag)] <- 0#
#
###Phi: fixed at a posterior mean of the example patient#
phi <- 0.1633080144659999899392 # small value (i.e. temporal correlation)#
phiI <- 100 # very large (i.e. no temporal correlation --> (I)ndependence)#
#
###Number of visits#
NuVec <- c(7, 21) #median and maximum setting#
#
###Number of simulations#
NThetas <- 100 # number of Theta vectors sampled#
NDatas <- 10 # number of datasets sampled/Theta vector#
NTotal <- NThetas * NDatas#
#
###Begin Simulation#
set.seed(54)#
#
###Days of the VF vists (first 9 days are fixed at those from the average patient we used to set delta, T and phi, then we randomly sampled to get the remaining visit days out to 21 visit days)#
day <- c(0, 126, 238, 406, 504, 588, 756, 868, 938) # vistis of the example patient#
WaitingTime <- rpois(NuVec[2] - length(day), mean(diff(day))) # sample the remaining visits#
day <- c(day, cumsum(WaitingTime) + day[9])#
#
###Assemble simulation Settings:#
Settings <- list(#
list(phi = phiI, T = TDiag, Nu = NuVec[[1]]),#
list(phi = phiI, T = TFull, Nu = NuVec[[1]]),#
list(phi = phi, T = TDiag, Nu = NuVec[[1]]),#
list(phi = phi, T = TFull, Nu = NuVec[[1]]),#
list(phi = phiI, T = TDiag, Nu = NuVec[[2]]),#
list(phi = phiI, T = TFull, Nu = NuVec[[2]]),#
list(phi = phi, T = TDiag, Nu = NuVec[[2]]),#
list(phi = phi, T = TFull, Nu = NuVec[[2]]))#
NSettings <- length(Settings)#
#
###Create Object For Simulation Summary#
SimSummary <- matrix(nrow = NTotal * NSettings, ncol = 9)#
SimSummary[, 1] <- c(rep(paste(LETTERS[1:4], "Median", sep = "_"), each = NTotal), rep(paste(LETTERS[1:4], "Maximum", sep = "_"), each = NTotal))#
colnames(SimSummary) <- c("Setting", "TrueCV", "MeanCV", "VarCV", "2.5%CV", "97.5%CV", "Bias", "MSE", "EC")#
SettingNames <- unique(SimSummary[, 1])#
#
####################################################################
### Generate data for simulation based on defined settings.#
####################################################################
#
###Begin simulation#
for (i in 1:NSettings) {#
#
  	###Set simulation settings#
	settings <- Settings[[i]]#
	settingName <- SettingNames[i]#
	Nu <- settings$Nu#
	Day <- day[1:Nu]#
	T <- settings$T#
	phi <- settings$phi#
#
	###Sample thetas and assign level 1 parameters#
	Thetas <- mvtnorm::rmvnorm(NThetas, kronecker(delta, rep(1, Nu)), kronecker(T, Sigma(phi, Day)))#
#
	Mu <- Thetas[, 1:Nu]#
	Tau2 <- exp(Thetas[, (Nu + 1):(2 * Nu)]) ^ 2#
	Alpha <- exp(Thetas[, (2 * Nu + 1):(3 * Nu)])#
#
	###Save True CV#
	SimSummary[(1:NTotal) + (i - 1) * NTotal, 2] <- rep(apply(Alpha, 1, f <- function(x) sd(x) / mean(x)), each = NDatas)#
#
	###Simulate data#
	Yout <- matrix(nrow = Nu * M, ncol = NTotal)#
	for (j in 1:NThetas) {#
#
	  ##Set parameters#
		mu <- Mu[j, ]#
		tau2 <- Tau2[j, ]#
		alpha <- Alpha[j, ]#
		WAlphas <- womblR:::WAlphaCube(alpha, Z, W, M, Nu, WeightsInd)#
		JointCovs <- womblR:::JointCovarianceCube(WAlphas, tau2, EyeM, Rho, M, Nu)#
		JointCovsList <- list()#
		for (k in 1:Nu) JointCovsList[[k]] <- JointCovs[ , , k]#
#
		#Joint moments#
		Q <- matrix(Matrix::bdiag(JointCovsList), nrow = Nu * M)#
		JointMean <- kronecker(mu, rep(1, M))#
#
		#Simulate data#
		YStar <- mvtnorm::rmvnorm(NDatas, JointMean, Q)#
		Yout[, (1:NDatas) + (j - 1) * NDatas] <- apply(YStar, 1, f <- function(x) pmax(0, x * ScaleY))#
#
		###Output iteration#
		cat(paste0("Setting: ", settingName, "; Simulation: ", j * NDatas, "\n"))#
#
	#End sims over thetas#
	}#
#
	###Save simulated data for each setting#
	save(Yout, file = paste0(berchuck.dir, "/berchuck_code/Simulation/Data/",settingName, ".RData"))#
#
}#
#
###Save SimSummary Object (Contains TrueCV values)#
SimSummary <- data.frame(SimSummary)#
save(SimSummary, file = paste0(berchuck.dir, "/berchuck_code/Simulation/Summary/SimSummary.RData"))
setwd("/Users/Sam/Desktop/berchuck_code/Simulation/Data/")
load("A_Median.RData")
head(Yout[1:10,1:10])
Yout[1:10,1:10]
